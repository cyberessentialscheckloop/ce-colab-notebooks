{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamwelMakombe-cloud/ce-colab-notebooks/blob/main/streamlitdeployment_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW0YL402zttH"
      },
      "source": [
        "### **INSTALLATION OF REQUIRED PACKAGES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tdOXpcngaR--",
        "outputId": "4cf9753e-e8b5-4fc8-a05c-57f295d51a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting captum\n",
            "  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from captum) (3.10.0)\n",
            "Collecting numpy<2.0 (from captum)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from captum) (25.0)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.12/dist-packages (from captum) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from captum) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10->captum) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10->captum) (3.0.2)\n",
            "Downloading captum-0.8.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, captum\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed captum-0.8.0 numpy-1.26.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "7f6a52173d3044f9945cefd43bda261f",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install captum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3dRXIBv_-IX",
        "outputId": "6d4e4138-0db0-48a4-f381-71caeebb4d7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5ge39e3rfhh",
        "outputId": "1bec7820-9ce8-4bf2-b9b0-59be6c40f8f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# install fresh libs (one‑liner)\n",
        "!pip install -qU \"transformers>=4.40\" datasets peft accelerate bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiH6qpTMrTAg"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2kHSqTkrT0w"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft bitsandbytes accelerate transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ICD7KMlrWPx"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate peft bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNT8RqKFrZNA",
        "outputId": "5ba42e8e-a769-4b5b-c29c-99e6a43ccbfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.56.1 1.10.1 0.17.1\n"
          ]
        }
      ],
      "source": [
        "import transformers, accelerate, peft\n",
        "print(transformers.__version__, accelerate.__version__, peft.__version__)\n",
        "# → 4.53.0  1.9.1  0.17.2\n",
        "from transformers import TrainingArguments   # should import cleanly now\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMKYFFF5AHCB",
        "outputId": "eda31570-1772-460f-f5c9-175064ed7f25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pdfplumber pandas tqdm\n",
        "\n",
        "\n",
        "import re, pdfplumber, pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP75OT-_AG9Q",
        "outputId": "fe24c97c-b669-4133-eef1-a2e32a649a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rv4RNrbrOFQ"
      },
      "outputs": [],
      "source": [
        "# ─── one-time HF authentication (needed for zephyr) ────────────────────\n",
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLZitjEHAG46",
        "outputId": "18f068eb-c09f-4e41-d780-a00c132fc887"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All necessary packages loaded.\n"
          ]
        }
      ],
      "source": [
        "# I import all the packages I need.\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"All necessary packages loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD6FiB0xv8xU",
        "outputId": "bbfc2ebc-c240-4b28-a034-fdebf85e15a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.12/dist-packages (0.11.7)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m127.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.49.1\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pdfplumber pymupdf\n",
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZKrCCU0tNfz",
        "outputId": "e5c2f839-d78a-4499-9d8a-047c65815355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.49.1)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.12/dist-packages (0.11.7)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n"
          ]
        }
      ],
      "source": [
        "pip install streamlit pdfplumber pymupdf pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ol1PrNtc9qcm",
        "outputId": "fdb2046b-dabc-4762-b523-4591628b96a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxOIBYmO97sL",
        "outputId": "a0f9571c-c707-4353-e6b2-a8f77a7a0c32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting geotext\n",
            "  Downloading geotext-0.4.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading geotext-0.4.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: geotext\n",
            "Successfully installed geotext-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install geotext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RInJntLanm92",
        "outputId": "1c9a2228-bc26-4f0c-ef5c-3aed5a5ebc3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdfkit\n",
            "  Downloading pdfkit-1.0.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: pdfkit\n",
            "Successfully installed pdfkit-1.0.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,006 kB]\n",
            "Get:10 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,790 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,238 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,581 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,273 kB]\n",
            "Fetched 24.2 MB in 2s (12.6 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  avahi-daemon geoclue-2.0 glib-networking glib-networking-common\n",
            "  glib-networking-services gsettings-desktop-schemas iio-sensor-proxy\n",
            "  libavahi-core7 libavahi-glib1 libdaemon0 libevdev2 libgudev-1.0-0 libhyphen0\n",
            "  libinput-bin libinput10 libjson-glib-1.0-0 libjson-glib-1.0-common\n",
            "  libmbim-glib4 libmbim-proxy libmd4c0 libmm-glib0 libmtdev1 libnl-genl-3-200\n",
            "  libnotify4 libnss-mdns libproxy1v5 libqmi-glib5 libqmi-proxy libqt5core5a\n",
            "  libqt5dbus5 libqt5gui5 libqt5network5 libqt5positioning5 libqt5printsupport5\n",
            "  libqt5qml5 libqt5qmlmodels5 libqt5quick5 libqt5sensors5 libqt5svg5\n",
            "  libqt5webchannel5 libqt5webkit5 libqt5widgets5 libsoup2.4-1\n",
            "  libsoup2.4-common libudev1 libwacom-bin libwacom-common libwacom9 libwoff1\n",
            "  libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0 libxcb-util1\n",
            "  libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxkbcommon-x11-0 modemmanager\n",
            "  qt5-gtk-platformtheme qttranslations5-l10n session-migration\n",
            "  systemd-hwe-hwdb udev usb-modeswitch usb-modeswitch-data wpasupplicant\n",
            "Suggested packages:\n",
            "  avahi-autoipd gnome-shell | notification-daemon avahi-autoipd | zeroconf\n",
            "  qt5-image-formats-plugins qtwayland5 qt5-qmltooling-plugins comgt wvdial\n",
            "  wpagui libengine-pkcs11-openssl\n",
            "The following NEW packages will be installed:\n",
            "  avahi-daemon geoclue-2.0 glib-networking glib-networking-common\n",
            "  glib-networking-services gsettings-desktop-schemas iio-sensor-proxy\n",
            "  libavahi-core7 libavahi-glib1 libdaemon0 libevdev2 libgudev-1.0-0 libhyphen0\n",
            "  libinput-bin libinput10 libjson-glib-1.0-0 libjson-glib-1.0-common\n",
            "  libmbim-glib4 libmbim-proxy libmd4c0 libmm-glib0 libmtdev1 libnl-genl-3-200\n",
            "  libnotify4 libnss-mdns libproxy1v5 libqmi-glib5 libqmi-proxy libqt5core5a\n",
            "  libqt5dbus5 libqt5gui5 libqt5network5 libqt5positioning5 libqt5printsupport5\n",
            "  libqt5qml5 libqt5qmlmodels5 libqt5quick5 libqt5sensors5 libqt5svg5\n",
            "  libqt5webchannel5 libqt5webkit5 libqt5widgets5 libsoup2.4-1\n",
            "  libsoup2.4-common libwacom-bin libwacom-common libwacom9 libwoff1\n",
            "  libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0 libxcb-util1\n",
            "  libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxkbcommon-x11-0 modemmanager\n",
            "  qt5-gtk-platformtheme qttranslations5-l10n session-migration\n",
            "  systemd-hwe-hwdb udev usb-modeswitch usb-modeswitch-data wkhtmltopdf\n",
            "  wpasupplicant\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 67 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 35.5 MB of archives.\n",
            "After this operation, 141 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libavahi-core7 amd64 0.8-5ubuntu5.2 [90.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdaemon0 amd64 0.14-7.1ubuntu3 [14.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 avahi-daemon amd64 0.8-5ubuntu5.2 [69.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5core5a amd64 5.15.3+dfsg-2ubuntu0.2 [2,006 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevdev2 amd64 1.12.1+dfsg-1 [39.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmtdev1 amd64 1.1.6-1build4 [14.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.16 [76.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgudev-1.0-0 amd64 1:237-2build1 [16.3 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-common all 2.2.0-1 [54.3 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom9 amd64 2.2.0-1 [22.0 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput-bin amd64 1.20.0-1ubuntu0.3 [19.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput10 amd64 1.20.0-1ubuntu0.3 [131 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmd4c0 amd64 0.4.8-1 [42.0 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5dbus5 amd64 5.15.3+dfsg-2ubuntu0.2 [222 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5network5 amd64 5.15.3+dfsg-2ubuntu0.2 [731 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5gui5 amd64 5.15.3+dfsg-2ubuntu0.2 [3,722 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5widgets5 amd64 5.15.3+dfsg-2ubuntu0.2 [2,561 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5svg5 amd64 5.15.3-1 [149 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhyphen0 amd64 2.8.8-7build2 [28.2 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5positioning5 amd64 5.15.3+dfsg-3 [223 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5printsupport5 amd64 5.15.3+dfsg-2ubuntu0.2 [214 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5qml5 amd64 5.15.3+dfsg-1 [1,472 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5qmlmodels5 amd64 5.15.3+dfsg-1 [205 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5quick5 amd64 5.15.3+dfsg-1 [1,748 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5sensors5 amd64 5.15.3-1 [123 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5webchannel5 amd64 5.15.3-1 [62.9 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5webkit5 amd64 5.212.0~alpha4-15ubuntu1 [12.8 MB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.16 [1,557 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libavahi-glib1 amd64 0.8-5ubuntu5.2 [8,296 B]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-glib-1.0-common all 1.6.6-1build1 [4,432 B]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-glib-1.0-0 amd64 1.6.6-1build1 [69.9 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmm-glib0 amd64 1.20.0-1~ubuntu22.04.4 [262 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libnotify4 amd64 0.7.9-3ubuntu5.22.04.1 [20.3 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 libproxy1v5 amd64 0.4.17-2 [51.9 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/main amd64 glib-networking-common all 2.72.0-1 [3,718 B]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/main amd64 glib-networking-services amd64 2.72.0-1 [9,982 B]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/main amd64 glib-networking amd64 2.72.0-1 [69.8 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsoup2.4-common all 2.74.2-3ubuntu0.6 [4,778 B]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsoup2.4-1 amd64 2.74.2-3ubuntu0.6 [288 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/main amd64 geoclue-2.0 amd64 2.5.7-3ubuntu3 [111 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 iio-sensor-proxy amd64 3.3-0ubuntu6 [34.4 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmbim-glib4 amd64 1.28.0-1~ubuntu20.04.2 [192 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmbim-proxy amd64 1.28.0-1~ubuntu20.04.2 [6,160 B]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnl-genl-3-200 amd64 3.5.0-0.1 [12.4 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnss-mdns amd64 0.15.1-1ubuntu1 [27.0 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libqmi-glib5 amd64 1.32.0-1ubuntu0.22.04.1 [772 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libqmi-proxy amd64 1.32.0-1ubuntu0.22.04.1 [6,072 B]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-bin amd64 2.2.0-1 [13.6 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 modemmanager amd64 1.20.0-1~ubuntu22.04.4 [1,094 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 qt5-gtk-platformtheme amd64 5.15.3+dfsg-2ubuntu0.2 [130 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qttranslations5-l10n all 5.15.3-1 [1,983 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 wpasupplicant amd64 2:2.10-6ubuntu2.2 [1,482 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu jammy/main amd64 usb-modeswitch-data all 20191128-4 [33.2 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu jammy/main amd64 usb-modeswitch amd64 2.6.1-3ubuntu2 [46.0 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu jammy/universe amd64 wkhtmltopdf amd64 0.12.6-2 [173 kB]\n",
            "Fetched 35.5 MB in 1s (40.8 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package libavahi-core7:amd64.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libavahi-core7_0.8-5ubuntu5.2_amd64.deb ...\n",
            "Unpacking libavahi-core7:amd64 (0.8-5ubuntu5.2) ...\n",
            "Selecting previously unselected package libdaemon0:amd64.\n",
            "Preparing to unpack .../1-libdaemon0_0.14-7.1ubuntu3_amd64.deb ...\n",
            "Unpacking libdaemon0:amd64 (0.14-7.1ubuntu3) ...\n",
            "Selecting previously unselected package avahi-daemon.\n",
            "Preparing to unpack .../2-avahi-daemon_0.8-5ubuntu5.2_amd64.deb ...\n",
            "Unpacking avahi-daemon (0.8-5ubuntu5.2) ...\n",
            "Selecting previously unselected package libqt5core5a:amd64.\n",
            "Preparing to unpack .../3-libqt5core5a_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libevdev2:amd64.\n",
            "Preparing to unpack .../4-libevdev2_1.12.1+dfsg-1_amd64.deb ...\n",
            "Unpacking libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Selecting previously unselected package libmtdev1:amd64.\n",
            "Preparing to unpack .../5-libmtdev1_1.1.6-1build4_amd64.deb ...\n",
            "Unpacking libmtdev1:amd64 (1.1.6-1build4) ...\n",
            "Preparing to unpack .../6-libudev1_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.16) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package libgudev-1.0-0:amd64.\n",
            "(Reading database ... 126443 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\n",
            "Unpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Selecting previously unselected package libwacom-common.\n",
            "Preparing to unpack .../01-libwacom-common_2.2.0-1_all.deb ...\n",
            "Unpacking libwacom-common (2.2.0-1) ...\n",
            "Selecting previously unselected package libwacom9:amd64.\n",
            "Preparing to unpack .../02-libwacom9_2.2.0-1_amd64.deb ...\n",
            "Unpacking libwacom9:amd64 (2.2.0-1) ...\n",
            "Selecting previously unselected package libinput-bin.\n",
            "Preparing to unpack .../03-libinput-bin_1.20.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libinput-bin (1.20.0-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libinput10:amd64.\n",
            "Preparing to unpack .../04-libinput10_1.20.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libmd4c0:amd64.\n",
            "Preparing to unpack .../05-libmd4c0_0.4.8-1_amd64.deb ...\n",
            "Unpacking libmd4c0:amd64 (0.4.8-1) ...\n",
            "Selecting previously unselected package libqt5dbus5:amd64.\n",
            "Preparing to unpack .../06-libqt5dbus5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5network5:amd64.\n",
            "Preparing to unpack .../07-libqt5network5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../08-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../09-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../10-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../11-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../12-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../13-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../14-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../15-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../16-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libqt5gui5:amd64.\n",
            "Preparing to unpack .../17-libqt5gui5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5widgets5:amd64.\n",
            "Preparing to unpack .../18-libqt5widgets5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5svg5:amd64.\n",
            "Preparing to unpack .../19-libqt5svg5_5.15.3-1_amd64.deb ...\n",
            "Unpacking libqt5svg5:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package libhyphen0:amd64.\n",
            "Preparing to unpack .../20-libhyphen0_2.8.8-7build2_amd64.deb ...\n",
            "Unpacking libhyphen0:amd64 (2.8.8-7build2) ...\n",
            "Selecting previously unselected package libqt5positioning5:amd64.\n",
            "Preparing to unpack .../21-libqt5positioning5_5.15.3+dfsg-3_amd64.deb ...\n",
            "Unpacking libqt5positioning5:amd64 (5.15.3+dfsg-3) ...\n",
            "Selecting previously unselected package libqt5printsupport5:amd64.\n",
            "Preparing to unpack .../22-libqt5printsupport5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5printsupport5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5qml5:amd64.\n",
            "Preparing to unpack .../23-libqt5qml5_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking libqt5qml5:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package libqt5qmlmodels5:amd64.\n",
            "Preparing to unpack .../24-libqt5qmlmodels5_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking libqt5qmlmodels5:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package libqt5quick5:amd64.\n",
            "Preparing to unpack .../25-libqt5quick5_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking libqt5quick5:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package libqt5sensors5:amd64.\n",
            "Preparing to unpack .../26-libqt5sensors5_5.15.3-1_amd64.deb ...\n",
            "Unpacking libqt5sensors5:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package libqt5webchannel5:amd64.\n",
            "Preparing to unpack .../27-libqt5webchannel5_5.15.3-1_amd64.deb ...\n",
            "Unpacking libqt5webchannel5:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package libwoff1:amd64.\n",
            "Preparing to unpack .../28-libwoff1_1.0.2-1build4_amd64.deb ...\n",
            "Unpacking libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Selecting previously unselected package libqt5webkit5:amd64.\n",
            "Preparing to unpack .../29-libqt5webkit5_5.212.0~alpha4-15ubuntu1_amd64.deb ...\n",
            "Unpacking libqt5webkit5:amd64 (5.212.0~alpha4-15ubuntu1) ...\n",
            "Selecting previously unselected package udev.\n",
            "Preparing to unpack .../30-udev_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package libavahi-glib1:amd64.\n",
            "Preparing to unpack .../31-libavahi-glib1_0.8-5ubuntu5.2_amd64.deb ...\n",
            "Unpacking libavahi-glib1:amd64 (0.8-5ubuntu5.2) ...\n",
            "Selecting previously unselected package libjson-glib-1.0-common.\n",
            "Preparing to unpack .../32-libjson-glib-1.0-common_1.6.6-1build1_all.deb ...\n",
            "Unpacking libjson-glib-1.0-common (1.6.6-1build1) ...\n",
            "Selecting previously unselected package libjson-glib-1.0-0:amd64.\n",
            "Preparing to unpack .../33-libjson-glib-1.0-0_1.6.6-1build1_amd64.deb ...\n",
            "Unpacking libjson-glib-1.0-0:amd64 (1.6.6-1build1) ...\n",
            "Selecting previously unselected package libmm-glib0:amd64.\n",
            "Preparing to unpack .../34-libmm-glib0_1.20.0-1~ubuntu22.04.4_amd64.deb ...\n",
            "Unpacking libmm-glib0:amd64 (1.20.0-1~ubuntu22.04.4) ...\n",
            "Selecting previously unselected package libnotify4:amd64.\n",
            "Preparing to unpack .../35-libnotify4_0.7.9-3ubuntu5.22.04.1_amd64.deb ...\n",
            "Unpacking libnotify4:amd64 (0.7.9-3ubuntu5.22.04.1) ...\n",
            "Selecting previously unselected package libproxy1v5:amd64.\n",
            "Preparing to unpack .../36-libproxy1v5_0.4.17-2_amd64.deb ...\n",
            "Unpacking libproxy1v5:amd64 (0.4.17-2) ...\n",
            "Selecting previously unselected package glib-networking-common.\n",
            "Preparing to unpack .../37-glib-networking-common_2.72.0-1_all.deb ...\n",
            "Unpacking glib-networking-common (2.72.0-1) ...\n",
            "Selecting previously unselected package glib-networking-services.\n",
            "Preparing to unpack .../38-glib-networking-services_2.72.0-1_amd64.deb ...\n",
            "Unpacking glib-networking-services (2.72.0-1) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../39-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../40-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package glib-networking:amd64.\n",
            "Preparing to unpack .../41-glib-networking_2.72.0-1_amd64.deb ...\n",
            "Unpacking glib-networking:amd64 (2.72.0-1) ...\n",
            "Selecting previously unselected package libsoup2.4-common.\n",
            "Preparing to unpack .../42-libsoup2.4-common_2.74.2-3ubuntu0.6_all.deb ...\n",
            "Unpacking libsoup2.4-common (2.74.2-3ubuntu0.6) ...\n",
            "Selecting previously unselected package libsoup2.4-1:amd64.\n",
            "Preparing to unpack .../43-libsoup2.4-1_2.74.2-3ubuntu0.6_amd64.deb ...\n",
            "Unpacking libsoup2.4-1:amd64 (2.74.2-3ubuntu0.6) ...\n",
            "Selecting previously unselected package geoclue-2.0.\n",
            "Preparing to unpack .../44-geoclue-2.0_2.5.7-3ubuntu3_amd64.deb ...\n",
            "Unpacking geoclue-2.0 (2.5.7-3ubuntu3) ...\n",
            "Selecting previously unselected package iio-sensor-proxy.\n",
            "Preparing to unpack .../45-iio-sensor-proxy_3.3-0ubuntu6_amd64.deb ...\n",
            "Unpacking iio-sensor-proxy (3.3-0ubuntu6) ...\n",
            "Selecting previously unselected package libmbim-glib4:amd64.\n",
            "Preparing to unpack .../46-libmbim-glib4_1.28.0-1~ubuntu20.04.2_amd64.deb ...\n",
            "Unpacking libmbim-glib4:amd64 (1.28.0-1~ubuntu20.04.2) ...\n",
            "Selecting previously unselected package libmbim-proxy.\n",
            "Preparing to unpack .../47-libmbim-proxy_1.28.0-1~ubuntu20.04.2_amd64.deb ...\n",
            "Unpacking libmbim-proxy (1.28.0-1~ubuntu20.04.2) ...\n",
            "Selecting previously unselected package libnl-genl-3-200:amd64.\n",
            "Preparing to unpack .../48-libnl-genl-3-200_3.5.0-0.1_amd64.deb ...\n",
            "Unpacking libnl-genl-3-200:amd64 (3.5.0-0.1) ...\n",
            "Selecting previously unselected package libnss-mdns:amd64.\n",
            "Preparing to unpack .../49-libnss-mdns_0.15.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnss-mdns:amd64 (0.15.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libqmi-glib5:amd64.\n",
            "Preparing to unpack .../50-libqmi-glib5_1.32.0-1ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libqmi-glib5:amd64 (1.32.0-1ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libqmi-proxy.\n",
            "Preparing to unpack .../51-libqmi-proxy_1.32.0-1ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libqmi-proxy (1.32.0-1ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libwacom-bin.\n",
            "Preparing to unpack .../52-libwacom-bin_2.2.0-1_amd64.deb ...\n",
            "Unpacking libwacom-bin (2.2.0-1) ...\n",
            "Selecting previously unselected package modemmanager.\n",
            "Preparing to unpack .../53-modemmanager_1.20.0-1~ubuntu22.04.4_amd64.deb ...\n",
            "Unpacking modemmanager (1.20.0-1~ubuntu22.04.4) ...\n",
            "Selecting previously unselected package qt5-gtk-platformtheme:amd64.\n",
            "Preparing to unpack .../54-qt5-gtk-platformtheme_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package qttranslations5-l10n.\n",
            "Preparing to unpack .../55-qttranslations5-l10n_5.15.3-1_all.deb ...\n",
            "Unpacking qttranslations5-l10n (5.15.3-1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../56-systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Selecting previously unselected package wpasupplicant.\n",
            "Preparing to unpack .../57-wpasupplicant_2%3a2.10-6ubuntu2.2_amd64.deb ...\n",
            "Unpacking wpasupplicant (2:2.10-6ubuntu2.2) ...\n",
            "Selecting previously unselected package usb-modeswitch-data.\n",
            "Preparing to unpack .../58-usb-modeswitch-data_20191128-4_all.deb ...\n",
            "Unpacking usb-modeswitch-data (20191128-4) ...\n",
            "Selecting previously unselected package usb-modeswitch.\n",
            "Preparing to unpack .../59-usb-modeswitch_2.6.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking usb-modeswitch (2.6.1-3ubuntu2) ...\n",
            "Selecting previously unselected package wkhtmltopdf.\n",
            "Preparing to unpack .../60-wkhtmltopdf_0.12.6-2_amd64.deb ...\n",
            "Unpacking wkhtmltopdf (0.12.6-2) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service → /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up libproxy1v5:amd64 (0.4.17-2) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Setting up libhyphen0:amd64 (2.8.8-7build2) ...\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up qttranslations5-l10n (5.15.3-1) ...\n",
            "Setting up libnotify4:amd64 (0.7.9-3ubuntu5.22.04.1) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up usb-modeswitch-data (20191128-4) ...\n",
            "Setting up udev (249.11-0ubuntu3.16) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libmtdev1:amd64 (1.1.6-1build4) ...\n",
            "Setting up libsoup2.4-common (2.74.2-3ubuntu0.6) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up libmm-glib0:amd64 (1.20.0-1~ubuntu22.04.4) ...\n",
            "Setting up libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libnl-genl-3-200:amd64 (3.5.0-0.1) ...\n",
            "Setting up libmd4c0:amd64 (0.4.8-1) ...\n",
            "Setting up libavahi-glib1:amd64 (0.8-5ubuntu5.2) ...\n",
            "Setting up libjson-glib-1.0-common (1.6.6-1build1) ...\n",
            "Setting up usb-modeswitch (2.6.1-3ubuntu2) ...\n",
            "Setting up glib-networking-common (2.72.0-1) ...\n",
            "Setting up libqt5sensors5:amd64 (5.15.3-1) ...\n",
            "Setting up libdaemon0:amd64 (0.14-7.1ubuntu3) ...\n",
            "Setting up libavahi-core7:amd64 (0.8-5ubuntu5.2) ...\n",
            "Setting up libnss-mdns:amd64 (0.15.1-1ubuntu1) ...\n",
            "First installation detected...\n",
            "Checking NSS setup...\n",
            "Setting up libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Setting up libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Setting up libmbim-glib4:amd64 (1.28.0-1~ubuntu20.04.2) ...\n",
            "Setting up libwacom-common (2.2.0-1) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up glib-networking-services (2.72.0-1) ...\n",
            "Setting up iio-sensor-proxy (3.3-0ubuntu6) ...\n",
            "Setting up libwacom9:amd64 (2.2.0-1) ...\n",
            "Setting up libqt5positioning5:amd64 (5.15.3+dfsg-3) ...\n",
            "Setting up libmbim-proxy (1.28.0-1~ubuntu20.04.2) ...\n",
            "Setting up libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libjson-glib-1.0-0:amd64 (1.6.6-1build1) ...\n",
            "Setting up libinput-bin (1.20.0-1ubuntu0.3) ...\n",
            "Setting up wpasupplicant (2:2.10-6ubuntu2.2) ...\n",
            "Created symlink /etc/systemd/system/dbus-fi.w1.wpa_supplicant1.service → /lib/systemd/system/wpa_supplicant.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/wpa_supplicant.service → /lib/systemd/system/wpa_supplicant.service.\n",
            "Setting up libqt5qml5:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up libqt5webchannel5:amd64 (5.15.3-1) ...\n",
            "Setting up libwacom-bin (2.2.0-1) ...\n",
            "Setting up avahi-daemon (0.8-5ubuntu5.2) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of force-reload.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Created symlink /etc/systemd/system/dbus-org.freedesktop.Avahi.service → /lib/systemd/system/avahi-daemon.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/avahi-daemon.service → /lib/systemd/system/avahi-daemon.service.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/avahi-daemon.socket → /lib/systemd/system/avahi-daemon.socket.\n",
            "Setting up libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
            "Setting up libqt5qmlmodels5:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqmi-glib5:amd64 (1.32.0-1ubuntu0.22.04.1) ...\n",
            "Setting up libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5printsupport5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5quick5:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up libqt5svg5:amd64 (5.15.3-1) ...\n",
            "Setting up libqmi-proxy (1.32.0-1ubuntu0.22.04.1) ...\n",
            "Setting up libqt5webkit5:amd64 (5.212.0~alpha4-15ubuntu1) ...\n",
            "Setting up modemmanager (1.20.0-1~ubuntu22.04.4) ...\n",
            "Created symlink /etc/systemd/system/dbus-org.freedesktop.ModemManager1.service → /lib/systemd/system/ModemManager.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/ModemManager.service → /lib/systemd/system/ModemManager.service.\n",
            "Setting up wkhtmltopdf (0.12.6-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Setting up glib-networking:amd64 (2.72.0-1) ...\n",
            "Setting up libsoup2.4-1:amd64 (2.74.2-3ubuntu0.6) ...\n",
            "Setting up geoclue-2.0 (2.5.7-3ubuntu3) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfkit\n",
        "!apt-get update\n",
        "!apt-get install -y wkhtmltopdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7k9Na55n5Gl",
        "outputId": "b70576e0-586e-4183-964b-08fc67857219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wkhtmltopdf 0.12.6\n"
          ]
        }
      ],
      "source": [
        "!wkhtmltopdf --version\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H-aHq_Vm6JP"
      },
      "source": [
        "### **PDF DATA ANONYMISATION BEFORE APP SUBMISSION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "CZ9TsJ-MjWLK",
        "outputId": "ccc24dbe-a2ff-4879-8508-a1031738f9e9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-83374f6a-ba6a-4e37-83eb-738e6f324157\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-83374f6a-ba6a-4e37-83eb-738e6f324157\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving answers (1).pdf to answers (1).pdf\n",
            "Done – download anonymized.pdf below\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_ba311323-bea3-4bed-896e-b81fee1ffc70\", \"anonymized.pdf\", 257801)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ╔════════════════════════════════════════════════════════════╗\n",
        "# ║  Cyber‑Essentials PDF Answer Redactor v3 – Answer Only    ║\n",
        "# ╠════════════════════════════════════════════════════════════╣\n",
        "# ║  * PyMuPDF for in‑place redaction                         ║\n",
        "# ║  * Redacts only answers, keeps questions untouched        ║\n",
        "# ╚════════════════════════════════════════════════════════════╝\n",
        "\n",
        "import fitz, re\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Upload PDF\n",
        "uploaded = files.upload()\n",
        "PDF_NAME = next(iter(uploaded))\n",
        "doc = fitz.open(stream=uploaded[PDF_NAME], filetype=\"pdf\")\n",
        "\n",
        "# 2. Your list of (question_id, question_regex, answer_regex)\n",
        "QNA_REGEX = [\n",
        "    # (question_id, pattern to match start, pattern to extract answer)\n",
        "    (\"A1.1\", r\"A1\\.1.*?\\n\", r\"A1\\.1.*?\\n(.*?)(?=\\nA\\d|\\Z)\"),\n",
        "    (\"A1.3\", r\"A1\\.3.*?\\n\", r\"A1\\.3.*?\\n(.*?)(?=\\nA\\d|\\Z)\"),\n",
        "    (\"A1.4\", r\"A1\\.4.*?\\n\", r\"A1\\.4.*?\\n(.*?)(?=\\nA\\d|\\Z)\"),\n",
        "    (\"A1.6\", r\"A1\\.6.*?\\n\", r\"A1\\.6.*?\\n(.*?)(?=\\nA\\d|\\Z)\"),\n",
        "    (\"A2.10\", r\"A2\\.10.*?\\n\", r\"A2\\.10.*?\\n(.*?)(?=\\nA\\d|\\Z)\"),\n",
        "    (\"A3.4\", r\"A3\\.4.*?\\n\", r\"A3\\.4.*?\\n(.*?)(?=\\nA\\d|\\Z)\"),\n",
        "    # add more as needed\n",
        "]\n",
        "\n",
        "BLANK_TOKENS = {\"\", \"n/a\", \"na\", \"not applicable\", \"-\"}\n",
        "\n",
        "def redact_answer_on_page(page, answer):\n",
        "    \"\"\"Redact each nonblank line in answer text on the page, replace with 'REDACTED'.\"\"\"\n",
        "    answer_lines = [l.strip() for l in answer.splitlines() if l.strip() and l.strip().lower() not in BLANK_TOKENS]\n",
        "    for line in answer_lines:\n",
        "        found = False\n",
        "        for b in page.get_text(\"blocks\"):\n",
        "            if line in b[4]:\n",
        "                r = fitz.Rect(b[:4])\n",
        "                page.add_redact_annot(r, fill=(1,1,1))\n",
        "                page.insert_textbox(r, \"REDACTED\", fontsize=11, color=(1,0,0), align=1)\n",
        "                found = True\n",
        "        # Optional: print if not found\n",
        "        if not found:\n",
        "            print(f\"Line not found for redaction: {line[:50]}...\")\n",
        "\n",
        "for page in doc:\n",
        "    full_text = page.get_text(\"text\")\n",
        "    for qid, qpat, apat in QNA_REGEX:\n",
        "        # Try to extract the answer for this question on this page\n",
        "        m = re.search(apat, full_text, re.S)\n",
        "        if m:\n",
        "            answer = m.group(1).strip()\n",
        "            if answer and answer.lower() not in BLANK_TOKENS:\n",
        "                redact_answer_on_page(page, answer)\n",
        "\n",
        "# Finalise\n",
        "for page in doc:\n",
        "    page.apply_redactions()\n",
        "doc.save(\"anonymized.pdf\", deflate=True, clean=True)\n",
        "doc.close()\n",
        "print(\"Done – download anonymized.pdf below\")\n",
        "files.download(\"anonymized.pdf\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ7U3IByz5l_"
      },
      "source": [
        "### **PYTHON LOGIC MODULES PREPARATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgu88Ums7dU7"
      },
      "source": [
        "### Deterministic Rules Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfyWxIeC1tH9",
        "outputId": "26b47d11-84a3-40aa-e3f9-f5a455b0d34c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting yesnorules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile yesnorules.py\n",
        "\"\"\"\n",
        "Rule‑based classifier for the simple Yes/No & information‑only questions\n",
        "used in the Cyber‑Essentials free‑text dataset.\n",
        "\n",
        "Returns a tuple (label, short_reason).\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────\n",
        "# 1.  ID buckets\n",
        "# ──────────────────────────────────────────────────────────────────\n",
        "INFO_ONLY_IDS = {\n",
        "    \"A1.1\",\"A1.2\",\"A1.3\",\"A1.4\",\"A1.5\",\"A1.6\",\"A1.7\",\n",
        "    \"A1.8\",\"A1.8.1\",\"A1.8.2\",\"A1.8.3\",\"A1.8.4\",\"A1.8.5\",\n",
        "    \"A1.10\",\n",
        "    \"A3.1\",\"A3.2\",\"A3.3\",\"A3.4\",\n",
        "    \"A4.11\",\n",
        "    \"A5.7\",\n",
        "    \"A6.4.1\",\"A6.5.1\",\"A1.11\",\"A7.15\"\n",
        "}\n",
        "\n",
        "YES_NO_ACCEPT_NO_IDS = {\"A4.5\", \"A7.14\"}\n",
        "\n",
        "YES_NO_MUST_CHECK_NOTES_IDS = {\n",
        "    \"A4.1\",\"A4.2\",\"A4.4\",\"A4.7\",\"A4.9\",\n",
        "    \"A5.2\",\"A5.3\",\"A5.8\",\"A5.9\",\n",
        "    \"A6.1\",\"A6.2\",\"A6.3\",\"A6.4\",\"A6.5\",\"A6.6\",\n",
        "    \"A7.2\",\"A7.8\",\"A7.9\",\"A7.13\",\"A7.16\",\"A7.17\",\n",
        "    \"A1.9\",\n",
        "    \"A8.2\",\"A8.3\",\"A8.4\",\"A8.5\",\"A7.5\",\"A4.5.1\"\n",
        "}\n",
        "\n",
        "YES_NO_NON_SCORING_IDS = {\"A2.1\",\"A3.1\",\"A4.8\",\"A4.11\",\"A5.4\"}\n",
        "\n",
        "# IDs that have bespoke logic later in the file\n",
        "SPECIAL_CASE_IDS = {\"A4.3\", \"A5.5\", \"A8.1\"} #multiple choices\n",
        "\n",
        "RULE_IDS = (\n",
        "      INFO_ONLY_IDS\n",
        "    | YES_NO_ACCEPT_NO_IDS\n",
        "    | YES_NO_MUST_CHECK_NOTES_IDS\n",
        "    | YES_NO_NON_SCORING_IDS\n",
        "    | SPECIAL_CASE_IDS\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────\n",
        "# 2.  Helpers\n",
        "# ──────────────────────────────────────────────────────────────────\n",
        "def _is_blank(txt: str) -> bool:\n",
        "    \"\"\"Common ‘blank’ placeholders.\"\"\"\n",
        "    return txt.strip().lower() in {\"\", \"n/a\", \"na\", \"none\", \"not applicable\"}\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────\n",
        "# 3.  Main classifier\n",
        "# ──────────────────────────────────────────────────────────────────\n",
        "def classify_compliance_with_reason(row):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    row : pandas.Series with at least\n",
        "        - question_id\n",
        "        - answer_text\n",
        "        - auto_fail_blank  (optional boolean/str)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (label, reason)   e.g. (\"Compliant\", \"Answered 'Yes' (compliant)\")\n",
        "    \"\"\"\n",
        "    qid = row.get(\"question_id\", \"\")\n",
        "    ans = str(row.get(\"answer_text\", \"\")).strip().lower()\n",
        "    auto_fail = str(row.get(\"auto_fail_blank\", \"False\")).lower() == \"true\"\n",
        "    blank = _is_blank(ans)\n",
        "\n",
        "    # — 3.1 generic guards —\n",
        "    if \"[redacted]\" in ans:\n",
        "        return \"Compliant\", \"Answer is redacted (accepted as compliant)\"\n",
        "    if auto_fail:\n",
        "        return \"Fail\", \"Marked as auto‑fail due to blank or flag\"\n",
        "\n",
        "    # — 3.2 bucket rules —\n",
        "    if qid in INFO_ONLY_IDS:\n",
        "        return (\n",
        "            (\"Compliant\", \"Info‑only question with non‑blank answer\")\n",
        "            if not blank else\n",
        "            (\"Fail\", \"Info‑only question left blank\")\n",
        "        )\n",
        "\n",
        "    if qid in YES_NO_ACCEPT_NO_IDS:\n",
        "        return (\n",
        "            (\"Compliant\", \"Yes/No (No accepted) answered\")\n",
        "            if not blank else\n",
        "            (\"Fail\", \"Yes/No (No accepted) left blank\")\n",
        "        )\n",
        "\n",
        "    if qid in YES_NO_MUST_CHECK_NOTES_IDS:\n",
        "        if ans.startswith(\"yes\"):\n",
        "            return \"Compliant\", \"Answered 'Yes' (compliant)\"\n",
        "        if ans.startswith(\"no\"):\n",
        "            return \"Non-compliant\", \"Answered 'No' (non‑compliant)\"\n",
        "        return \"Fail\", \"Did not answer with 'Yes' or 'No'\"\n",
        "\n",
        "    if qid in YES_NO_NON_SCORING_IDS:\n",
        "        return (\n",
        "            (\"Compliant\", \"Non‑scoring Yes/No with answer\")\n",
        "            if not blank else\n",
        "            (\"Fail\", \"Non‑scoring Yes/No left blank\")\n",
        "        )\n",
        "\n",
        "    # — 3.3 bespoke multiple‑choice rules —\n",
        "    if qid in {\"A4.3\", \"A5.5\"}:\n",
        "        only_d_selected = (\n",
        "            re.search(r\"\\bd[\\.\\)]?\\s*none of the above\", ans)\n",
        "            and not re.search(r\"\\b[abc][\\.\\)]\", ans)\n",
        "        )\n",
        "        if only_d_selected:\n",
        "            explanation = re.sub(r\"d[\\.\\)]?\\s*none of the above\", \"\", ans).strip()\n",
        "            if len(explanation.split()) < 8:\n",
        "                return \"Non-compliant\", (\n",
        "                    \"Only 'None of the above' selected; explanation too short\"\n",
        "                )\n",
        "        return \"Compliant\", \"Other option selected or a valid explanation provided\"\n",
        "\n",
        "    if qid == \"A8.1\":\n",
        "        only_c_selected = (\n",
        "            re.search(r\"\\bc[\\.\\)]?\\s*none of the above\", ans)\n",
        "            and not re.search(r\"\\b[abd][\\.\\)]\", ans)\n",
        "        )\n",
        "        if only_c_selected:\n",
        "            explanation = re.sub(r\"c[\\.\\)]?\\s*none of the above\", \"\", ans).strip()\n",
        "            if len(explanation.split()) < 8:\n",
        "                return \"Non-compliant\", (\n",
        "                    \"Only 'None of the above' selected; explanation too short\"\n",
        "                )\n",
        "        return \"Compliant\", \"Other option selected or a valid explanation provided\"\n",
        "\n",
        "    # — 3.4 fallback —\n",
        "    return \"Unknown\", \"No matching rule applied\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDOf9q0u7YVk"
      },
      "source": [
        "### Global Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3r8tEAY7LSe",
        "outputId": "580b3fc8-4719-4aa4-c52a-ea7d104fa019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting global_patterns.py\n"
          ]
        }
      ],
      "source": [
        "# 🔹 Add global patterns (optional, not per-question specific)\n",
        "%%writefile global_patterns.py\n",
        "\n",
        "global_patterns = [\n",
        "{\"label\": \"TOOL\", \"pattern\": \"SentinelOne\"},\n",
        "{\"label\": \"TOOL\", \"pattern\": \"Antivirus\"},\n",
        "{\"label\": \"DEVICE\", \"pattern\": \"laptops\"},\n",
        "{\"label\": \"DEVICE\", \"pattern\": \"servers\"},\n",
        "{\"label\": \"ROLE\", \"pattern\": \"administrator\"},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"it\"}, {\"LOWER\": \"manager\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"director\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"head\"}, {\"LOWER\": \"of\"}, {\"LOWER\": \"it\"}]},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"aberdeen\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"glasgow\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"bathgate\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"elgin\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"dundee\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"swansea\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"bengaluru\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"sydney\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"frankfurt\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"rosyth\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"san jose\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"lafayette\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"congo\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"winchester\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"nairobi\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"istanbul\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"hong kong\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"turkey\"},\n",
        "{\"label\": \"SCOPE\", \"pattern\": \"all uk offices\"},\n",
        "{\"label\": \"SCOPE\", \"pattern\": \"remote\"},\n",
        "{\"label\": \"SCOPE\", \"pattern\": \"hybrid\"},\n",
        "{\"label\": \"SCOPE\", \"pattern\": \"buildings\"},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"windows\"}, {\"IS_DIGIT\": True}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"windows\"}, {\"TEXT\": {\"REGEX\": \"10|11\"}}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"macos\"}, {\"TEXT\": {\"REGEX\": \"sonoma|ventura|sequoia|monterey\"}}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"linux\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"ubuntu\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"mageia\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"fedora\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"android\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"ios\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"ipados\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"intune\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"sccm\"}]},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"laptop\"}]},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"desktop\"}]},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"virtual\"}, {\"LOWER\": \"desktop\"}]},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"macbook\"}]},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"surface\"}]},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"ipad\"}]},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"nuc\"}]},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"thin\"}, {\"LOWER\": \"client\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"igel\"}, {\"LOWER\": \"os\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"hp\"}, {\"LOWER\": \"thin\"}, {\"LOWER\": \"os\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"windows\"}, {\"IS_DIGIT\": True}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"linux\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"ubuntu\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"virtual\"}, {\"LOWER\": \"desktop\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"windows\"}, {\"LOWER\": \"server\"}, {\"IS_DIGIT\": True}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"ubuntu\"}, {\"TEXT\": {\"REGEX\": \"\\d+\\.\\d+\"}}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"redhat\"}, {\"LOWER\": \"linux\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"rocky\"}, {\"LOWER\": \"linux\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"oracle\"}, {\"LOWER\": \"linux\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"centos\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"debian\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"vmware\"}, {\"LOWER\": \"esxi\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"hyper\"}, {\"LOWER\": \"v\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"proxmox\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"vcenter\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"azure\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"gcp\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"cloud\"}, {\"LOWER\": \"run\"}]},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"iphone\"}]},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"ipad\"}]},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"android\"}, {\"LOWER\": \"phone\"}]},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"tablet\"}]},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"mobile\"}, {\"LOWER\": \"device\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"ios\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"ipados\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"android\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"coloros\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"magic\"}, {\"LOWER\": \"os\"}]},\n",
        "{\"label\": \"OS\", \"pattern\": [{\"LOWER\": \"emu1\"}]},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"aberdeen\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"glasgow\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"london\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"coventry\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"lichfield\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"turkey\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"reading\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"zurich\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"bangalore\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"gurgaon\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"virginia\"},\n",
        "{\"label\": \"LOCATION\", \"pattern\": \"il\"},\n",
        "{\"label\": \"DEVICE\", \"pattern\": [{\"LOWER\": \"router\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"vpn\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"azure\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"lan\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"wi-fi\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"network\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"home\"}, {\"LOWER\": \"worker\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"remote\"}, {\"LOWER\": \"worker\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"hybrid\"}, {\"LOWER\": \"worker\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"vpn\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"remote\"}, {\"LOWER\": \"access\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"fortigate\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"cisco\"}, {\"TEXT\": {\"REGEX\": \".*\"}}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"fortigate\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"meraki\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"draytek\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"palo\"}, {\"LOWER\": \"alto\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"watchguard\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"juniper\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"tp-link\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"asus\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"bitdefender\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"windows\"}, {\"LOWER\": \"defender\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"sophos\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"untangle\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"pfsense\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"technicolor\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"vodafone\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"smart\"}, {\"LOWER\": \"hub\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"nokia\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"algosec\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"barracuda\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"azure\"}, {\"LOWER\": \"virtual\"}, {\"LOWER\": \"firewall\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"microsoft\"}, {\"LOWER\": \"365\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"google\"}, {\"LOWER\": \"workspace\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"aws\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"azure\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"dropbox\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"salesforce\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"zendesk\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"jira\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"github\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"adobe\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"crowdstrike\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"cloudflare\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"slack\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"zoom\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"asana\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"teamviewer\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"jamf\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"xero\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"imperva\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"wiz\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"oracle\"}, {\"LOWER\": \"cloud\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"netsuite\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"famly\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"doctract\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"datadog\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"survey\"}, {\"LOWER\": \"monkey\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"sharepoint\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"onedrive\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"software\"}, {\"LOWER\": \"firewall\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"windows\"}, {\"LOWER\": \"firewall\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"defender\"}, {\"LOWER\": \"firewall\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"macos\"}, {\"LOWER\": \"firewall\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"bitdefender\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"eset\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"iptables\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"vpn\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"globalprotect\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"intune\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"cloudflare\"}, {\"LOWER\": \"warp\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"group\"}, {\"LOWER\": \"policy\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"jamf\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"posture\"}, {\"LOWER\": \"check\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"roboshadow\"}]},\n",
        "{\"label\": \"SECURITY\", \"pattern\": [{\"LOWER\": \"mfa\"}]},\n",
        "{\"label\": \"SECURITY\", \"pattern\": [{\"LOWER\": \"2fa\"}]},\n",
        "{\"label\": \"SECURITY\", \"pattern\": [{\"LOWER\": \"multi\"}, {\"LOWER\": \"factor\"}, {\"LOWER\": \"authentication\"}]},\n",
        "{\"label\": \"SECURITY\", \"pattern\": [{\"LOWER\": \"trusted\"}, {\"LOWER\": \"ip\"}]},\n",
        "{\"label\": \"SECURITY\", \"pattern\": [{\"LOWER\": \"ip\"}, {\"LOWER\": \"restriction\"}]},\n",
        "{\"label\": \"SECURITY\", \"pattern\": [{\"LOWER\": \"ip\"}, {\"LOWER\": \"filter\"}]},\n",
        "{\"label\": \"SECURITY\", \"pattern\": [{\"LOWER\": \"geo\"}, {\"LOWER\": \"block\"}]},\n",
        "{\"label\": \"SECURITY\", \"pattern\": [{\"LOWER\": \"brute\"}, {\"LOWER\": \"force\"}]},\n",
        "{\"label\": \"SECURITY\", \"pattern\": [{\"LOWER\": \"captcha\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": \"JumpCloud\"},\n",
        "{\"label\": \"TOOL\", \"pattern\": \"Meraki\"},\n",
        "{\"label\": \"TOOL\", \"pattern\": \"LastPass\"},\n",
        "{\"label\": \"TOOL\", \"pattern\": \"Bitwarden\"},\n",
        "{\"label\": \"TOOL\", \"pattern\": \"Azure\"},\n",
        "{\"label\": \"TOOL\", \"pattern\": \"Cisco\"},\n",
        "{\"label\": \"TOOL\", \"pattern\": \"Fortigate\"},\n",
        "{\"label\": \"ROLE\", \"pattern\": \"IT support\"},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"network\"}, {\"LOWER\": \"admin\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": \"Jira\"},\n",
        "{\"label\": \"TOOL\", \"pattern\": \"ServiceNow\"},\n",
        "{\"label\": \"TOOL\", \"pattern\": \"Firewall Register\"},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"ict\"}, {\"LOWER\": \"manager\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"security\"}, {\"LOWER\": \"director\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"support\"}, {\"LOWER\": \"director\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"senior\"}, {\"LOWER\": \"management\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": \"partner\"},\n",
        "{\"label\": \"ORG\", \"pattern\": \"Gamma\"},\n",
        "{\"label\": \"ORG\", \"pattern\": \"Converged Communication Solutions\"},\n",
        "{\"label\": \"ORG\", \"pattern\": \"Simblox Technologies\"},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"intune\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"jamf\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"rmm\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"sccm\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"datto\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"endpoint\"}, {\"LOWER\": \"central\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"manageengine\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"crowdstrike\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"tanium\"}]},\n",
        "{\"label\": \"PROCESS\", \"pattern\": [{\"LOWER\": \"standard\"}, {\"LOWER\": \"build\"}]},\n",
        "{\"label\": \"PROCESS\", \"pattern\": [{\"LOWER\": \"remove\"}, {\"LOWER\": \"bloatware\"}]},\n",
        "{\"label\": \"PROCESS\", \"pattern\": [{\"LOWER\": \"preinstalled\"}, {\"LOWER\": \"software\"}]},\n",
        "{\"label\": \"CLOUD\", \"pattern\": [{\"LOWER\": \"microsoft\"}, {\"LOWER\": \"365\"}]},\n",
        "{\"label\": \"CLOUD\", \"pattern\": [{\"LOWER\": \"cloud\"}, {\"LOWER\": \"services\"}]},\n",
        "{\"label\": \"CLOUD\", \"pattern\": [{\"LOWER\": \"untick\"}, {\"LOWER\": \"services\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"intune\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"jamf\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"rmm\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"sccm\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"datto\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"endpoint\"}, {\"LOWER\": \"central\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"manageengine\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"crowdstrike\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"tanium\"}]},\n",
        "{\"label\": \"PROCESS\", \"pattern\": [{\"LOWER\": \"standard\"}, {\"LOWER\": \"build\"}]},\n",
        "{\"label\": \"PROCESS\", \"pattern\": [{\"LOWER\": \"bloatware\"}, {\"LOWER\": \"removed\"}]},\n",
        "{\"label\": \"PROCESS\", \"pattern\": [{\"LOWER\": \"reviewed\"}, {\"LOWER\": \"quarterly\"}]},\n",
        "{\"label\": \"CLOUD\", \"pattern\": [{\"LOWER\": \"microsoft\"}, {\"LOWER\": \"365\"}, {\"LOWER\": \"admin\"}, {\"LOWER\": \"portal\"}]},\n",
        "{\"label\": \"CLOUD\", \"pattern\": [{\"LOWER\": \"cloud\"}, {\"LOWER\": \"services\"}]},\n",
        "{\"label\": \"CLOUD\", \"pattern\": [{\"LOWER\": \"cloud\"}, {\"LOWER\": \"applications\"}]},\n",
        "{\"label\": \"CLOUD\", \"pattern\": [{\"LOWER\": \"untick\"}, {\"LOWER\": \"services\"}]},\n",
        "{\"label\": \"AUTH\", \"pattern\": [{\"LOWER\": \"biometric\"}]},\n",
        "{\"label\": \"AUTH\", \"pattern\": [{\"LOWER\": \"fingerprint\"}]},\n",
        "{\"label\": \"AUTH\", \"pattern\": [{\"LOWER\": \"faceid\"}]},\n",
        "{\"label\": \"AUTH\", \"pattern\": [{\"LOWER\": \"windows\"}, {\"LOWER\": \"hello\"}]},\n",
        "{\"label\": \"AUTH\", \"pattern\": [{\"LOWER\": \"touch\"}, {\"LOWER\": \"id\"}]},\n",
        "{\"label\": \"AUTH\", \"pattern\": [{\"LOWER\": \"yubikey\"}]},\n",
        "{\"label\": \"AUTH\", \"pattern\": [{\"LOWER\": \"password\"}]},\n",
        "{\"label\": \"AUTH\", \"pattern\": [{\"LOWER\": \"passphrase\"}]},\n",
        "{\"label\": \"AUTH\", \"pattern\": [{\"LOWER\": \"pin\"}]},\n",
        "{\"label\": \"SECURITY\", \"pattern\": [{\"LOWER\": \"mfa\"}]},\n",
        "{\"label\": \"SECURITY\", \"pattern\": [{\"LOWER\": \"2fa\"}]},\n",
        "{\"label\": \"REQUIREMENT\", \"pattern\": [{\"LOWER\": \"physical\"}, {\"LOWER\": \"presence\"}]},\n",
        "{\"label\": \"REQUIREMENT\", \"pattern\": [{\"LOWER\": \"minimum\"}, {\"LOWER\": \"password\"}, {\"LOWER\": \"length\"}]},\n",
        "{\"label\": \"REQUIREMENT\", \"pattern\": [{\"LOWER\": \"account\"}, {\"LOWER\": \"lockout\"}]},\n",
        "{\"label\": \"REQUIREMENT\", \"pattern\": [{\"LOWER\": \"deny\"}, {\"LOWER\": \"list\"}]},\n",
        "{\"label\": \"REQUIREMENT\", \"pattern\": [{\"LOWER\": \"password\"}, {\"LOWER\": \"complexity\"}]},\n",
        "{\"label\": \"REQUIREMENT\", \"pattern\": [{\"LOWER\": \"unlock\"}, {\"LOWER\": \"device\"}]},\n",
        "{\"label\": \"BROWSER\", \"pattern\": [{\"LOWER\": \"chrome\"}]},\n",
        "{\"label\": \"BROWSER\", \"pattern\": [{\"LOWER\": \"google\"}, {\"LOWER\": \"chrome\"}]},\n",
        "{\"label\": \"BROWSER\", \"pattern\": [{\"LOWER\": \"safari\"}]},\n",
        "{\"label\": \"BROWSER\", \"pattern\": [{\"LOWER\": \"edge\"}]},\n",
        "{\"label\": \"BROWSER\", \"pattern\": [{\"LOWER\": \"microsoft\"}, {\"LOWER\": \"edge\"}]},\n",
        "{\"label\": \"BROWSER\", \"pattern\": [{\"LOWER\": \"firefox\"}]},\n",
        "{\"label\": \"BROWSER\", \"pattern\": [{\"LOWER\": \"mozilla\"}, {\"LOWER\": \"firefox\"}]},\n",
        "{\"label\": \"BROWSER\", \"pattern\": [{\"LOWER\": \"opera\"}]},\n",
        "{\"label\": \"BROWSER\", \"pattern\": [{\"LOWER\": \"chromium\"}]},\n",
        "{\"label\": \"BROWSER\", \"pattern\": [{\"LOWER\": \"samsung\"}, {\"LOWER\": \"internet\"}]},\n",
        "{\"label\": \"VERSION\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"^(\\\\d{2,3})(\\\\.\\\\d+)+$\"}}]},\n",
        "{\"label\": \"VERSION\", \"pattern\": [{\"LOWER\": \"version\"}, {\"TEXT\": {\"REGEX\": \"^(\\\\d{2,3})(\\\\.\\\\d+)+$\"}}]},\n",
        "{\"label\": \"ANTIVIRUS\", \"pattern\": [{\"LOWER\": \"windows\"}, {\"LOWER\": \"defender\"}]},\n",
        "{\"label\": \"ANTIVIRUS\", \"pattern\": [{\"LOWER\": \"bitdefender\"}]},\n",
        "{\"label\": \"ANTIVIRUS\", \"pattern\": [{\"LOWER\": \"eset\"}]},\n",
        "{\"label\": \"ANTIVIRUS\", \"pattern\": [{\"LOWER\": \"sophos\"}]},\n",
        "{\"label\": \"ANTIVIRUS\", \"pattern\": [{\"LOWER\": \"sentinelone\"}]},\n",
        "{\"label\": \"ANTIVIRUS\", \"pattern\": [{\"LOWER\": \"crowdstrike\"}]},\n",
        "{\"label\": \"ANTIVIRUS\", \"pattern\": [{\"LOWER\": \"mcafee\"}]},\n",
        "{\"label\": \"ANTIVIRUS\", \"pattern\": [{\"LOWER\": \"clamav\"}]},\n",
        "{\"label\": \"ANTIVIRUS\", \"pattern\": [{\"LOWER\": \"norton\"}]},\n",
        "{\"label\": \"ANTIVIRUS\", \"pattern\": [{\"LOWER\": \"xprotect\"}]},\n",
        "{\"label\": \"VERSION\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"^\\\\d{1,3}(\\\\.\\\\d+){1,4}$\"}}]},\n",
        "{\"label\": \"VERSION\", \"pattern\": [{\"LOWER\": \"version\"}, {\"TEXT\": {\"REGEX\": \"^\\\\d{1,3}(\\\\.\\\\d+){1,4}$\"}}]},\n",
        "{\"label\": \"EMAIL_APP\", \"pattern\": [{\"LOWER\": \"outlook\"}]},\n",
        "{\"label\": \"EMAIL_APP\", \"pattern\": [{\"LOWER\": \"gmail\"}]},\n",
        "{\"label\": \"EMAIL_APP\", \"pattern\": [{\"LOWER\": \"exchange\"}]},\n",
        "{\"label\": \"EMAIL_APP\", \"pattern\": [{\"LOWER\": \"office\"}, {\"LOWER\": \"365\"}]},\n",
        "{\"label\": \"EMAIL_APP\", \"pattern\": [{\"LOWER\": \"microsoft\"}, {\"LOWER\": \"365\"}]},\n",
        "{\"label\": \"EMAIL_APP\", \"pattern\": [{\"LOWER\": \"google\"}, {\"LOWER\": \"workspace\"}]},\n",
        "{\"label\": \"EMAIL_APP\", \"pattern\": [{\"LOWER\": \"superhuman\"}]},\n",
        "{\"label\": \"EMAIL_APP\", \"pattern\": [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"mail\"}]},\n",
        "{\"label\": \"VERSION\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"^\\\\d{1,3}(\\\\.\\\\d+){1,4}$\"}}]},\n",
        "{\"label\": \"VERSION\", \"pattern\": [{\"LOWER\": \"version\"}, {\"TEXT\": {\"REGEX\": \"^\\\\d{1,3}(\\\\.\\\\d+){1,4}$\"}}]},\n",
        "{\"label\": \"OFFICE_APP\", \"pattern\": [{\"LOWER\": \"microsoft\"}, {\"LOWER\": \"365\"}]},\n",
        "{\"label\": \"OFFICE_APP\", \"pattern\": [{\"LOWER\": \"office\"}, {\"LOWER\": \"365\"}]},\n",
        "{\"label\": \"OFFICE_APP\", \"pattern\": [{\"LOWER\": \"google\"}, {\"LOWER\": \"workspace\"}]},\n",
        "{\"label\": \"OFFICE_APP\", \"pattern\": [{\"LOWER\": \"libreoffice\"}]},\n",
        "{\"label\": \"OFFICE_APP\", \"pattern\": [{\"LOWER\": \"office\"}, {\"LOWER\": \"2016\"}]},\n",
        "{\"label\": \"OFFICE_APP\", \"pattern\": [{\"LOWER\": \"office\"}, {\"LOWER\": \"2019\"}]},\n",
        "{\"label\": \"OFFICE_APP\", \"pattern\": [{\"LOWER\": \"office\"}, {\"LOWER\": \"2021\"}]},\n",
        "{\"label\": \"OFFICE_APP\", \"pattern\": [{\"LOWER\": \"office\"}, {\"LOWER\": \"2024\"}]},\n",
        "{\"label\": \"OFFICE_APP\", \"pattern\": [{\"LOWER\": \"adobe\"}, {\"LOWER\": \"creative\"}, {\"LOWER\": \"cloud\"}]},\n",
        "{\"label\": \"OFFICE_APP\", \"pattern\": [{\"LOWER\": \"affinity\"}, {\"LOWER\": \"publisher\"}]},\n",
        "{\"label\": \"OFFICE_APP\", \"pattern\": [{\"LOWER\": \"netdocuments\"}]},\n",
        "{\"label\": \"VERSION\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"^v?\\\\d{2,4}(\\\\.\\\\d+)*$\"}}]},\n",
        "{\"label\": \"VERSION\", \"pattern\": [{\"LOWER\": \"version\"}, {\"TEXT\": {\"REGEX\": \"^\\\\d{2,4}(\\\\.\\\\d+)*$\"}}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"manual\"}, {\"LOWER\": \"updates\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"patching\"}, {\"LOWER\": \"schedule\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"auto\"}, {\"LOWER\": \"updates\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"weekly\"}, {\"LOWER\": \"updates\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"firmware\"}, {\"LOWER\": \"updates\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"intune\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"datto\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"rmm\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"connectwise\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"patchmypc\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"solarwinds\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"appcheck\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"helpdesk\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"manual\"}, {\"LOWER\": \"intervention\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"manual\"}, {\"LOWER\": \"update\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"patch\"}, {\"LOWER\": \"management\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"ticket\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"helpdesk\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"engineer\"}, {\"LOWER\": \"assigned\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"remote\"}, {\"LOWER\": \"session\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"14\"}, {\"LOWER\": \"days\"}]},\n",
        "{\"label\": \"SECURITY_PROCESS\", \"pattern\": [{\"LOWER\": \"update\"}, {\"LOWER\": \"installed\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"intune\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"datto\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"rmm\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"connectwise\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"patchmypc\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"crowdstrike\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"jamf\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"cyrisma\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"manageengine\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"wsus\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"noc\"}, {\"LOWER\": \"team\"}]},\n",
        "{\"label\": \"SECURITY_POLICY\", \"pattern\": [{\"LOWER\": \"no\"}, {\"LOWER\": \"unsupported\"}, {\"LOWER\": \"software\"}]},\n",
        "{\"label\": \"SECURITY_POLICY\", \"pattern\": [{\"LOWER\": \"unsupported\"}, {\"LOWER\": \"software\"}, {\"LOWER\": \"removed\"}]},\n",
        "{\"label\": \"SEGREGATION_METHOD\", \"pattern\": [{\"LOWER\": \"vlan\"}]},\n",
        "{\"label\": \"SEGREGATION_METHOD\", \"pattern\": [{\"LOWER\": \"sub\"}, {\"LOWER\": \"set\"}]},\n",
        "{\"label\": \"SEGREGATION_METHOD\", \"pattern\": [{\"LOWER\": \"firewall\"}]},\n",
        "{\"label\": \"SEGREGATION_METHOD\", \"pattern\": [{\"LOWER\": \"segregated\"}]},\n",
        "{\"label\": \"SEGREGATION_METHOD\", \"pattern\": [{\"LOWER\": \"proxy\"}]},\n",
        "{\"label\": \"SEGREGATION_METHOD\", \"pattern\": [{\"LOWER\": \"network\"}, {\"LOWER\": \"separation\"}]},\n",
        "{\"label\": \"SEGREGATION_METHOD\", \"pattern\": [{\"LOWER\": \"no\"}, {\"LOWER\": \"internet\"}, {\"LOWER\": \"access\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"jamf\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"intune\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"datto\"}, {\"LOWER\": \"rmm\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"soe\"}]},\n",
        "{\"label\": \"UPDATE_TOOL\", \"pattern\": [{\"LOWER\": \"software\"}, {\"LOWER\": \"register\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"hr\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"director\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"ceo\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"manager\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"line\"}, {\"LOWER\": \"manager\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"founder\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"operations\"}, {\"LOWER\": \"director\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"engineering\"}, {\"LOWER\": \"lead\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"cab\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"coo\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"servicedesk\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"jira\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"personio\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"jumpcloud\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"microsoft\"}, {\"LOWER\": \"admin\"}, {\"LOWER\": \"centre\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"mdm\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"intune\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"sword\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"eskimo\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"adaxes\"}]},\n",
        "{\"label\": \"SECURITY_MEASURE\", \"pattern\": [{\"LOWER\": \"mfa\"}]},\n",
        "{\"label\": \"SECURITY_MEASURE\", \"pattern\": [{\"LOWER\": \"multi\"}, {\"LOWER\": \"factor\"}, {\"LOWER\": \"authentication\"}]},\n",
        "{\"label\": \"SECURITY_MEASURE\", \"pattern\": [{\"LOWER\": \"2fa\"}]},\n",
        "{\"label\": \"SECURITY_MEASURE\", \"pattern\": [{\"LOWER\": \"rate\"}, {\"LOWER\": \"limit\"}]},\n",
        "{\"label\": \"SECURITY_MEASURE\", \"pattern\": [{\"LOWER\": \"throttling\"}]},\n",
        "{\"label\": \"SECURITY_MEASURE\", \"pattern\": [{\"LOWER\": \"lockout\"}, {\"LOWER\": \"policy\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"smart\"}, {\"LOWER\": \"lockout\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"entra\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"bitdefender\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"azure\"}, {\"LOWER\": \"ad\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"defender\"}]},\n",
        "{\"label\": \"SECURITY_MEASURE\", \"pattern\": [{\"LOWER\": \"mfa\"}]},\n",
        "{\"label\": \"SECURITY_MEASURE\", \"pattern\": [{\"LOWER\": \"multi\"}, {\"LOWER\": \"factor\"}, {\"LOWER\": \"authentication\"}]},\n",
        "{\"label\": \"SECURITY_MEASURE\", \"pattern\": [{\"LOWER\": \"deny\"}, {\"LOWER\": \"list\"}]},\n",
        "{\"label\": \"SECURITY_MEASURE\", \"pattern\": [{\"LOWER\": \"automatic\"}, {\"LOWER\": \"blocking\"}, {\"LOWER\": \"of\"}, {\"LOWER\": \"common\"}, {\"LOWER\": \"passwords\"}]},\n",
        "{\"label\": \"SECURITY_MEASURE\", \"pattern\": [{\"LOWER\": \"password\"}, {\"LOWER\": \"length\"}]},\n",
        "{\"label\": \"POLICY\", \"pattern\": [{\"LOWER\": \"password\"}, {\"LOWER\": \"policy\"}]},\n",
        "{\"label\": \"POLICY\", \"pattern\": [{\"LOWER\": \"password\"}, {\"LOWER\": \"history\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"entra\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"intune\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"jumpcloud\"}]},\n",
        "{\"label\": \"SECURITY_PRACTICE\", \"pattern\": [{\"LOWER\": \"three\"}, {\"LOWER\": \"random\"}, {\"LOWER\": \"words\"}]},\n",
        "{\"label\": \"SECURITY_PRACTICE\", \"pattern\": [{\"LOWER\": \"password\"}, {\"LOWER\": \"manager\"}]},\n",
        "{\"label\": \"SECURITY_PRACTICE\", \"pattern\": [{\"LOWER\": \"avoid\"}, {\"LOWER\": \"common\"}, {\"LOWER\": \"passwords\"}]},\n",
        "{\"label\": \"SECURITY_PRACTICE\", \"pattern\": [{\"LOWER\": \"password\"}, {\"LOWER\": \"training\"}]},\n",
        "{\"label\": \"SECURITY_PRACTICE\", \"pattern\": [{\"LOWER\": \"password\"}, {\"LOWER\": \"advice\"}]},\n",
        "{\"label\": \"SECURITY_PRACTICE\", \"pattern\": [{\"LOWER\": \"no\"}, {\"LOWER\": \"regular\"}, {\"LOWER\": \"expiry\"}]},\n",
        "{\"label\": \"SECURITY_PRACTICE\", \"pattern\": [{\"LOWER\": \"not\"}, {\"LOWER\": \"enforcing\"}, {\"LOWER\": \"complexity\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"authenticator\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"google\"}, {\"LOWER\": \"authenticator\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"ms\"}, {\"LOWER\": \"authenticator\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"windows\"}, {\"LOWER\": \"authenticator\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"proton\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"dropbox\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"saml\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"entra\"}, {\"LOWER\": \"id\"}]},\n",
        "{\"label\": \"SERVICE\", \"pattern\": [{\"LOWER\": \"proofpoint\"}]},\n",
        "{\"label\": \"SERVICE\", \"pattern\": [{\"LOWER\": \"homemaster\"}]},\n",
        "{\"label\": \"SERVICE\", \"pattern\": [{\"LOWER\": \"dropbox\"}]},\n",
        "{\"label\": \"SERVICE\", \"pattern\": [{\"LOWER\": \"proton\"}]},\n",
        "{\"label\": \"SERVICE\", \"pattern\": [{\"LOWER\": \"google\"}, {\"LOWER\": \"workspace\"}]},\n",
        "{\"label\": \"SERVICE\", \"pattern\": [{\"LOWER\": \"microsoft\"}, {\"LOWER\": \"365\"}]},\n",
        "{\"label\": \"PROCESS\", \"pattern\": [{\"LOWER\": \"leaver\"}, {\"LOWER\": \"process\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"hr\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"line\"}, {\"LOWER\": \"manager\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"servicenow\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"helpdesk\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"intune\"}]},\n",
        "{\"label\": \"ACTION\", \"pattern\": [{\"LOWER\": \"disable\"}, {\"LOWER\": \"account\"}]},\n",
        "{\"label\": \"ACTION\", \"pattern\": [{\"LOWER\": \"delete\"}, {\"LOWER\": \"account\"}]},\n",
        "{\"label\": \"POLICY\", \"pattern\": [{\"LOWER\": \"least\"}, {\"LOWER\": \"privilege\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"active\"}, {\"LOWER\": \"directory\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"rbac\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"google\"}, {\"LOWER\": \"workspace\"}, {\"LOWER\": \"groups\"}]},\n",
        "{\"label\": \"PROCESS\", \"pattern\": [{\"LOWER\": \"access\"}, {\"LOWER\": \"review\"}]},\n",
        "{\"label\": \"PROCESS\", \"pattern\": [{\"LOWER\": \"role\"}, {\"LOWER\": \"change\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"line\"}, {\"LOWER\": \"manager\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"helpdesk\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"itsm\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"it\"}, {\"LOWER\": \"manager\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"director\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"head\"}, {\"LOWER\": \"of\"}, {\"LOWER\": \"it\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"ciso\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"servicenow\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"jira\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"helpdesk\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"laps\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"privileged\"}, {\"LOWER\": \"identity\"}, {\"LOWER\": \"management\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"laps\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"intune\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"pim\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"entra\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"jamf\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"syncro\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"threatlocker\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"group\"}, {\"LOWER\": \"policy\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"head\"}, {\"LOWER\": \"of\"}, {\"LOWER\": \"technology\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"it\"}, {\"LOWER\": \"manager\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"threatlocker\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"pim\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"jamf\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"syncro\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"entra\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"laps\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"firewall\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"it\"}, {\"LOWER\": \"helpdesk\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"cyber\"}, {\"LOWER\": \"trainer\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"head\"}, {\"LOWER\": \"of\"}, {\"LOWER\": \"technology\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"head\"}, {\"LOWER\": \"of\"}, {\"LOWER\": \"operations\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"it\"}, {\"LOWER\": \"helpdesk\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"azure\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"pim\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"rmm\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"psa\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"jumpcloud\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"preside\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"entra\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"whatfix\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"sentinel\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"gsd\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"google\"}, {\"LOWER\": \"alerts\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"password\"}, {\"LOWER\": \"reset\"}]},\n",
        "{\"label\": \"TOOL\", \"pattern\": [{\"LOWER\": \"mfa\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"group\"}, {\"LOWER\": \"it\"}]},\n",
        "{\"label\": \"ROLE\", \"pattern\": [{\"LOWER\": \"global\"}, {\"LOWER\": \"service\"}, {\"LOWER\": \"desk\"}]},\n",
        "{\"label\": \"PROCESS\", \"pattern\": [{\"LOWER\": \"lockout\"}]},\n",
        "{\"label\": \"PROCESS\", \"pattern\": [{\"LOWER\": \"account\"}, {\"LOWER\": \"recovery\"}]},\n",
        "{\"label\": \"PROCESS\", \"pattern\": [{\"LOWER\": \"self\"}, {\"LOWER\": \"service\"}, {\"LOWER\": \"reset\"}]},\n",
        "{\"label\": \"SCOPE\", \"pattern\": [{\"LOWER\": \"cssf\"}, {\"LOWER\": \"network\"}]},\n",
        "{\"label\": \"SCOPE\", \"pattern\": [{\"LOWER\": \"production\"}, {\"LOWER\": \"network\"}]},\n",
        "{\"label\": \"SCOPE\", \"pattern\": [{\"LOWER\": \"nairobi\"}, {\"LOWER\": \"network\"}]},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0mmCVAB1GUW"
      },
      "source": [
        "### Semantic Frames Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fORdkP19Ax3",
        "outputId": "c4f2ee01-3720-4443-9ef1-caa5bdc27f33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing semantic_rules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile semantic_rules.py\n",
        "import spacy\n",
        "from global_patterns import global_patterns\n",
        "from typing import Dict\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "ruler.add_patterns(global_patterns)\n",
        "\n",
        "#  Per-question rule registry\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# A2.10  ―\n",
        "# --------------------------------------------------------------------------- #\n",
        "def rule_a2_10(text):\n",
        "    import re\n",
        "    doc = nlp(text)\n",
        "    frame = {\n",
        "        \"actor\": None,\n",
        "        \"role\": None,\n",
        "        \"compliant\": False,\n",
        "        \"needs_more_info\": False,\n",
        "        \"reason\":             \"\"\n",
        "\n",
        "    }\n",
        "\n",
        "    # --- Try to extract name before 'notes:' (allow lower/upper case) ---\n",
        "    match = re.match(r\"([a-zA-Z ,.'-]+)\\s*notes:\", text, re.IGNORECASE)\n",
        "    if match:\n",
        "        possible_name = match.group(1).strip()\n",
        "        # Accept if it looks like a person (has at least two \"words\")\n",
        "        if len(possible_name.split()) >= 2:\n",
        "            frame[\"actor\"] = possible_name\n",
        "\n",
        "    # --- Use NER for name (if not already found) ---\n",
        "    if not frame[\"actor\"]:\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == \"PERSON\":\n",
        "                frame[\"actor\"] = ent.text.strip()\n",
        "                break\n",
        "\n",
        "    # --- Fallback: any two-word pattern anywhere (case-insensitive) ---\n",
        "    if not frame[\"actor\"]:\n",
        "        match = re.search(r\"\\b([a-zA-Z]+ [a-zA-Z]+)\\b\", text)\n",
        "        if match:\n",
        "            frame[\"actor\"] = match.group(1).strip()\n",
        "\n",
        "    # --- Role Extraction (NER then fallback) ---\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"ROLE\" and not frame[\"role\"]:\n",
        "            frame[\"role\"] = ent.text.strip()\n",
        "\n",
        "    if not frame[\"role\"]:\n",
        "        match = re.search(r\"role:\\s*([a-zA-Z0-9 &\\-/()]+)\", text)\n",
        "        if match:\n",
        "            frame[\"role\"] = match.group(1).strip()\n",
        "\n",
        "    # --- Compliance logic ---\n",
        "    lower = text.lower()\n",
        "    if frame[\"actor\"] and frame[\"role\"]:\n",
        "        frame[\"compliant\"] = True\n",
        "    elif re.search(r\"(security reason|provided.*separately|phone call|central pool|iasme|not provid|see comment)\", lower):\n",
        "        frame[\"compliant\"] = False\n",
        "        frame[\"needs_more_info\"] = True\n",
        "    else:\n",
        "        frame[\"compliant\"] = False\n",
        "        frame[\"needs_more_info\"] = True\n",
        "    # --- Reason assignment ---\n",
        "    if frame[\"compliant\"]:\n",
        "        frame[\"reason\"] = (\n",
        "            f\"Compliant: Name ('{frame['actor']}') and role ('{frame['role']}') identified.\"\n",
        "        )\n",
        "    elif not frame[\"actor\"] and not frame[\"role\"]:\n",
        "        frame[\"reason\"] = \"Needs more information: Neither responsible person's name nor role identified.\"\n",
        "    elif not frame[\"actor\"]:\n",
        "        frame[\"reason\"] = \"Needs more information: Responsible person's name not identified.\"\n",
        "    elif not frame[\"role\"]:\n",
        "        frame[\"reason\"] = \"Needs more information: Responsible person's role not identified.\"\n",
        "    elif re.search(r\"(security reason|provided.*separately|phone call|central pool|iasme|not provid|see comment)\", lower):\n",
        "        frame[\"reason\"] = \"Needs more information: Applicant deferred answer or referenced out-of-band communication.\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: Incomplete or unclear answer.\"\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# A2.2  ―\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def rule_a2_2(text: str) -> dict:\n",
        "    import re\n",
        "    text_l = text.lower().strip()\n",
        "\n",
        "    frame = {\n",
        "        \"mentions_network_boundary\": False,\n",
        "        \"mentions_named_subnet\": False,\n",
        "        \"mentions_vague_geography\": False,\n",
        "        \"mentions_teams_or_departments\": False,\n",
        "        \"mentions_device_exclusion\": False,\n",
        "        \"mentions_cloud_exclusion\": False,\n",
        "        \"mentions_microsegmentation\": False,\n",
        "        \"mentions_firewall_rule_exc\": False,\n",
        "        \"mentions_vague_only\": False,\n",
        "        \"mentions_company_name_only\": False,\n",
        "        \"mentions_location_only\": False,\n",
        "        \"mentions_product_or_project_only\": False,\n",
        "        \"mentions_all_devices_group\": False,\n",
        "        \"compliant\": False,\n",
        "        \"non_compliant\": False,\n",
        "        \"needs_more_info\": False,\n",
        "        \"reason\": \"\"\n",
        "    }\n",
        "\n",
        "    boundary_patterns = [\n",
        "        r\"\\b(vlans?|subnets?)\\b\",\n",
        "        r\"\\b(layer\\s*[23])\\b\",\n",
        "        r\"\\b(?:firewall|segmented|segregated)\\b\",\n",
        "        r\"\\bscoped\\s+network\\b\",\n",
        "        r\"\\bproduction\\s+network\\b\",\n",
        "        r\"\\bnetwork\\s+segment\\b\",\n",
        "    ]\n",
        "    if any(re.search(p, text_l) for p in boundary_patterns):\n",
        "        frame[\"mentions_network_boundary\"] = True\n",
        "\n",
        "    if re.search(r\"\\b\\w[\\w\\-]{2,40}\\s+(?:network|vlan|subnet|segment|infrastructure|environment)\\b\", text_l):\n",
        "        frame[\"mentions_named_subnet\"] = True\n",
        "\n",
        "    # ---- Improved Vague Geography/Location Handling ----\n",
        "    # Geography (country/region/city) in any position, with or without \"only\"\n",
        "    if re.search(r\"\\b(uk|us|europe|asia|apac|emea|americas|london|birmingham|manchester|new york|paris|tokyo)\\b\", text_l):\n",
        "        frame[\"mentions_vague_geography\"] = True\n",
        "\n",
        "    # Department/team/network exclusion\n",
        "    if re.search(r\"\\b(excluding|except)\\s+(?:finance|hr|sales|marketing|legal|admin|it|support|team|production|development|network|workstations|shore operations|ships|infrastructure)\\b\", text_l):\n",
        "        frame[\"mentions_teams_or_departments\"] = True\n",
        "\n",
        "    # Device-type exclusion\n",
        "    if re.search(r\"\\b(excluding|except)\\s+(?:mobile|phone|tablet|laptop|desktop|server|firewall|router|switch|pc|device)(es)?\\b\", text_l):\n",
        "        frame[\"mentions_device_exclusion\"] = True\n",
        "\n",
        "    # Cloud exclusion\n",
        "    if re.search(r\"\\b(excluding|except)\\s+(?:aws|azure|gcp|cloud[- ]?services?)\\b\", text_l):\n",
        "        frame[\"mentions_cloud_exclusion\"] = True\n",
        "\n",
        "    # Micro-segmentation / ACL / AD\n",
        "    if re.search(r\"\\bmicro[- ]?seg(mentation)?\\b\", text_l) \\\n",
        "       or re.search(r\"\\baccess\\s+control\\s+lists?\\b\", text_l) \\\n",
        "       or re.search(r\"\\bactive\\s+directory\\b\", text_l):\n",
        "        frame[\"mentions_microsegmentation\"] = True\n",
        "\n",
        "    # Excluded by firewall rule\n",
        "    if re.search(r\"\\b(excluding|except).*firewall\\s+rule\", text_l):\n",
        "        frame[\"mentions_firewall_rule_exc\"] = True\n",
        "\n",
        "    # Generic \"... only\" with no boundary\n",
        "    if re.fullmatch(r\"\\s*\\w[\\w\\s\\-]{0,40}\\s+only\\s*\", text_l):\n",
        "        frame[\"mentions_vague_only\"] = True\n",
        "\n",
        "    # All devices, vague group\n",
        "    if re.search(r\"\\ball\\s+(computers?|laptops?|servers?|firewalls?|routers?|it infrastructure)\", text_l):\n",
        "        frame[\"mentions_all_devices_group\"] = True\n",
        "\n",
        "    # Company name only (vague, single entity or Ltd/LLP/Inc etc with no network)\n",
        "    if re.fullmatch(r\".*\\b(ltd|llp|inc|corp|plc|group|company|enterprises?)\\b.*\", text_l) and not frame[\"mentions_network_boundary\"]:\n",
        "        frame[\"mentions_company_name_only\"] = True\n",
        "\n",
        "    # Location only (city, region, address)\n",
        "    if re.fullmatch(r\".*\\b(london|birmingham|manchester|new york|paris|tokyo|[a-zA-Z]+ office)\\b.*\", text_l) and not frame[\"mentions_network_boundary\"]:\n",
        "        frame[\"mentions_location_only\"] = True\n",
        "\n",
        "    # Product/project name only\n",
        "    if re.fullmatch(r\"\\s*system\\s*\\w+\\s*only\\s*\", text_l) or re.fullmatch(r\".*project\\s+\\w+\\s*only.*\", text_l):\n",
        "        frame[\"mentions_product_or_project_only\"] = True\n",
        "\n",
        "    disallowed_flags = [\n",
        "        \"mentions_vague_geography\",\n",
        "        \"mentions_teams_or_departments\",\n",
        "        \"mentions_device_exclusion\",\n",
        "        \"mentions_cloud_exclusion\",\n",
        "        \"mentions_microsegmentation\",\n",
        "        \"mentions_firewall_rule_exc\",\n",
        "        \"mentions_vague_only\",\n",
        "        \"mentions_company_name_only\",\n",
        "        \"mentions_location_only\",\n",
        "        \"mentions_product_or_project_only\",\n",
        "        \"mentions_all_devices_group\",\n",
        "    ]\n",
        "\n",
        "    if frame[\"mentions_network_boundary\"] and not any(frame[f] for f in disallowed_flags):\n",
        "        frame[\"compliant\"] = True\n",
        "    elif any(frame[f] for f in disallowed_flags):\n",
        "        frame[\"non_compliant\"] = True\n",
        "    else:\n",
        "        frame[\"needs_more_info\"] = True\n",
        "\n",
        "    # --- Reason assignment ---\n",
        "    if frame[\"compliant\"]:\n",
        "        frame[\"reason\"] = \"Compliant: Network boundary (e.g. subnet, VLAN, firewall) identified with no disallowed exclusions.\"\n",
        "    elif frame[\"non_compliant\"]:\n",
        "        triggered = [f.replace(\"mentions_\", \"\").replace(\"_\", \" \") for f in disallowed_flags if frame[f]]\n",
        "        frame[\"reason\"] = \"Non-compliant: Disallowed exclusions found – \" + \", \".join(triggered) + \".\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: No valid network boundary or disallowed exclusions identified.\"\n",
        "\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# A2.3  ― “Describe the geographical locations of your business in‑scope”\n",
        "# --------------------------------------------------------------------------- #\n",
        "#   • Accept clear addresses, towns/cities, data‑centres, offices, sites.\n",
        "#   • Flag obviously vague answers (“UK”, “Global”, “Online only”, blank, …).\n",
        "#   • Provide a structured frame so the assessor UI can highlight gaps.\n",
        "# --------------------------------------------------------------------------- #\n",
        "def rule_a2_3(text: str) -> dict:\n",
        "    import re\n",
        "    answer = text.strip()\n",
        "    doc = nlp(answer)\n",
        "\n",
        "    frame = {\n",
        "        \"locations\": [],\n",
        "        \"has_vague_location\": False,\n",
        "        \"mentions_remote\": False,\n",
        "        \"mentions_datacentre\": False,\n",
        "        \"needs_more_info\": True,\n",
        "        \"compliant\": False,\n",
        "        \"reason\": \"\"\n",
        "    }\n",
        "\n",
        "    # 1. spaCy NER\n",
        "    LOC_LABELS = [\"GPE\", \"LOC\", \"FAC\", \"ORG\"]\n",
        "    frame[\"locations\"].extend(\n",
        "        ent.text.strip() for ent in doc.ents if ent.label_ in LOC_LABELS\n",
        "    )\n",
        "\n",
        "    # 2. GeoText city extraction\n",
        "    try:\n",
        "        from geotext import GeoText\n",
        "        geo = GeoText(answer)\n",
        "        frame[\"locations\"].extend(geo.cities)\n",
        "        frame[\"locations\"].extend(geo.country_mentions)  # Adds country names if detected\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    # 3. pycountry for countries\n",
        "    try:\n",
        "        import pycountry\n",
        "        for c in pycountry.countries:\n",
        "            if c.name.lower() in answer.lower() and c.name not in frame[\"locations\"]:\n",
        "                frame[\"locations\"].append(c.name)\n",
        "        # -------- pycountry.subdivisions (UK regions) --------\n",
        "        uk_subdivs = {s.name.lower() for s in pycountry.subdivisions.get(country_code=\"GB\")}\n",
        "        # Add common forms if not present\n",
        "        uk_subdivs |= {\"england\", \"wales\", \"scotland\", \"northern ireland\"}\n",
        "        for subdiv in uk_subdivs:\n",
        "            if subdiv in answer.lower() and subdiv.title() not in frame[\"locations\"]:\n",
        "                frame[\"locations\"].append(subdiv.title())\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "           # --- NEW: Always extract UK subdivisions explicitly ---\n",
        "    uk_subdivisions = [\"england\", \"wales\", \"scotland\", \"northern ireland\"]\n",
        "    for region in uk_subdivisions:\n",
        "        if re.search(rf\"\\b{region}\\b\", answer, re.I):\n",
        "            # Title case for display, e.g., \"England\"\n",
        "            display = region.title() if region != \"northern ireland\" else \"Northern Ireland\"\n",
        "            if display not in frame[\"locations\"]:\n",
        "                frame[\"locations\"].append(display)\n",
        "\n",
        "    # 4. Datacentre/colo regex\n",
        "    dc_match = re.findall(\n",
        "        r\"\\b(?:aws|azure|gcp|equix?|telehouse|datacentre|data\\s*center)\\b[\\w\\-\\. ]*\",\n",
        "        answer, flags=re.I\n",
        "    )\n",
        "    if dc_match:\n",
        "        frame[\"mentions_datacentre\"] = True\n",
        "        frame[\"locations\"].extend([m.strip() for m in dc_match])\n",
        "\n",
        "    # 5. Address-like\n",
        "    street_like = re.findall(\n",
        "        r\"\\b[A-Z][\\w\\s]{2,40}\\s(?:road|street|lane|park|business\\s+park|campus)\\b\",\n",
        "        answer, flags=re.I\n",
        "    )\n",
        "    frame[\"locations\"].extend(map(str.strip, street_like))\n",
        "\n",
        "    # 6. Postcodes (UK style)\n",
        "    postcodes = re.findall(r\"\\b[A-Z]{1,2}\\d{1,2}[A-Z]?\\s*\\d[A-Z]{2}\\b\", answer)\n",
        "    frame[\"locations\"].extend(postcodes)\n",
        "\n",
        "    # 6a. '[place] office|branch|site...'\n",
        "    trailing_place_pat = r\"([A-Za-z][A-Za-z0-9' \\-]{1,60})\\s+(?:head\\s+)?(?:regional\\s+)?(?:office|branch|site|premises|workplace|location|shop|store|factory|warehouse|building|facility|campus)s?\"\n",
        "    for match in re.findall(trailing_place_pat, answer, flags=re.I):\n",
        "        frame[\"locations\"].append(match.strip())\n",
        "\n",
        "    # 6b. 'head office: swansea, wales.' / 'regional office: cork, ireland.'\n",
        "    labelled_office_pat = r\"(?:head|regional|main|uk|us)?\\s*office:\\s*([A-Za-z0-9' ,\\-]+)\"\n",
        "    for match in re.findall(labelled_office_pat, answer, flags=re.I):\n",
        "        parts = [p.strip() for p in match.split(\",\") if p.strip()]\n",
        "        frame[\"locations\"].extend(parts)\n",
        "\n",
        "    labelled_branch_pat = r\"(?:branch|site|campus):\\s*([A-Za-z0-9' ,\\-]+)\"\n",
        "    for match in re.findall(labelled_branch_pat, answer, flags=re.I):\n",
        "        parts = [p.strip() for p in match.split(\",\") if p.strip()]\n",
        "        frame[\"locations\"].extend(parts)\n",
        "\n",
        "    # 6c. 'in/at/from <place>'\n",
        "    loc_phrases = re.findall(r\"\\b(?:in|at|from)\\s+([A-Za-z][A-Za-z' \\-]{1,40})\", answer)\n",
        "    vague_words = {\n",
        "        \"uk\", \"england\", \"scotland\", \"wales\", \"northern ireland\",\n",
        "        \"united kingdom\", \"britain\", \"ireland\", \"europe\", \"emea\", \"global\", \"worldwide\"\n",
        "    }\n",
        "    for loc in loc_phrases:\n",
        "        if loc.strip().lower() not in vague_words:\n",
        "            frame[\"locations\"].append(loc.strip())\n",
        "\n",
        "    # 6d. '[place] and [place]' chained after in/at (e.g., 'dunfermline and forfar')\n",
        "    chain_matches = re.findall(\n",
        "        r\"(?:in|at|from)?\\s*([A-Za-z][A-Za-z' \\-]{1,40})\\s+and\\s+([A-Za-z][A-Za-z' \\-]{1,40})\",\n",
        "        answer, flags=re.I\n",
        "    )\n",
        "    for a, b in chain_matches:\n",
        "        if a.strip().lower() not in vague_words:\n",
        "            frame[\"locations\"].append(a.strip())\n",
        "        if b.strip().lower() not in vague_words:\n",
        "            frame[\"locations\"].append(b.strip())\n",
        "\n",
        "    # 6e. Manual override for common places seen in CE data\n",
        "    known_uk_places = [\n",
        "        \"aberdeen\", \"perthshire\", \"london\", \"manchester\", \"edinburgh\", \"glasgow\",\n",
        "        \"st andrews\", \"dunfermline\", \"forfar\", \"epsom\", \"surrey\", \"winchester\",\n",
        "        \"newbury\", \"berkshire\", \"westhill\", \"newbridge\", \"cork\", \"swansea\"\n",
        "    ]\n",
        "    for place in known_uk_places:\n",
        "        if re.search(r\"\\b{}\\b\".format(re.escape(place)), answer, flags=re.I):\n",
        "            frame[\"locations\"].append(place.title())\n",
        "\n",
        "    # Remove duplicates & blanks\n",
        "    frame[\"locations\"] = sorted(set(l for l in frame[\"locations\"] if l))\n",
        "\n",
        "    # Remote work\n",
        "    if re.search(r\"\\b(remote|home[- ]?working|work\\s+from\\s+home|home\\s+office)\\b\", answer, flags=re.I):\n",
        "        frame[\"mentions_remote\"] = True\n",
        "\n",
        "    # --- VAGUE LOGIC ---\n",
        "    extracted = {l.lower().strip() for l in frame[\"locations\"]}\n",
        "    if not extracted or extracted.issubset(vague_words):\n",
        "        frame[\"has_vague_location\"] = True\n",
        "    else:\n",
        "        frame[\"has_vague_location\"] = False\n",
        "\n",
        "    # --- FINAL COMPLIANCE LOGIC ---\n",
        "    if frame[\"locations\"] and not frame[\"has_vague_location\"]:\n",
        "        frame[\"needs_more_info\"] = False\n",
        "        frame[\"compliant\"] = True\n",
        "    else:\n",
        "        frame[\"needs_more_info\"] = True\n",
        "        frame[\"compliant\"] = False\n",
        "\n",
        "    # --- Reason assignment ---\n",
        "    if frame[\"compliant\"]:\n",
        "        frame[\"reason\"] = \"Compliant: Specific location(s) identified without vague or global terms.\"\n",
        "    elif frame[\"has_vague_location\"]:\n",
        "        frame[\"reason\"] = \"Needs more information: Only vague or global location terms were found (e.g. UK, Europe).\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: No valid or recognizable location identified.\"\n",
        "\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────\n",
        "# A2.4  ·  Quantities & OS versions for laptops / desktops / VDI\n",
        "#        → raises Auto-Fail if unsupported or pentest OS / hypervisor\n",
        "#        → NOW also checks endoflife.date and applies your EOL policy\n",
        "# ──────────────────────────────────────────────────────────\n",
        "\n",
        "from typing import Dict, List\n",
        "import re, datetime as _dt\n",
        "\n",
        "# -------------------- EOL helpers (A2.4-scoped) --------------------\n",
        "# Free API: https://endoflife.date/api/v1/products/<slug>.json\n",
        "A24_EOL_V1_ENDPOINT = \"https://endoflife.date/api/v1/products/{slug}.json\"\n",
        "A24_EOL_LOOKAHEAD_DAYS = 180\n",
        "\n",
        "# Minimal slug map for user endpoints/devices\n",
        "_A24_EOL_KNOWN_SLUGS = {\n",
        "    \"windows\": \"windows\",\n",
        "    \"windows 10\": \"windows\",\n",
        "    \"windows 11\": \"windows\",\n",
        "    \"macos\": \"macos\",\n",
        "    \"ubuntu\": \"ubuntu\",\n",
        "    \"debian\": \"debian\",\n",
        "    \"rhel\": \"rhel\",\n",
        "    \"red hat enterprise linux\": \"rhel\",\n",
        "    \"centos\": \"centos\",\n",
        "    \"rocky\": \"rocky-linux\",\n",
        "    \"rocky linux\": \"rocky-linux\",\n",
        "    \"alma\": \"almalinux-os\",\n",
        "    \"almalinux\": \"almalinux-os\",\n",
        "    \"android\": \"android\",\n",
        "}\n",
        "\n",
        "def _a24_eol_resolve_slug(product_hint: str) -> str | None:\n",
        "    h = (product_hint or \"\").lower().strip()\n",
        "    if h in _A24_EOL_KNOWN_SLUGS:\n",
        "        return _A24_EOL_KNOWN_SLUGS[h]\n",
        "    for k, v in _A24_EOL_KNOWN_SLUGS.items():\n",
        "        if k in h:\n",
        "            return v\n",
        "    return None\n",
        "\n",
        "def _a24_eol_fetch_cycles(slug: str):\n",
        "    try:\n",
        "        import requests\n",
        "        r = requests.get(A24_EOL_V1_ENDPOINT.format(slug=slug), timeout=12)\n",
        "        if r.ok:\n",
        "            return r.json()\n",
        "    except Exception:\n",
        "        return []\n",
        "    return []\n",
        "\n",
        "def _a24_eol_norm(s: str) -> str:\n",
        "    return (s or \"\").lower().replace(\"ltsc\", \"lts\").replace(\" r2\", \"r2\").strip()\n",
        "\n",
        "def _a24_eol_best_match(cycles: list, version_text: str):\n",
        "    \"\"\"Lightweight fuzzy-ish match without extra deps.\"\"\"\n",
        "    if not cycles or not version_text:\n",
        "        return None\n",
        "    names = [_a24_eol_norm(c.get(\"cycle\",\"\")) for c in cycles]\n",
        "    q = _a24_eol_norm(version_text)\n",
        "    # 1) exact\n",
        "    if q in names:\n",
        "        return cycles[names.index(q)]\n",
        "    # 2) contains either way\n",
        "    for i, n in enumerate(names):\n",
        "        if q in n or n in q:\n",
        "            return cycles[i]\n",
        "    # 3) token overlap heuristic\n",
        "    q_tokens = set(re.findall(r\"[a-z0-9\\.]+\", q))\n",
        "    best_i, best_score = None, 0\n",
        "    for i, n in enumerate(names):\n",
        "        t = set(re.findall(r\"[a-z0-9\\.]+\", n))\n",
        "        if not t:\n",
        "            continue\n",
        "        score = len(q_tokens & t) / len(t)\n",
        "        if score > best_score:\n",
        "            best_score, best_i = score, i\n",
        "    return cycles[best_i] if best_i is not None else None\n",
        "\n",
        "def _a24_eol_status(entry: dict, today=None, lookahead_days=A24_EOL_LOOKAHEAD_DAYS):\n",
        "    today = today or _dt.date.today()\n",
        "    eol = entry.get(\"eol\")\n",
        "    if eol in (True, \"true\"):\n",
        "        return \"EOL\", None\n",
        "    if eol in (False, \"false\", None):\n",
        "        return \"Supported\", None\n",
        "    # parse date (ISO YYYY-MM-DD is typical)\n",
        "    try:\n",
        "        eol_date = _dt.date.fromisoformat(str(eol)[:10])\n",
        "    except Exception:\n",
        "        return \"Unknown\", None\n",
        "    if eol_date <= today:\n",
        "        return \"EOL\", eol_date\n",
        "    if lookahead_days and eol_date <= (today + _dt.timedelta(days=lookahead_days)):\n",
        "        return \"Near EOL\", eol_date\n",
        "    return \"Supported\", eol_date\n",
        "\n",
        "def _a24_eol_check(product_name: str, version_text: str):\n",
        "    slug = _a24_eol_resolve_slug(product_name)\n",
        "    if not slug:\n",
        "        return {\"product\": product_name, \"version\": version_text, \"status\": \"Unknown\"}\n",
        "    cycles = _a24_eol_fetch_cycles(slug)\n",
        "    if not isinstance(cycles, list) or not cycles:\n",
        "        return {\"product\": product_name, \"version\": version_text, \"slug\": slug, \"status\": \"Unknown\"}\n",
        "    entry = _a24_eol_best_match(cycles, version_text)\n",
        "    if not entry:\n",
        "        return {\"product\": product_name, \"version\": version_text, \"slug\": slug, \"status\": \"Unknown\"}\n",
        "    status, eol_date = _a24_eol_status(entry)\n",
        "    return {\n",
        "        \"product\": product_name, \"version\": version_text, \"slug\": slug,\n",
        "        \"cycle\": entry.get(\"cycle\"), \"eol\": entry.get(\"eol\"), \"status\": status,\n",
        "        \"eol_date\": eol_date\n",
        "    }\n",
        "\n",
        "def _a24_pairs_from_os_strings(os_list: List[str]) -> List[tuple]:\n",
        "    \"\"\"Very small normaliser: 'windows 11 pro 24h2' → ('Windows','11 24H2'), etc.\"\"\"\n",
        "    pairs, seen = [], set()\n",
        "    for s in os_list or []:\n",
        "        l = s.lower()\n",
        "        prod, ver = None, None\n",
        "        if \"windows\" in l and \"server\" not in l:\n",
        "            prod = \"Windows\"\n",
        "            m = re.search(r\"\\b(10|11)\\b\", l)\n",
        "            h = re.search(r\"\\b(\\d{2}h\\d)\\b\", l, re.I)\n",
        "            b = re.search(r\"\\bbuild\\s*\\d+\\b\", l)\n",
        "            if m and h:\n",
        "                ver = f\"{m.group(1)} {h.group(1).upper()}\"\n",
        "            elif m and b:\n",
        "                ver = f\"{m.group(1)} {b.group(0)}\".replace(\"build\",\"build\")\n",
        "            elif m:\n",
        "                ver = m.group(1)\n",
        "        elif \"macos\" in l or \"mac os\" in l:\n",
        "            prod = \"macOS\"\n",
        "            v = re.search(r\"\\b(\\d+(?:\\.\\d+){0,2})\\b\", l)\n",
        "            if v:\n",
        "                ver = v.group(1)\n",
        "            else:\n",
        "                name = re.search(r\"\\b(ventura|sonoma|sequoia|monterey|catalina|bigsur|big sur)\\b\", l)\n",
        "                if name:\n",
        "                    ver = name.group(1).title()\n",
        "        elif \"ubuntu\" in l:\n",
        "            prod = \"Ubuntu\"\n",
        "            v = re.search(r\"\\b(\\d{2}\\.\\d{2})\\b\", l)\n",
        "            if v: ver = v.group(1)\n",
        "        elif \"debian\" in l:\n",
        "            prod = \"Debian\"\n",
        "            v = re.search(r\"\\b(\\d+)\\b\", l)\n",
        "            if v: ver = v.group(1)\n",
        "        elif \"rhel\" in l or \"red hat\" in l:\n",
        "            prod = \"RHEL\"\n",
        "            v = re.search(r\"\\b(\\d+)\\b\", l)\n",
        "            if v: ver = v.group(1)\n",
        "        elif \"centos\" in l:\n",
        "            prod = \"CentOS\"\n",
        "            v = re.search(r\"\\b(\\d+)\\b\", l)\n",
        "            if v: ver = v.group(1)\n",
        "        elif \"rocky\" in l:\n",
        "            prod = \"Rocky\"\n",
        "            v = re.search(r\"\\b(\\d+)\\b\", l)\n",
        "            if v: ver = v.group(1)\n",
        "        elif \"alma\" in l:\n",
        "            prod = \"Alma\"\n",
        "            v = re.search(r\"\\b(\\d+)\\b\", l)\n",
        "            if v: ver = v.group(1)\n",
        "        elif \"android\" in l:\n",
        "            prod = \"Android\"\n",
        "            v = re.search(r\"\\b(\\d+(?:\\.\\d+)*)\\b\", l)\n",
        "            if v: ver = v.group(1)\n",
        "        if prod and ver:\n",
        "            key = (prod.lower(), ver.lower())\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                pairs.append((prod, ver))\n",
        "    return pairs\n",
        "\n",
        "# -------------------- Original A2.4 code (unchanged) --------------------\n",
        "DEVICE_TYPE_KEYWORDS = {\n",
        "    \"laptop\": [\"laptop\", \"notebook\", \"macbook\", \"latitude\", \"thinkpad\", \"ideapad\", \"yoga\", \"surface\"],\n",
        "    \"desktop\": [\"desktop\", \"pc\", \"computer\", \"imac\", \"nuc\"],\n",
        "    \"vdi\": [\"vdi\", \"virtual desktop\", \"azure virtual desktop\"],\n",
        "}\n",
        "BRANDS = [\"dell\", \"hp\", \"lenovo\", \"apple\", \"asus\", \"acer\", \"huawei\", \"macbook\", \"imac\", \"nuc\", \"surface\"]\n",
        "\n",
        "def infer_device_type(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    if any(x in text for x in [\"laptop\", \"notebook\", \"macbook\", \"ideapad\", \"yoga\", \"surface\"]):\n",
        "        return \"laptop\"\n",
        "    if any(x in text for x in [\"desktop\", \"pc\", \"imac\", \"nuc\"]):\n",
        "        return \"desktop\"\n",
        "    if \"virtual\" in text or \"vdi\" in text:\n",
        "        return \"vdi\"\n",
        "    for brand in BRANDS:\n",
        "        if brand in text:\n",
        "            if brand in {\"macbook\", \"latitude\", \"thinkpad\", \"ideapad\", \"yoga\", \"surface\"}:\n",
        "                return \"laptop\"\n",
        "            if brand in {\"imac\", \"nuc\"}:\n",
        "                return \"desktop\"\n",
        "            return \"other\"\n",
        "    return \"other\"\n",
        "\n",
        "def extract_device_counts(answer: str):\n",
        "    version_spans = []\n",
        "    for m in re.finditer(r'\\d+\\.\\d+(?:\\.\\d+)?', answer):\n",
        "        version_spans.append((m.start(), m.end()))\n",
        "    pattern = re.compile(r'(\\d+)\\s*(?:x)?\\s*([\\w\\- ]+)', re.I)\n",
        "    counts = {\"laptop\": 0, \"desktop\": 0, \"vdi\": 0, \"other\": 0}\n",
        "    device_count_total = 0\n",
        "    for m in pattern.finditer(answer):\n",
        "        n = int(m.group(1))\n",
        "        start, end = m.span(1)\n",
        "        if 2000 <= n <= 2100:\n",
        "            continue\n",
        "        if any(start >= vstart and end <= vend for (vstart, vend) in version_spans):\n",
        "            continue\n",
        "        desc = m.group(2).strip().lower()\n",
        "        dtype = infer_device_type(desc)\n",
        "        counts[dtype] += n\n",
        "        device_count_total += n\n",
        "    return counts, device_count_total\n",
        "\n",
        "def rule_a2_4(text: str) -> Dict[str, object]:\n",
        "    doc = nlp(text)\n",
        "    lowered = text.lower().strip()\n",
        "    frame: Dict[str, object] = {\n",
        "        \"counts\": {\"laptop\": 0, \"desktop\": 0, \"vdi\": 0, \"other\": 0},\n",
        "        \"device_count_total\": 0,\n",
        "        \"os_versions\": [],\n",
        "        \"mentions_byod\": False,\n",
        "        \"mentions_hypervisor\": False,\n",
        "        \"unsupported_os\": False,\n",
        "        \"pentest_os_flag\": False,\n",
        "        \"needs_more_info\": True,\n",
        "        \"auto_fail\": False,\n",
        "        \"compliant\": False,\n",
        "        \"all_devices_supported_os\": False,\n",
        "        \"reason\": \"\"\n",
        "    }\n",
        "\n",
        "    # --- \"attached\" anywhere: immediately compliant ---\n",
        "    if re.search(r\"\\battached\\b\", lowered):\n",
        "        frame[\"compliant\"] = True\n",
        "        frame[\"needs_more_info\"] = False\n",
        "        frame[\"all_devices_supported_os\"] = True\n",
        "        return frame\n",
        "\n",
        "    if re.search(r\"\\b(byod|bring\\s+your\\s+own\\s+device|personal\\s+device)\\b\", lowered):\n",
        "        frame[\"mentions_byod\"] = True\n",
        "\n",
        "    counts, device_count_total = extract_device_counts(lowered)\n",
        "    for k in frame[\"counts\"]:\n",
        "        frame[\"counts\"][k] = counts.get(k, 0)\n",
        "    frame[\"device_count_total\"] = device_count_total\n",
        "\n",
        "    match_all_devices_os = re.search(r\"\\ball\\s+devices\\s+(?:are|running|use)\\s+([\\w\\s\\d\\.]+)\", lowered)\n",
        "    if match_all_devices_os:\n",
        "        os_guess = match_all_devices_os.group(1).strip()\n",
        "        if os_guess:\n",
        "            frame[\"all_devices_supported_os\"] = True\n",
        "            frame[\"os_versions\"].append(os_guess)\n",
        "\n",
        "    # --- OS / Version extraction ---\n",
        "    rx_bank: List[str] = [\n",
        "        r\"(windows\\s+(?:10|11)\\s+(?:pro|professional|enterprise|home|education|business)\\s*(?:version\\s*)?(?:\\d{2,4}h\\d|\\d{4}|build\\s*\\d+))\",\n",
        "        r\"(windows\\s+server\\s+\\d{4}\\s*(?:standard|datacentre|essentials)?)\",\n",
        "        r\"(windows\\s+(?:vista|xp|7|8(?:\\.1)?))\",\n",
        "        r\"(windows\\s+\\d+\\.\\d+\\.\\d+)\",\n",
        "        r\"(mac\\s?os\\s+[a-z\\s]+(?:\\d+\\.\\d+(?:\\.\\d+)?)?)\",\n",
        "        r\"(opensuse\\s+leap\\s+\\d+\\.\\d+)\",\n",
        "        r\"(mageia\\s+linux\\s+\\d+)\",\n",
        "        r\"(ubuntu\\s+\\d{2}\\.\\d{2}(?:\\s+lts)?)\",\n",
        "        r\"(debian\\s+\\d+)\",\n",
        "        r\"(centos\\s+(?:stream\\s+)?\\d+)\",\n",
        "        r\"(android\\s+\\d+)\",\n",
        "        r\"(kali\\s+linux|parrot\\s+os)\",\n",
        "        r\"(esxi\\s+\\d+(?:\\.\\d+)?|hyper[-\\s]?v|vmware\\s+v?sphere|virtualbox)\",\n",
        "    ]\n",
        "    os_found: List[str] = []\n",
        "    for rx in rx_bank:\n",
        "        os_found += re.findall(rx, lowered)\n",
        "\n",
        "    # --- Standalone version numbers ---\n",
        "    os_found += re.findall(r'\\b\\d+\\.\\d+\\.\\d+\\b', lowered)\n",
        "    os_found += re.findall(r'\\b\\d+h\\d+\\b', lowered)\n",
        "    os_found += re.findall(r'build\\s*[-: ]*\\d+', lowered)\n",
        "    os_found += re.findall(r'\\(\\s*\\d+\\s*\\)', lowered)\n",
        "    os_found += re.findall(r'\\b\\d{5}\\b', lowered)\n",
        "\n",
        "    # spaCy mac product names\n",
        "    mac_names = {\"ventura\", \"sonoma\", \"sequoia\", \"monterey\", \"big sur\", \"catalina\"}\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PRODUCT\" and ent.text.lower() in mac_names:\n",
        "            os_found.append(\"macos \" + ent.text.lower())\n",
        "\n",
        "    # Require edition+version for Windows 10/11\n",
        "    cleaned_os = []\n",
        "    for os in os_found:\n",
        "        if re.match(r'windows\\s+(?:10|11)\\s+(?:pro|professional|enterprise|home|education|business).*?(?:\\d{2,4}h\\d|\\d{4}|build\\s*\\d+)', os):\n",
        "            cleaned_os.append(os)\n",
        "        elif re.match(r'windows\\s+(?:10|11)\\b', os):\n",
        "            continue\n",
        "        else:\n",
        "            cleaned_os.append(os)\n",
        "    frame[\"os_versions\"] = sorted(set(s.strip() for s in cleaned_os + frame[\"os_versions\"] if s.strip()))\n",
        "\n",
        "    unsupported_kw = {\n",
        "        \"windows vista\", \"windows xp\", \"windows 7\", \"windows 8\", \"windows 8.1\",\n",
        "        \"windows 10 home 21h1\", \"windows 10 1607\", \"windows 10 1809\", \"windows 10 1909\",\n",
        "        \"android 9\", \"macos high sierra\", \"macos el capitan\",\n",
        "    }\n",
        "    pentest_kw = {\"kali linux\", \"parrot os\"}\n",
        "\n",
        "    for os_str in frame[\"os_versions\"]:\n",
        "        l = os_str.lower()\n",
        "        if any(bad in l for bad in unsupported_kw):\n",
        "            frame[\"unsupported_os\"] = True\n",
        "            frame[\"auto_fail\"] = True\n",
        "        if any(pen in l for pen in pentest_kw):\n",
        "            frame[\"pentest_os_flag\"] = True\n",
        "            frame[\"auto_fail\"] = True\n",
        "        if re.search(r\"(hyper[-\\s]?v|esxi\\s+\\d|vmware|virtualbox)\", l):\n",
        "            frame[\"mentions_hypervisor\"] = True\n",
        "\n",
        "    if frame[\"device_count_total\"] > 0 and frame[\"os_versions\"] and not frame[\"auto_fail\"]:\n",
        "        frame[\"compliant\"] = True\n",
        "        frame[\"needs_more_info\"] = False\n",
        "    elif frame[\"all_devices_supported_os\"] and not frame[\"auto_fail\"]:\n",
        "        frame[\"compliant\"] = True\n",
        "        frame[\"needs_more_info\"] = False\n",
        "    else:\n",
        "        frame[\"compliant\"] = False\n",
        "        frame[\"needs_more_info\"] = True\n",
        "\n",
        "    # --- Reason assignment (original) ---\n",
        "    if frame[\"compliant\"]:\n",
        "        if frame[\"device_count_total\"] > 0 and frame[\"os_versions\"]:\n",
        "            frame[\"reason\"] = f\"Compliant: Device count ({frame['device_count_total']}) and OS version(s) provided.\"\n",
        "        elif frame[\"all_devices_supported_os\"]:\n",
        "            frame[\"reason\"] = \"Compliant: All devices confirmed to run supported OS.\"\n",
        "    elif frame[\"auto_fail\"]:\n",
        "        if frame[\"unsupported_os\"]:\n",
        "            frame[\"reason\"] = \"Auto-fail: Unsupported operating system detected.\"\n",
        "        elif frame[\"pentest_os_flag\"]:\n",
        "            frame[\"reason\"] = \"Auto-fail: Penetration testing OS detected (e.g. Kali Linux).\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Auto-fail: Critical failure condition triggered.\"\n",
        "    else:\n",
        "        if frame[\"device_count_total\"] == 0:\n",
        "            frame[\"reason\"] = \"Needs more information: No countable user devices found.\"\n",
        "        elif not frame[\"os_versions\"]:\n",
        "            frame[\"reason\"] = \"Needs more information: No valid operating system version identified.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Needs more information: Incomplete information on devices or OS.\"\n",
        "\n",
        "    # -------------------- EOL policy merge (A2.4 helpers) --------------------\n",
        "    if frame[\"os_versions\"]:\n",
        "        pairs = _a24_pairs_from_os_strings(frame[\"os_versions\"])\n",
        "        eol_results = []\n",
        "        for (prod, ver) in pairs:\n",
        "            eol_results.append(_a24_eol_check(prod, ver))\n",
        "\n",
        "        eol_hits = [r for r in eol_results if r.get(\"status\") == \"EOL\"]\n",
        "        near_hits = [r for r in eol_results if r.get(\"status\") == \"Near EOL\"]\n",
        "\n",
        "        def _a24_fmt(rs):\n",
        "            out = []\n",
        "            for r in rs:\n",
        "                cyc = r.get(\"cycle\") or r.get(\"version\")\n",
        "                eol = r.get(\"eol\")\n",
        "                out.append(f\"{r.get('product')} {cyc} (EOL {eol})\" if r.get(\"status\")==\"EOL\"\n",
        "                           else f\"{r.get('product')} {cyc} (near EOL {eol})\")\n",
        "            return \"; \".join(out)\n",
        "\n",
        "        if eol_hits:\n",
        "            frame[\"auto_fail\"] = True\n",
        "            frame[\"compliant\"] = False\n",
        "            frame[\"needs_more_info\"] = False\n",
        "            summary = _a24_fmt(eol_hits) or \"Unsupported software detected via endoflife.date\"\n",
        "            frame[\"reason\"] = f\"Overruled (EOL via endoflife.date): {summary}\"\n",
        "            frame[\"eol_overrule\"] = True\n",
        "        elif near_hits:\n",
        "            note = _a24_fmt(near_hits) or \"Near EOL software detected via endoflife.date\"\n",
        "            if frame.get(\"reason\"):\n",
        "                frame[\"reason\"] = frame[\"reason\"].rstrip(\".\") + f\" | Note: {note}.\"\n",
        "            else:\n",
        "                frame[\"reason\"] = f\"Note: {note}.\"\n",
        "            frame[\"near_eol_note\"] = True\n",
        "\n",
        "        frame[\"eol_results\"] = eol_results\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# A2.4.1  ―  UPDATED with endoflife.date EOL overlay  (A2.4.1-scoped helpers)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "import re\n",
        "import requests\n",
        "from typing import Dict, List\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers for endoflife.date (A2.4.1-scoped)\n",
        "# -----------------------------\n",
        "A241_NEAR_EOL_DAYS = 180  # tweak if you want a different “near EOL” window\n",
        "\n",
        "def _a241_eol_slug_and_cycle_candidates_thin(text: str) -> List[tuple]:\n",
        "    \"\"\"\n",
        "    Extract (slug, version_hint) pairs from thin-client style answers.\n",
        "    Covers: Windows 10/11 (+ IoT/LTSC), ChromeOS, Ubuntu/Debian, IGEL OS,\n",
        "    HP ThinPro/ThinOS (best-effort), Dell Wyse ThinOS, Android.\n",
        "    Unknown slugs will simply return 'unknown' from the API layer and be ignored.\n",
        "    \"\"\"\n",
        "    t = text.lower()\n",
        "    pairs: List[tuple] = []\n",
        "\n",
        "    # ---- Windows 10 / 11 (incl. IoT / LTSC) ----\n",
        "    # Accept cycles like 20H2/21H2/22H2/23H2/24H2 and build numbers\n",
        "    for m in re.finditer(r\"\\bwindows\\s+10\\b\", t):\n",
        "        # Try to find a nearby cycle/build\n",
        "        win10_cycle = re.search(r\"(?:10\\s*(?:iot\\s+enterprise\\s*)?(?:ltsc|ltsb)?\\s*)(\\d{2}h\\d)\\b\", t[m.end():m.end()+24])\n",
        "        if win10_cycle:\n",
        "            pairs.append((\"windows-10\", win10_cycle.group(1).upper()))\n",
        "        else:\n",
        "            build = re.search(r\"build\\s*[-: ]*\\d+\", t[m.end():m.end()+24])\n",
        "            if build:\n",
        "                pairs.append((\"windows-10\", build.group(0)))\n",
        "            else:\n",
        "                pairs.append((\"windows-10\", \"21H2\"))  # sensible default cycle if none detected\n",
        "\n",
        "    for m in re.finditer(r\"\\bwindows\\s+11\\b\", t):\n",
        "        win11_cycle = re.search(r\"(?:11\\s*(?:iot\\s+enterprise\\s*)?(?:ltsc)?\\s*)(\\d{2}h\\d)\\b\", t[m.end():m.end()+24])\n",
        "        if win11_cycle:\n",
        "            pairs.append((\"windows-11\", win11_cycle.group(1).upper()))\n",
        "        else:\n",
        "            build = re.search(r\"build\\s*[-: ]*\\d+\", t[m.end():m.end()+24])\n",
        "            if build:\n",
        "                pairs.append((\"windows-11\", build.group(0)))\n",
        "            else:\n",
        "                pairs.append((\"windows-11\", \"23H2\"))\n",
        "\n",
        "    # Explicit LTSC year hints (map to Win10/Win11 cycles broadly)\n",
        "    if re.search(r\"windows\\s+10\\s+iot\\s+enterprise\\s+ltsc\\s+2019\", t):\n",
        "        pairs.append((\"windows-10\", \"1809\"))\n",
        "    if re.search(r\"windows\\s+10\\s+iot\\s+enterprise\\s+ltsc\\s+2021\", t):\n",
        "        pairs.append((\"windows-10\", \"21H2\"))\n",
        "    if re.search(r\"windows\\s+11\\s+iot\\s+enterprise\\s+ltsc\\s+2024\", t):\n",
        "        pairs.append((\"windows-11\", \"24H2\"))\n",
        "\n",
        "    # ---- ChromeOS ----\n",
        "    # Accept \"ChromeOS 119\" / \"Chrome OS 120\" / \"Chromebook on 120\"\n",
        "    for m in re.finditer(r\"\\bchrome\\s*os\\s*(\\d{2,3})\\b\", t):\n",
        "        pairs.append((\"chrome-os\", m.group(1)))\n",
        "    for m in re.finditer(r\"\\bchromeos\\s*(\\d{2,3})\\b\", t):\n",
        "        pairs.append((\"chrome-os\", m.group(1)))\n",
        "    if re.search(r\"\\bchrome\\s*os\\b|\\bchromeos\\b|\\bchromebook\\b\", t) and not any(s == \"chrome-os\" for s, _ in pairs):\n",
        "        # No version mentioned — still add a candidate with a blank hint\n",
        "        pairs.append((\"chrome-os\", \"\"))\n",
        "\n",
        "    # ---- Linux distros sometimes used on thin clients ----\n",
        "    for m in re.finditer(r\"ubuntu\\s+(\\d{2}\\.\\d{2})\", t):\n",
        "        pairs.append((\"ubuntu\", m.group(1)))\n",
        "    for m in re.finditer(r\"\\bdebian\\s+(\\d{1,2})\\b\", t):\n",
        "        pairs.append((\"debian\", m.group(1)))\n",
        "\n",
        "    # ---- Android on thin clients (rare but possible) ----\n",
        "    for m in re.finditer(r\"\\bandroid\\s+(\\d{1,2})\\b\", t):\n",
        "        pairs.append((\"android\", m.group(1)))  # endoflife.date has android\n",
        "\n",
        "    # ---- IGEL OS (best-effort) ----\n",
        "    for m in re.finditer(r\"\\bigel\\s*os\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"igel-os\", m.group(1)))\n",
        "    if re.search(r\"\\bigel\\s*os\\b\", t) and not any(s == \"igel-os\" for s, _ in pairs):\n",
        "        pairs.append((\"igel-os\", \"\"))\n",
        "\n",
        "    # ---- HP ThinPro / ThinOS (best-effort) ----\n",
        "    for m in re.finditer(r\"\\bhp\\s*thinpro\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"hp-thinpro\", m.group(1)))\n",
        "    if re.search(r\"\\bhp\\s*thinpro\\b\", t) and not any(s == \"hp-thinpro\" for s, _ in pairs):\n",
        "        pairs.append((\"hp-thinpro\", \"\"))\n",
        "\n",
        "    for m in re.finditer(r\"\\bthin\\s*os\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        # Could be HP ThinOS or Dell Wyse ThinOS — add both as candidates\n",
        "        pairs.append((\"hp-thinos\", m.group(1)))\n",
        "        pairs.append((\"wyse-thinos\", m.group(1)))\n",
        "    if re.search(r\"\\bthin\\s*os\\b\", t):\n",
        "        if not any(s == \"hp-thinos\" for s, _ in pairs):\n",
        "            pairs.append((\"hp-thinos\", \"\"))\n",
        "        if not any(s == \"wyse-thinos\" for s, _ in pairs):\n",
        "            pairs.append((\"wyse-thinos\", \"\"))\n",
        "\n",
        "    # Deduplicate while preserving order\n",
        "    seen = set()\n",
        "    out: List[tuple] = []\n",
        "    for p in pairs:\n",
        "        if p not in seen:\n",
        "            seen.add(p)\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "def _a241_fetch_product_cycles(slug: str):\n",
        "    \"\"\"\n",
        "    Fetch product cycles from endoflife.date and cache per-process.\n",
        "    \"\"\"\n",
        "    if not hasattr(_a241_fetch_product_cycles, \"_cache\"):\n",
        "        _a241_fetch_product_cycles._cache = {}\n",
        "    cache = _a241_fetch_product_cycles._cache\n",
        "    if slug in cache:\n",
        "        return cache[slug]\n",
        "\n",
        "    url = f\"https://endoflife.date/api/v1/products/{slug}.json\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=6)\n",
        "        if r.status_code == 200:\n",
        "            data = r.json()\n",
        "            cache[slug] = data\n",
        "            return data\n",
        "    except Exception:\n",
        "        pass\n",
        "    cache[slug] = None\n",
        "    return None\n",
        "\n",
        "def _a241_best_cycle_match(cycles, version_hint: str):\n",
        "    \"\"\"\n",
        "    Fuzzy match the version text to a cycle in the product list.\n",
        "    \"\"\"\n",
        "    if not cycles:\n",
        "        return None\n",
        "    vh = (version_hint or \"\").strip().lower()\n",
        "    vh_norm = vh.replace(\" \", \"\")\n",
        "    if not vh:\n",
        "        return None\n",
        "\n",
        "    for c in cycles:\n",
        "        cyc = str(c.get(\"cycle\") or c.get(\"releaseCycle\") or \"\").lower()\n",
        "        if not cyc:\n",
        "            continue\n",
        "        cyc_norm = cyc.replace(\" \", \"\")\n",
        "        if vh_norm == cyc_norm or vh in cyc or cyc in vh:\n",
        "            return c\n",
        "        # Allow LTSC variants (remove 'lts' to widen)\n",
        "        if vh in cyc.replace(\"lts\", \"\") or cyc.replace(\"lts\", \"\") in vh:\n",
        "            return c\n",
        "        # 2019/2021 -> 1809/21H2 heuristics handled by input above; still allow prefix\n",
        "        if cyc.startswith(vh):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def _a241_check_eol_status(slug: str, version_hint: str):\n",
        "    \"\"\"\n",
        "    Returns (status, date_str, days_left):\n",
        "      status: 'eol' | 'near' | 'ok' | 'unknown'\n",
        "    \"\"\"\n",
        "    cycles = _a241_fetch_product_cycles(slug)\n",
        "    if not cycles:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "    c = _a241_best_cycle_match(cycles, version_hint)\n",
        "    if not c:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "    eol_date = c.get(\"eol\") or c.get(\"support\") or c.get(\"discontinued\")\n",
        "    if not eol_date or isinstance(eol_date, bool):\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "    try:\n",
        "        d = datetime.fromisoformat(str(eol_date)).replace(tzinfo=timezone.utc)\n",
        "        today = datetime.now(timezone.utc)\n",
        "        delta = (d - today).days\n",
        "        if delta < 0:\n",
        "            return (\"eol\", d.date().isoformat(), delta)\n",
        "        if delta <= A241_NEAR_EOL_DAYS:\n",
        "            return (\"near\", d.date().isoformat(), delta)\n",
        "        return (\"ok\", d.date().isoformat(), delta)\n",
        "    except Exception:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Original A2.4.1 logic + EOL overlay\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def rule_a2_4_1(text: str) -> Dict[str, object]:\n",
        "    doc = nlp(text)\n",
        "    text_l = text.lower().strip()\n",
        "\n",
        "    frame = {\n",
        "        \"mentions_thin_clients\": False,\n",
        "        \"quantity\": 0,\n",
        "        \"has_os_info\": False,\n",
        "        \"unsupported_os\": False,\n",
        "        \"clear_no_thin_clients\": False,\n",
        "        \"non_compliant_missing_os\": False,\n",
        "        \"auto_fail\": False,\n",
        "        \"compliant\": False,\n",
        "        \"needs_more_info\": True,\n",
        "        \"reason\": \"\",\n",
        "        # --- EOL diagnostics (optional for UI) ---\n",
        "        \"eol_hits\": [],\n",
        "        \"near_eol_hits\": []\n",
        "    }\n",
        "\n",
        "    # --- 0 / none / n/a by itself: treat as \"no thin clients\" ---\n",
        "    if text_l in {\"0\", \"none\", \"n/a\", \"not applicable\", \"no\"}:\n",
        "        frame[\"clear_no_thin_clients\"] = True\n",
        "        frame[\"compliant\"] = True\n",
        "        frame[\"needs_more_info\"] = False\n",
        "        return frame  # no EOL overlay if there are none\n",
        "\n",
        "    # Detect mention of thin clients\n",
        "    if re.search(r\"\\bthin client(s)?\\b\", text_l):\n",
        "        frame[\"mentions_thin_clients\"] = True\n",
        "\n",
        "    # Explicit \"no thin clients\" phrases — early exit\n",
        "    no_phrases = [\n",
        "        r\"\\bno thin clients?\\b\",\n",
        "        r\"\\b(we|i)\\s+(do not|don’t|dont|are not|aren't|not)\\s+(have|use|using|utilise|utilizing|deploy(ed|ing)?)\\b.*thin clients?\",\n",
        "        r\"\\bnone\\b\", r\"\\bn/a\\b\", r\"\\bnot applicable\\b\",\n",
        "        r\"\\bno\\b.*thin clients?.*in use\",\n",
        "        r\"\\bdo not utilise\\b.*thin clients?\",\n",
        "        r\"\\bno.*thin client.*on the network\",\n",
        "        r\"\\bnot using thin clients?\\b\",\n",
        "        r\"\\bnot.*using.*thin clients?\\b\",\n",
        "        r\"\\bnot.*thin client.*environment\\b\"\n",
        "    ]\n",
        "    for pattern in no_phrases:\n",
        "        if re.search(pattern, text_l):\n",
        "            frame[\"clear_no_thin_clients\"] = True\n",
        "            frame[\"compliant\"] = True   # Explicit \"no thin clients\" is acceptable/compliant\n",
        "            frame[\"needs_more_info\"] = False\n",
        "            return frame  # no EOL overlay if there are none\n",
        "\n",
        "    # Quantity detection — only if thin clients are mentioned\n",
        "    if frame[\"mentions_thin_clients\"] is True:\n",
        "        number_matches = re.findall(r\"\\b\\d+\\b\", text_l)\n",
        "        if number_matches:\n",
        "            numbers = [int(n) for n in number_matches]\n",
        "            frame[\"quantity\"] = max(numbers)\n",
        "\n",
        "    # OS detection (supported and mentioned)\n",
        "    os_patterns = [\n",
        "        r\"windows\\s+(10|11|7|8|xp|vista|iot|home|enterprise|ltsc|pro)?\",\n",
        "        r\"ubuntu\", r\"linux\", r\"debian\", r\"macos\", r\"el capitan\",\n",
        "        r\"android\", r\"chromeos|chrome\\s*os\", r\"igel\", r\"hp thin os|thin\\s*os|hp thinpro\",\n",
        "        r\"parrot\", r\"kali\"\n",
        "    ]\n",
        "    if any(re.search(p, text_l) for p in os_patterns):\n",
        "        frame[\"has_os_info\"] = True\n",
        "\n",
        "    # Unsupported OS = Auto-fail (unchanged)\n",
        "    unsupported_os_keywords = [\n",
        "        \"windows 7\", \"windows 8\", \"vista\", \"xp\",\n",
        "        \"1809\", \"1607\", \"1903\", \"1909\", \"2004\",\n",
        "        \"macos high sierra\", \"el capitan\",\n",
        "        \"parrot\", \"kali\",\n",
        "        \"android 9\", \"windows 10 home 21h1\"\n",
        "    ]\n",
        "    if any(os in text_l for os in unsupported_os_keywords):\n",
        "        frame[\"unsupported_os\"] = True\n",
        "        frame[\"auto_fail\"] = True\n",
        "\n",
        "    # Windows 10 without safe versions (optional stricter rule) — unchanged\n",
        "    if re.search(r\"\\bwindows\\s+10\\b\", text_l) and not re.search(r\"\\b(22h2|iot|enterprise|ltsc)\\b\", text_l):\n",
        "        frame[\"unsupported_os\"] = True\n",
        "        frame[\"auto_fail\"] = True\n",
        "\n",
        "    # Flag missing OS info if thin clients are mentioned with quantity — unchanged\n",
        "    if frame[\"mentions_thin_clients\"] and frame[\"quantity\"] > 0 and not frame[\"has_os_info\"]:\n",
        "        frame[\"non_compliant_missing_os\"] = True\n",
        "\n",
        "    # --- Final label logic (unchanged) ---\n",
        "    if (\n",
        "        frame[\"mentions_thin_clients\"]\n",
        "        and frame[\"quantity\"] > 0\n",
        "        and frame[\"has_os_info\"]\n",
        "        and not frame[\"auto_fail\"]\n",
        "        and not frame[\"non_compliant_missing_os\"]\n",
        "    ):\n",
        "        frame[\"compliant\"] = True\n",
        "        frame[\"needs_more_info\"] = False\n",
        "    elif (\n",
        "        frame[\"mentions_thin_clients\"]\n",
        "        and (frame[\"quantity\"] == 0 or not frame[\"has_os_info\"])\n",
        "        and not frame[\"auto_fail\"]\n",
        "    ):\n",
        "        frame[\"needs_more_info\"] = True\n",
        "\n",
        "    # --- Reason assignment (unchanged) ---\n",
        "    if frame[\"compliant\"]:\n",
        "        if frame[\"clear_no_thin_clients\"]:\n",
        "            frame[\"reason\"] = \"Compliant: Applicant explicitly states no thin clients in use.\"\n",
        "        elif frame[\"quantity\"] > 0 and frame[\"has_os_info\"]:\n",
        "            frame[\"reason\"] = f\"Compliant: {frame['quantity']} thin clients listed with OS information provided.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Compliant: Thin client use confirmed with acceptable supporting detail.\"\n",
        "    elif frame[\"auto_fail\"]:\n",
        "        if frame[\"unsupported_os\"]:\n",
        "            frame[\"reason\"] = \"Auto-fail: Unsupported OS version mentioned for thin clients.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Auto-fail: Critical OS or configuration not supported.\"\n",
        "    elif frame[\"non_compliant_missing_os\"]:\n",
        "        frame[\"reason\"] = \"Needs more information: Thin client count provided but OS version is missing.\"\n",
        "    elif frame[\"mentions_thin_clients\"]:\n",
        "        frame[\"reason\"] = \"Needs more information: Thin clients mentioned but device count or OS info is unclear.\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: Thin client usage unclear or not stated explicitly.\"\n",
        "\n",
        "    # ---------------- EOL overlay (A2.4.1 helpers) ----------------\n",
        "    # If any are EOL -> Fail and overrule; if near EOL -> append note; else leave as-is.\n",
        "    try:\n",
        "        candidates = _a241_eol_slug_and_cycle_candidates_thin(text)\n",
        "        eol_hits, near_hits = [], []\n",
        "\n",
        "        for slug, ver in candidates:\n",
        "            status, date_str, days_left = _a241_check_eol_status(slug, ver)\n",
        "            label = f\"{slug} {ver}\".strip()\n",
        "            pretty = f\"{label} – EOL {date_str}\" if date_str else label\n",
        "            if status == \"eol\":\n",
        "                eol_hits.append(pretty)\n",
        "            elif status == \"near\":\n",
        "                near_hits.append(f\"{pretty} (in {days_left} days)\")\n",
        "\n",
        "        if eol_hits:\n",
        "            frame[\"auto_fail\"] = True\n",
        "            frame[\"compliant\"] = False\n",
        "            frame[\"needs_more_info\"] = False\n",
        "            frame[\"eol_hits\"] = eol_hits\n",
        "            frame[\"reason\"] = \"Fail: End-of-life software detected – \" + \"; \".join(eol_hits) + \".\"\n",
        "        elif near_hits:\n",
        "            frame[\"near_eol_hits\"] = near_hits\n",
        "            note = \" Note: Near end-of-life – \" + \"; \".join(near_hits) + \".\"\n",
        "            frame[\"reason\"] = (frame[\"reason\"] + note).strip()\n",
        "    except Exception:\n",
        "        # If the EOL API lookup fails, keep the original label and reason.\n",
        "        pass\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# A2.5  — UPDATED with EOL overlay\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers for endoflife.date\n",
        "# -----------------------------\n",
        "import re\n",
        "import requests\n",
        "from typing import Dict, List\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "NEAR_EOL_DAYS = 180  # tweak if you want a different “near EOL” window\n",
        "\n",
        "# product slug resolver for endoflife.date\n",
        "def _eol_slug_and_cycle_candidates(text: str):\n",
        "    \"\"\"\n",
        "    Return list of (slug, version_hint) tuples detected in free text.\n",
        "    We keep it broad to cover server OS, hypervisors and popular server apps.\n",
        "    \"\"\"\n",
        "    t = text.lower()\n",
        "    pairs = []\n",
        "\n",
        "    # --- Windows Server (years & R2) ---\n",
        "    # e.g., \"Windows Server 2012 R2\", \"Windows Server 2016/2019/2022/2025\", \"Server 23H2\"\n",
        "    for m in re.finditer(r\"windows\\s+server\\s+((?:20)?\\d{2})(?:\\s*r2)?\", t):\n",
        "        year = m.group(1)\n",
        "        # add explicit 'R2' if it appears nearby\n",
        "        r2 = bool(re.search(rf\"windows\\s+server\\s+{year}\\s*r2\", t))\n",
        "        ver = f\"{year}{' R2' if r2 else ''}\"\n",
        "        pairs.append((\"windows-server\", ver))\n",
        "\n",
        "    # Annual Channel (23H2 etc.)\n",
        "    for m in re.finditer(r\"windows\\s+server\\s+(\\d{2}h\\d)\\b\", t):\n",
        "        pairs.append((\"windows-server\", m.group(1).upper()))\n",
        "\n",
        "    # --- Ubuntu ---\n",
        "    for m in re.finditer(r\"ubuntu\\s+(\\d{2}\\.\\d{2})\", t):\n",
        "        pairs.append((\"ubuntu\", m.group(1)))\n",
        "\n",
        "    # --- Debian ---\n",
        "    for m in re.finditer(r\"debian\\s+(\\d{1,2})\\b\", t):\n",
        "        pairs.append((\"debian\", m.group(1)))\n",
        "\n",
        "    # --- RHEL / Red Hat Enterprise Linux ---\n",
        "    for m in re.finditer(r\"(?:rhel|red\\s*hat\\s+enterprise\\s+linux)\\s+(\\d{1,2})\\b\", t):\n",
        "        pairs.append((\"rhel\", m.group(1)))\n",
        "\n",
        "    # --- CentOS (classic) ---\n",
        "    for m in re.finditer(r\"centos\\s+(\\d{1,2})\\b\", t):\n",
        "        pairs.append((\"centos\", m.group(1)))\n",
        "\n",
        "    # --- CentOS Stream ---\n",
        "    for m in re.finditer(r\"centos\\s*stream\\s+(\\d{1,2})\\b\", t):\n",
        "        pairs.append((\"centos-stream\", m.group(1)))\n",
        "\n",
        "    # --- Rocky / Alma / Oracle Linux ---\n",
        "    for m in re.finditer(r\"rocky\\s+linux\\s+(\\d{1,2})\\b\", t):\n",
        "        pairs.append((\"rocky-linux\", m.group(1)))\n",
        "    for m in re.finditer(r\"(?:alma\\s*linux|almalinux)\\s+(\\d{1,2})\\b\", t):\n",
        "        pairs.append((\"almalinux-os\", m.group(1)))\n",
        "    for m in re.finditer(r\"oracle\\s+linux\\s+(\\d{1,2})\\b\", t):\n",
        "        pairs.append((\"oracle-linux\", m.group(1)))\n",
        "\n",
        "    # --- SUSE / SLES & openSUSE Leap ---\n",
        "    for m in re.finditer(r\"(?:sles|suse\\s+linux\\s+enterprise\\s+server)\\s+(\\d{1,2})\\b\", t):\n",
        "        pairs.append((\"sles\", m.group(1)))\n",
        "    for m in re.finditer(r\"(?:opensuse|open\\s*suse)(?:\\s*leap)?\\s*(\\d{1,2}(?:\\.\\d)?)\", t):\n",
        "        pairs.append((\"opensuse\", m.group(1)))\n",
        "\n",
        "    # --- Amazon Linux ---\n",
        "    if re.search(r\"amazon\\s+linux\\s+2\\b\", t):\n",
        "        pairs.append((\"amazon-linux-2\", \"2\"))\n",
        "    if re.search(r\"amazon\\s+linux\\s+2023\\b\", t):\n",
        "        pairs.append((\"amazon-linux-2023\", \"2023\"))\n",
        "    for m in re.finditer(r\"amazon\\s+linux\\s+(\\d{4}\\.\\d{2})\\b\", t):\n",
        "        pairs.append((\"amazon-linux\", m.group(1)))\n",
        "\n",
        "    # --- VMware ESXi / vCenter ---\n",
        "    for m in re.finditer(r\"\\besxi\\s+(\\d+(?:\\.\\d)?)\\b\", t):\n",
        "        pairs.append((\"esxi\", m.group(1)))\n",
        "    for m in re.finditer(r\"\\bvcenter(?:\\s+server)?\\s+(\\d+(?:\\.\\d)?)\\b\", t):\n",
        "        pairs.append((\"vcenter\", m.group(1)))\n",
        "\n",
        "    # --- Proxmox VE ---\n",
        "    for m in re.finditer(r\"proxmox\\s+ve\\s+(\\d+(?:\\.\\d)?)\\b\", t):\n",
        "        pairs.append((\"proxmox-ve\", m.group(1)))\n",
        "\n",
        "    # --- Databases / Server Apps (common in A2.5 answers) ---\n",
        "    for m in re.finditer(r\"(?:sql\\s*server|mssql)\\s+(20\\d{2})\\b\", t):\n",
        "        pairs.append((\"mssqlserver\", m.group(1)))\n",
        "    for m in re.finditer(r\"exchange\\s+server\\s+(20\\d{2})\\b\", t):\n",
        "        pairs.append((\"msexchange\", m.group(1)))\n",
        "    for m in re.finditer(r\"sharepoint\\s+(?:server\\s+)?(20\\d{2})\\b\", t):\n",
        "        pairs.append((\"sharepoint\", m.group(1)))\n",
        "    for m in re.finditer(r\"mysql\\s+(\\d+\\.\\d+)\", t):\n",
        "        pairs.append((\"mysql\", m.group(1)))\n",
        "    for m in re.finditer(r\"mariadb\\s+(\\d+\\.\\d+)\", t):\n",
        "        pairs.append((\"mariadb\", m.group(1)))\n",
        "    for m in re.finditer(r\"(?:postgres|postgresql)\\s+(\\d+(?:\\.\\d)?)\", t):\n",
        "        pairs.append((\"postgresql\", m.group(1)))\n",
        "\n",
        "    # Deduplicate while preserving order\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for p in pairs:\n",
        "        if p not in seen:\n",
        "            seen.add(p)\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "def _fetch_product_cycles(slug: str):\n",
        "    \"\"\"\n",
        "    Fetch the product JSON from endoflife.date and return list of dicts.\n",
        "    Cache results per process to avoid repeated HTTP calls.\n",
        "    \"\"\"\n",
        "    if not hasattr(_fetch_product_cycles, \"_cache\"):\n",
        "        _fetch_product_cycles._cache = {}\n",
        "    cache = _fetch_product_cycles._cache\n",
        "    if slug in cache:\n",
        "        return cache[slug]\n",
        "\n",
        "    url = f\"https://endoflife.date/api/v1/products/{slug}.json\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=6)\n",
        "        if r.status_code == 200:\n",
        "            data = r.json()\n",
        "            cache[slug] = data\n",
        "            return data\n",
        "    except Exception:\n",
        "        pass\n",
        "    cache[slug] = None\n",
        "    return None\n",
        "\n",
        "def _best_cycle_match(cycles, version_hint: str):\n",
        "    \"\"\"\n",
        "    Fuzzy match the applicant's version string to a cycle from endoflife.date.\n",
        "    We normalize spaces/case and allow substring matches both ways.\n",
        "    \"\"\"\n",
        "    if not cycles:\n",
        "        return None\n",
        "    vh = version_hint.strip().lower()\n",
        "    vh_norm = vh.replace(\" \", \"\")\n",
        "\n",
        "    best = None\n",
        "    for c in cycles:\n",
        "        cyc = c.get(\"cycle\") or c.get(\"releaseCycle\") or \"\"\n",
        "        cyc_l = str(cyc).lower()\n",
        "        cyc_norm = cyc_l.replace(\" \", \"\")\n",
        "        if vh_norm == cyc_norm or vh in cyc_l or cyc_l in vh:\n",
        "            best = c\n",
        "            break\n",
        "        # Ubuntu: accept \"20.04\" matching \"20.04 LTS\"\n",
        "        if vh in cyc_l.replace(\"lts\", \"\").strip():\n",
        "            best = c\n",
        "            break\n",
        "        # Windows Server: accept \"2012r2\" vs \"2012 R2\"\n",
        "        if vh_norm.replace(\"r2\", \" r2\") in cyc_norm or vh_norm == cyc_norm.replace(\"r2\", \" r2\"):\n",
        "            best = c\n",
        "            break\n",
        "    # If still nothing, try prefix match (e.g., \"8\" -> \"8.0\")\n",
        "    if not best:\n",
        "        for c in cycles:\n",
        "            cyc = str(c.get(\"cycle\", \"\")).lower()\n",
        "            if cyc.startswith(vh):\n",
        "                best = c\n",
        "                break\n",
        "    return best\n",
        "\n",
        "def _check_eol_status(slug: str, version_hint: str):\n",
        "    \"\"\"\n",
        "    Returns tuple: (status, date_str, days_left)\n",
        "    status: 'eol' | 'near' | 'ok' | 'unknown'\n",
        "    date_str: ISO date string for eol (if known) else ''\n",
        "    days_left: integer (positive for days remaining, negative if past)\n",
        "    \"\"\"\n",
        "    cycles = _fetch_product_cycles(slug)\n",
        "    if not cycles:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "    c = _best_cycle_match(cycles, version_hint)\n",
        "    if not c:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "    # Prefer 'eol' if present, else 'support' (some pages use different keys)\n",
        "    eol_date = c.get(\"eol\") or c.get(\"support\") or c.get(\"discontinued\")\n",
        "    if not eol_date:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "    try:\n",
        "        # Handle 'false' or 'true' values (some entries use booleans)\n",
        "        if isinstance(eol_date, bool):\n",
        "            return (\"unknown\", \"\", 0)\n",
        "        d = datetime.fromisoformat(str(eol_date)).replace(tzinfo=timezone.utc)\n",
        "        today = datetime.now(timezone.utc)\n",
        "        delta = (d - today).days\n",
        "        if delta < 0:\n",
        "            return (\"eol\", d.date().isoformat(), delta)\n",
        "        if delta <= NEAR_EOL_DAYS:\n",
        "            return (\"near\", d.date().isoformat(), delta)\n",
        "        return (\"ok\", d.date().isoformat(), delta)\n",
        "    except Exception:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "#\n",
        "# ------------------------------------------------------------\n",
        "try:\n",
        "    from text2num import text2num\n",
        "except ImportError:\n",
        "    text2num = None\n",
        "\n",
        "def extract_all_numbers(text: str) -> List[int]:\n",
        "    nums = [int(x) for x in re.findall(r\"\\b\\d{1,4}\\b\", text)]\n",
        "    if text2num:\n",
        "        for m in re.finditer(\n",
        "            r'\\b(?:zero|one|two|three|four|five|six|seven|eight|nine|ten|'\n",
        "            r'eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|'\n",
        "            r'eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|'\n",
        "            r'eighty|ninety|hundred|thousand|and|-)+\\b(?:\\s+\\b(?:zero|one|'\n",
        "            r'two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|'\n",
        "            r'thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|'\n",
        "            r'twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|and|-)+\\b)*',\n",
        "            text, re.I):\n",
        "            try:\n",
        "                nums.append(text2num(m.group(0)))\n",
        "            except Exception:\n",
        "                continue\n",
        "    return nums\n",
        "\n",
        "def rule_a2_5(text: str) -> Dict[str, object]:\n",
        "    doc = nlp(text)\n",
        "    lowered = text.lower().strip()\n",
        "\n",
        "    frame: Dict[str, object] = {\n",
        "        \"server_count\": 0,\n",
        "        \"hypervisor_os\": [],\n",
        "        \"server_os\": [],\n",
        "        \"unsupported_os\": False,\n",
        "        \"auto_fail\": False,\n",
        "        \"compliant\": False,\n",
        "        \"needs_more_info\": True,\n",
        "        \"no_servers\": False,   # Explicit no/none/n/a\n",
        "        \"attached\": False,      # Explicit attached\n",
        "        \"reason\": \"\",\n",
        "        # --- new diagnostics (for UI/debug) ---\n",
        "        \"eol_hits\": [],         # list of \"Product ver – EOL yyyy-mm-dd\"\n",
        "        \"near_eol_hits\": []     # list of \"Product ver – EOL yyyy-mm-dd (in N days)\"\n",
        "    }\n",
        "\n",
        "    # 1. Mark 'attached' as compliant\n",
        "    if re.search(r\"\\battached\\b\", lowered):\n",
        "        frame.update({\n",
        "            \"compliant\": True,\n",
        "            \"needs_more_info\": False,\n",
        "            \"attached\": True,\n",
        "            \"reason\": \"Compliant: Answer refers to attached document.\"\n",
        "        })\n",
        "        # no EOL overlay for 'attached'\n",
        "        return frame\n",
        "\n",
        "    # 2. Mark negative/none answers as compliant (no servers)\n",
        "    negative_phrases = [\n",
        "        r\"\\bno servers?\\b\", r\"\\bnone\\b\", r\"\\bn/a\\b\", r\"\\bnot applicable\\b\",\n",
        "        r\"\\bdo not use servers?\\b\", r\"\\bwe do not use servers?\\b\",\n",
        "        r\"\\bno virtual servers?\\b\", r\"\\bno physical servers?\\b\",\n",
        "        r\"\\bno servers? used\\b\", r\"\\bdo not have any servers?\\b\",\n",
        "        r\"\\bwe currently do not have any servers?\\b\",\n",
        "        r\"\\bwe don't use servers?\\b\", r\"\\bthere are none\\b\",\n",
        "        r\"\\bnone used\\b\",\n",
        "        r\"\\bjust laptops?\\b\", r\"\\bonly laptops?\\b\", r\"\\bjust desktops?\\b\", r\"\\bonly desktops?\\b\",\n",
        "        r\"\\bi don't have any servers\\b\", r\"\\bthere (are|is) none\\b\", r\"\\bonly\\b.*\\blaptop[s]?\\b\", r\"\\bjust\\b.*\\blaptop[s]?\\b\"\n",
        "    ]\n",
        "    if any(re.search(p, lowered) for p in negative_phrases) or lowered.strip() in {\"0\", \"none\"}:\n",
        "        frame.update({\n",
        "            \"compliant\": True,\n",
        "            \"needs_more_info\": False,\n",
        "            \"no_servers\": True,\n",
        "            \"reason\": \"Compliant: Applicant clearly states that no servers are in use.\"\n",
        "        })\n",
        "        # no EOL overlay if no servers\n",
        "        return frame\n",
        "\n",
        "    # 3. Handle cloud servers: treat as at least one if plural and OS present\n",
        "    if re.search(r\"\\bcloud servers?\\b\", lowered) and (\n",
        "        re.search(r\"windows server|ubuntu|linux|centos|redhat|debian\", lowered)\n",
        "    ):\n",
        "        frame.update({\n",
        "            \"server_count\": 1,\n",
        "            \"compliant\": True,\n",
        "            \"needs_more_info\": False,\n",
        "            \"reason\": \"Compliant: Cloud server(s) mentioned with valid OS.\"\n",
        "        })\n",
        "        # continue to OS extraction below (and still check EOL)\n",
        "\n",
        "    # 4. Server Count Extraction (robust)\n",
        "    count = 0\n",
        "    count_patterns = [\n",
        "        r\"((?:\\d+|zero|one|two|three|four|five|six|seven|eight|nine|ten|\"\n",
        "        r\"eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|\"\n",
        "        r\"thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand)+)\\s*(?:x\\s*)?\"\n",
        "        r\"(?:dell|hp|microsoft|ubuntu|linux|rocky|oracle|redhat|rhel|centos)?\\s*\"\n",
        "        r\"(?:windows\\s+server\\s+\\d{4}|windows\\s+\\d{4}|linux)?\\s*\"\n",
        "        r\"(servers?|vms?|instances?|machines?)\",\n",
        "        r\"((?:\\d+|zero|one|two|three|four|five|six|seven|eight|nine|ten|\"\n",
        "        r\"eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|\"\n",
        "        r\"thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand)+)\\s*(?:x\\s*)?(standard_b\\d+ms)\"\n",
        "    ]\n",
        "\n",
        "    for pattern in count_patterns:\n",
        "        for match in re.findall(pattern, lowered):\n",
        "            num = match[0] if isinstance(match, tuple) else match\n",
        "            try:\n",
        "                count += int(num)\n",
        "            except ValueError:\n",
        "                if text2num:\n",
        "                    try:\n",
        "                        count += text2num(num)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "    # Fallback: scan for all numbers (avoid years)\n",
        "    if count == 0:\n",
        "        for n in extract_all_numbers(lowered):\n",
        "            if 2000 <= n <= 2100:  # ignore years\n",
        "                continue\n",
        "            count += n\n",
        "    frame[\"server_count\"] = count\n",
        "\n",
        "    # 5. OS / Hypervisor Extraction (robust)\n",
        "    os_patterns = [\n",
        "        r\"(windows server \\d{4}(?: [a-z]+)?)\",\n",
        "        r\"(windows \\d{4}(?: [a-z]+)?)\",\n",
        "        r\"(ubuntu \\d{2}\\.\\d{2})\", r\"(debian \\d+)\",\n",
        "        r\"(redhat(?: enterprise)? linux \\d+)\", r\"(centos \\d+)\", r\"(oracle linux \\d+)\",\n",
        "        r\"(rocky linux \\d+)\",\n",
        "        r\"\\(\\s*\\d+\\s*\\)\", r\"build\\s*[-: ]*\\d+\", r\"\\b\\d{5}\\b\"\n",
        "    ]\n",
        "    hypervisor_patterns = [\n",
        "        r\"(vmware esxi \\d+\\.\\d+)\", r\"\\b(hyper[- ]?v)\\b\", r\"(proxmox)\",\n",
        "        r\"(azure)\", r\"(gcp)\", r\"(aws)\", r\"(cloud run)\", r\"(vcenter)\",\n",
        "        r\"\\bstandard_b\\d+[a-z]*\\b\"\n",
        "    ]\n",
        "    server_os = []\n",
        "    for pattern in os_patterns:\n",
        "        server_os += [m for m in re.findall(pattern, lowered)]\n",
        "\n",
        "    # Windows Server year patterns (allow \"server 2016\", \"2019\", \"2022\", \"2025\")\n",
        "    if not server_os:\n",
        "        for yr in (\"2016\", \"2019\", \"2022\", \"2025\"):\n",
        "            if re.search(rf\"server\\s*{yr}\", lowered) or re.search(rf\"\\b{yr}\\b\", lowered):\n",
        "                server_os.append(f\"windows server {yr}\")\n",
        "\n",
        "    # Add standalone version/build numbers\n",
        "    server_os += re.findall(r'\\b\\d+\\.\\d+\\.\\d+\\b', lowered)    # e.g. 10.0.19045\n",
        "    server_os += re.findall(r'\\b\\d+h\\d+\\b', lowered)          # e.g. 22h2\n",
        "    server_os += re.findall(r'\\b\\d{4}h\\d\\b', lowered)         # e.g. 1123h2\n",
        "\n",
        "    # spaCy mac product names (if your org uses non-Windows servers)\n",
        "    mac_names = {\"ventura\", \"sonoma\", \"sequoia\", \"monterey\", \"big sur\", \"catalina\"}\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PRODUCT\" and ent.text.lower() in mac_names:\n",
        "            server_os.append(\"macos \" + ent.text.lower())\n",
        "\n",
        "    frame[\"server_os\"] = sorted(set(s.strip() for s in server_os if s.strip()))\n",
        "\n",
        "    # --- Hypervisors ---\n",
        "    hypervisors = []\n",
        "    for pattern in hypervisor_patterns:\n",
        "        for match in re.findall(pattern, lowered):\n",
        "            hypervisors.append(match.strip())\n",
        "    if \"azure\" in lowered and \"azure\" not in hypervisors:\n",
        "        hypervisors.append(\"azure\")\n",
        "    frame[\"hypervisor_os\"] = sorted(set(hypervisors))\n",
        "\n",
        "    # 6. Unsupported/Pentest OS detection (quick guards)\n",
        "    unsupported_keywords = [\n",
        "        \"windows server 2008\", \"windows server 2012\", \"windows 7\", \"windows 8\",\n",
        "        \"centos 6\", \"centos 7\", \"oracle linux 6\", \"esxi 6.5\", \"parrot\", \"kali\", \"rhel 6\"\n",
        "    ]\n",
        "    for keyword in unsupported_keywords:\n",
        "        if keyword in lowered:\n",
        "            frame[\"unsupported_os\"] = True\n",
        "            frame[\"auto_fail\"] = True\n",
        "            break\n",
        "    for os_version in frame[\"server_os\"]:\n",
        "        if any(u in os_version.lower() for u in unsupported_keywords):\n",
        "            frame[\"unsupported_os\"] = True\n",
        "            frame[\"auto_fail\"] = True\n",
        "            break\n",
        "\n",
        "    # 7. Final Compliance / MIR logic (original baseline)\n",
        "    if (\n",
        "        frame[\"server_count\"] > 0 and\n",
        "        (frame[\"server_os\"] or frame[\"hypervisor_os\"]) and\n",
        "        not frame[\"auto_fail\"]\n",
        "    ):\n",
        "        frame[\"compliant\"] = True\n",
        "        frame[\"needs_more_info\"] = False\n",
        "        frame[\"reason\"] = f\"Compliant: {frame['server_count']} server(s) listed with OS or hypervisor information.\"\n",
        "\n",
        "    elif not frame[\"auto_fail\"]:\n",
        "        frame[\"compliant\"] = False\n",
        "        frame[\"needs_more_info\"] = True\n",
        "        if frame[\"server_count\"] == 0:\n",
        "            frame[\"reason\"] = \"Needs more information: No server count could be determined.\"\n",
        "        elif not (frame[\"server_os\"] or frame[\"hypervisor_os\"]):\n",
        "            frame[\"reason\"] = \"Needs more information: Server(s) listed but missing OS or hypervisor info.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Needs more information: Server data incomplete or unclear.\"\n",
        "\n",
        "    # --- Auto-fail rationale (original)\n",
        "    if frame[\"auto_fail\"]:\n",
        "        if frame[\"unsupported_os\"]:\n",
        "            frame[\"reason\"] = \"Auto-fail: Unsupported server OS or configuration detected.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Auto-fail: Critical unsupported software or environment detected.\"\n",
        "\n",
        "    # 8. EOL overlay (ONLY augments/overrides after your original logic)\n",
        "    #    - If any EOL => override to Fail\n",
        "    #    - Else if any near-EOL => append to reason (do not change label)\n",
        "    try:\n",
        "        # Build EOL candidates from raw text (covers far more than server_os list)\n",
        "        candidates = _eol_slug_and_cycle_candidates(text)\n",
        "\n",
        "        eol_hits = []\n",
        "        near_hits = []\n",
        "\n",
        "        for slug, ver in candidates:\n",
        "            status, date_str, days_left = _check_eol_status(slug, ver)\n",
        "            pretty = f\"{slug} {ver} – EOL {date_str}\" if date_str else f\"{slug} {ver}\"\n",
        "            if status == \"eol\":\n",
        "                eol_hits.append(pretty)\n",
        "            elif status == \"near\":\n",
        "                near_hits.append(f\"{pretty} (in {days_left} days)\")\n",
        "\n",
        "        if eol_hits:\n",
        "            frame[\"auto_fail\"] = True\n",
        "            frame[\"compliant\"] = False\n",
        "            frame[\"needs_more_info\"] = False\n",
        "            frame[\"eol_hits\"] = eol_hits\n",
        "            frame[\"reason\"] = \"Fail: End-of-life software detected – \" + \"; \".join(eol_hits) + \".\"\n",
        "        elif near_hits:\n",
        "            frame[\"near_eol_hits\"] = near_hits\n",
        "            # keep whatever label was decided above; just append detail\n",
        "            suffix = \" Note: Near end-of-life – \" + \"; \".join(near_hits) + \".\"\n",
        "            frame[\"reason\"] = (frame[\"reason\"] + suffix).strip()\n",
        "\n",
        "    except Exception:\n",
        "        # If EOL API fails for any reason, we silently keep the original label/reason.\n",
        "        pass\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# A2.6 — “Quantities of tablets / mobiles in scope”  (UPDATED with EOL overlay)\n",
        "#           Helpers namespaced for A2.6 to avoid collisions\n",
        "# ---------------------------------------------------------------------------\n",
        "from typing import List, Dict\n",
        "import re\n",
        "\n",
        "try:\n",
        "    from text2num import text2num\n",
        "except ImportError:\n",
        "    text2num = None\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers for endoflife.date (A2.6-scoped)\n",
        "# -----------------------------\n",
        "import requests\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "A26_NEAR_EOL_DAYS = 180  # tweak if you want a different “near EOL” window\n",
        "\n",
        "\n",
        "def _a26_eol_slug_and_cycle_candidates_mobile(text: str) -> List[tuple]:\n",
        "    \"\"\"\n",
        "    Extract (slug, version_hint) pairs from mobile/tablet answers.\n",
        "    Covers:\n",
        "      - Android (11/12/13/14...), optional decimals (12.1)\n",
        "      - iOS / iPadOS (12/13/14/15/16/17...), optional decimals (16.6.1)\n",
        "      - ChromeOS (tablet form factor sometimes)\n",
        "      - Optional best-effort: HarmonyOS, Fire OS, KaiOS, Windows 10 Mobile, BlackBerry OS\n",
        "      - Android skins (One UI / ColorOS / OxygenOS) are mapped only if an Android base version is present nearby\n",
        "    Unknown slugs will simply yield 'unknown' and be ignored by the overlay.\n",
        "    \"\"\"\n",
        "    t = text.lower()\n",
        "    pairs: List[tuple] = []\n",
        "\n",
        "    # --- iOS / iPadOS ---\n",
        "    for m in re.finditer(r\"\\bios\\s+(\\d{1,2}(?:\\.\\d+){0,2})\\b\", t, re.I):\n",
        "        pairs.append((\"ios\", m.group(1)))\n",
        "\n",
        "    for m in re.finditer(r\"\\b(?:ipad\\s*os|ipados)\\s+(\\d{1,2}(?:\\.\\d+){0,2})\\b\", t, re.I):\n",
        "        pairs.append((\"ipados\", m.group(1)))\n",
        "\n",
        "    # If iPadOS mentioned with no version, keep a blank hint candidate\n",
        "    if re.search(r\"\\b(?:ipad\\s*os|ipados)\\b\", t, re.I) and not any(s == \"ipados\" for s, _ in pairs):\n",
        "        pairs.append((\"ipados\", \"\"))\n",
        "\n",
        "    # --- Android ---\n",
        "    for m in re.finditer(r\"\\bandroid\\s+(\\d{1,2}(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"android\", m.group(1)))\n",
        "\n",
        "    # Android skins — map to Android ONLY if a nearby Android version is stated\n",
        "    # one ui 6.1 (android 14), coloros 13 (android 13), oxygenos 14 (android 14)\n",
        "    for skin in (\"one\\s*ui\", \"color\\s*os|coloros\", \"oxygen\\s*os|oxygenos\", \"miui\"):\n",
        "        for m in re.finditer(rf\"\\b(?:{skin})\\s+(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "            # look ahead/behind for \"android <ver>\"\n",
        "            span_start, span_end = m.span()\n",
        "            window = t[max(0, span_start - 40): min(len(t), span_end + 40)]\n",
        "            mm = re.search(r\"\\bandroid\\s+(\\d{1,2}(?:\\.\\d+)*)\\b\", window)\n",
        "            if mm:\n",
        "                pairs.append((\"android\", mm.group(1)))\n",
        "\n",
        "    # --- ChromeOS (some tablets run it) ---\n",
        "    for m in re.finditer(r\"\\bchrome\\s*os\\s*(\\d{2,3})\\b\", t):\n",
        "        pairs.append((\"chrome-os\", m.group(1)))\n",
        "    for m in re.finditer(r\"\\bchromeos\\s*(\\d{2,3})\\b\", t):\n",
        "        pairs.append((\"chrome-os\", m.group(1)))\n",
        "    if re.search(r\"\\bchrome\\s*os\\b|\\bchromeos\\b|\\bchromebook\\b\", t) and not any(s == \"chrome-os\" for s, _ in pairs):\n",
        "        pairs.append((\"chrome-os\", \"\"))  # allow unknown cycle\n",
        "\n",
        "    # --- HarmonyOS / Fire OS / KaiOS / Windows 10 Mobile / BlackBerry (best-effort) ---\n",
        "    for m in re.finditer(r\"\\bharmony\\s*os\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"harmonyos\", m.group(1)))\n",
        "    for m in re.finditer(r\"\\bfire\\s*os\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"fire-os\", m.group(1)))\n",
        "    for m in re.finditer(r\"\\bkai\\s*os\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"kaios\", m.group(1)))\n",
        "    # Windows 10 Mobile\n",
        "    if re.search(r\"\\bwindows\\s+10\\s+mobile\\b\", t):\n",
        "        pairs.append((\"windows-10-mobile\", \"10\"))\n",
        "    # Windows Phone generic (rare; unknown slug likely)\n",
        "    for m in re.finditer(r\"\\bwindows\\s+phone\\s*(\\d+(?:\\.\\d+)*)?\\b\", t):\n",
        "        ver = m.group(1) or \"\"\n",
        "        pairs.append((\"windows-phone\", ver))\n",
        "    # BlackBerry OS / BB10\n",
        "    for m in re.finditer(r\"\\bblackberry\\s*(?:os|10)\\s*(\\d+(?:\\.\\d+)*)?\\b\", t):\n",
        "        ver = m.group(1) or \"\"\n",
        "        pairs.append((\"blackberry-os\", ver))\n",
        "\n",
        "    # Deduplicate while preserving order\n",
        "    seen = set()\n",
        "    out: List[tuple] = []\n",
        "    for p in pairs:\n",
        "        if p not in seen:\n",
        "            seen.add(p)\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "\n",
        "def _a26_fetch_product_cycles(slug: str):\n",
        "    \"\"\"\n",
        "    Fetch the product JSON from endoflife.date and return list of dicts.\n",
        "    Cache results per process to avoid repeated HTTP calls.\n",
        "    \"\"\"\n",
        "    if not hasattr(_a26_fetch_product_cycles, \"_cache\"):\n",
        "        _a26_fetch_product_cycles._cache = {}\n",
        "    cache = _a26_fetch_product_cycles._cache\n",
        "    if slug in cache:\n",
        "        return cache[slug]\n",
        "\n",
        "    url = f\"https://endoflife.date/api/v1/products/{slug}.json\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=6)\n",
        "        if r.status_code == 200:\n",
        "            data = r.json()\n",
        "            cache[slug] = data\n",
        "            return data\n",
        "    except Exception:\n",
        "        pass\n",
        "    cache[slug] = None\n",
        "    return None\n",
        "\n",
        "\n",
        "def _a26_best_cycle_match(cycles, version_hint: str):\n",
        "    \"\"\"\n",
        "    Fuzzy match the applicant's version string to a cycle from endoflife.date.\n",
        "    We normalize spaces/case and allow substring matches both ways.\n",
        "    \"\"\"\n",
        "    if not cycles:\n",
        "        return None\n",
        "    vh = (version_hint or \"\").strip().lower()\n",
        "    if not vh:\n",
        "        return None\n",
        "    vh_norm = vh.replace(\" \", \"\")\n",
        "\n",
        "    for c in cycles:\n",
        "        cyc = str(c.get(\"cycle\") or c.get(\"releaseCycle\") or \"\").lower()\n",
        "        if not cyc:\n",
        "            continue\n",
        "        cyc_norm = cyc.replace(\" \", \"\")\n",
        "        if vh_norm == cyc_norm or vh in cyc or cyc in vh:\n",
        "            return c\n",
        "        if cyc.startswith(vh):  # prefix match (e.g., 16 -> 16.6)\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "\n",
        "def _a26_check_eol_status(slug: str, version_hint: str):\n",
        "    \"\"\"\n",
        "    Returns tuple: (status, date_str, days_left)\n",
        "      status: 'eol' | 'near' | 'ok' | 'unknown'\n",
        "    \"\"\"\n",
        "    cycles = _a26_fetch_product_cycles(slug)\n",
        "    if not cycles:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "    c = _a26_best_cycle_match(cycles, version_hint)\n",
        "    if not c:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "    eol_date = c.get(\"eol\") or c.get(\"support\") or c.get(\"discontinued\")\n",
        "    if not eol_date or isinstance(eol_date, bool):\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "    try:\n",
        "        d = datetime.fromisoformat(str(eol_date)).replace(tzinfo=timezone.utc)\n",
        "        today = datetime.now(timezone.utc)\n",
        "        delta = (d - today).days\n",
        "        if delta < 0:\n",
        "            return (\"eol\", d.date().isoformat(), delta)\n",
        "        if delta <= A26_NEAR_EOL_DAYS:\n",
        "            return (\"near\", d.date().isoformat(), delta)\n",
        "        return (\"ok\", d.date().isoformat(), delta)\n",
        "    except Exception:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Optional number extractor (A2.6-scoped; unchanged behavior)\n",
        "# -----------------------------\n",
        "def _a26_extract_all_numbers(text: str) -> List[int]:\n",
        "    nums = [int(x) for x in re.findall(r\"\\b\\d{1,4}\\b\", text)]\n",
        "    if text2num:\n",
        "        for m in re.finditer(\n",
        "            r'\\b(?:zero|one|two|three|four|five|six|seven|eight|nine|ten|'\n",
        "            r'eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|'\n",
        "            r'eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|'\n",
        "            r'eighty|ninety|hundred|thousand|and|-)+\\b(?:\\s+\\b(?:zero|one|'\n",
        "            r'two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|'\n",
        "            r'thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|'\n",
        "            r'twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|and|-)+\\b)*',\n",
        "            text, re.I):\n",
        "            try:\n",
        "                nums.append(text2num(m.group(0)))\n",
        "            except Exception:\n",
        "                continue\n",
        "    return nums\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main rule (kept same + EOL overlay)\n",
        "# -----------------------------\n",
        "def rule_a2_6(text: str) -> Dict[str, object]:\n",
        "    \"\"\"\n",
        "    Returns a frame describing the applicant’s answer to A2.6 (mobile/tablet OS).\n",
        "    Every number counts as a device unless:\n",
        "      - It is in a decimal (e.g., 18.3.1, 14.0),\n",
        "      - OR it is immediately after 'samsung' or 'iphone'.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    lower = text.lower().strip()\n",
        "    frame: Dict[str, object] = {\n",
        "        \"mobile_device_count\": 0,\n",
        "        \"os_versions\":        [],\n",
        "        \"unsupported_os\":     False,\n",
        "        \"auto_fail\":          False,\n",
        "        \"compliant\":          False,\n",
        "        \"needs_more_info\":    False,\n",
        "        \"non_compliant\":      False,\n",
        "        \"mentions_byod\":      False,\n",
        "        \"attached\":           False,\n",
        "        \"no_devices\":         False,\n",
        "        \"reason\":             \"\",\n",
        "        # --- EOL diagnostics (optional for UI/debug) ---\n",
        "        \"eol_hits\":           [],\n",
        "        \"near_eol_hits\":      []\n",
        "    }\n",
        "\n",
        "    # 1. \"Attached\" shortcut\n",
        "    if re.search(r\"\\battached\\b\", lower):\n",
        "        frame[\"compliant\"] = True\n",
        "        frame[\"attached\"] = True\n",
        "        return frame\n",
        "\n",
        "    # 2. Explicit negative/none patterns (more aggressive)\n",
        "    NEG_PATTERNS = [\n",
        "        r\"\\bno (mobiles?|tablets?|devices?|iphones?|ipads?)\\b\",\n",
        "        r\"\\bnone\\b\",\n",
        "        r\"\\bn/a\\b\",\n",
        "        r\"\\bnot applicable\\b\",\n",
        "        r\"\\bdo not use\\b.*(mobile|tablet|device|phone|ipad|iphone)s?\",\n",
        "        r\"\\bdo not have\\b.*(mobile|tablet|device|phone|ipad|iphone)s?\",\n",
        "        r\"\\bwe do not use\\b.*(mobile|tablet|device|phone|ipad|iphone)s?\",\n",
        "        r\"\\bwe do not have\\b.*(mobile|tablet|device|phone|ipad|iphone)s?\",\n",
        "        r\"\\bnot using\\b.*(mobile|tablet|device|phone|ipad|iphone)s?\",\n",
        "        r\"\\bno company mobile\\b\",\n",
        "        r\"\\bno company tablet\\b\",\n",
        "        r\"\\bno company device\\b\",\n",
        "        r\"\\bnot in use\\b\",\n",
        "        r\"\\bno one allowed\\b.*(mobile|tablet|device|phone|ipad|iphone)s?\",\n",
        "        r\"\\bno byod\\b\",\n",
        "        r\"\\bnot used\\b\",\n",
        "        r\"\\bthere (are|is) none\\b\",\n",
        "        r\"\\bnone used\\b\"\n",
        "    ]\n",
        "    if any(re.search(p, lower) for p in NEG_PATTERNS) or lower in {\"0\", \"none\"}:\n",
        "        frame[\"compliant\"] = True\n",
        "        frame[\"no_devices\"] = True\n",
        "        return frame\n",
        "\n",
        "    # 3. BYOD / personal device mention (informational)\n",
        "    if re.search(r\"\\b(byod|bring\\s+your\\s+own\\s+device|personal\\s+device)\\b\", lower):\n",
        "        frame[\"mentions_byod\"] = True\n",
        "\n",
        "    # 4. Device-count extraction (handles 1x, 6x, 2*, etc.)\n",
        "    norm = re.sub(r\"(\\d+)\\s*[\\*x×]\\s*\", r\"\\1 \", lower)\n",
        "    device_count = 0\n",
        "    for m in re.finditer(r\"\\b\\d{1,4}\\b\", norm):\n",
        "        # --- SKIP: If part of a decimal (e.g. 18.3.1, 14.0) ---\n",
        "        start, end = m.span()\n",
        "        if re.match(r\"\\.\\d\", norm[end:end+2]) or re.match(r\"\\d+\\.\", norm[max(0, start-2):start]):\n",
        "            continue\n",
        "        # --- SKIP: If immediately after common handset brands (keep original behaviour) ---\n",
        "        preceding = norm[max(0, start-10):start]\n",
        "        if re.search(r\"(samsung|iphone|apple|huawei|android|pixel|nokia|sony|oneplus|xiaomi|oppo|vivo|motorola|lg|htc)s?\\s*$\", preceding):\n",
        "            continue\n",
        "        device_count += int(m.group())\n",
        "    frame[\"mobile_device_count\"] = device_count\n",
        "\n",
        "    # 5. OS detection: Patterns for explicit and standalone versions\n",
        "    OS_PATTERNS: List[str] = [\n",
        "        r\"\\b(android)[^\\d\\n]{0,10}(\\d{1,2}(?:\\.\\d+){0,2})\",\n",
        "        r\"\\b(ios|ipados)[^\\d\\n]{0,10}(\\d{1,2}(?:\\.\\d+){0,2})\",\n",
        "        r\"\\b(coloros|oxygen\\s?os|one\\sui)[^\\d\\n]{0,10}(\\d{1,2}(?:\\.\\d+){0,2})\",\n",
        "    ]\n",
        "    os_found: List[str] = []\n",
        "    for pat in OS_PATTERNS:\n",
        "        for name, ver in re.findall(pat, lower):\n",
        "            os_found.append(f\"{name.strip()} {ver.strip()}\")\n",
        "            # Unsupported check: Android ≤8, iOS/iPadOS ≤12\n",
        "            try:\n",
        "                major = int(ver.split(\".\")[0])\n",
        "                if (name == \"android\" and major <= 8) or (name in {\"ios\", \"ipados\"} and major <= 12):\n",
        "                    frame[\"unsupported_os\"] = True\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    # 6. Standalone version numbers (kept from original approach)\n",
        "    os_found += re.findall(r\"\\b\\d+\\.\\d+\\.\\d+\\b\", lower)\n",
        "    os_found += re.findall(r\"\\b\\d+h\\d+\\b\", lower)         # e.g. 22h2\n",
        "    os_found += re.findall(r\"build\\s*[-: ]*\\d+\", lower)   # e.g. build 22621\n",
        "    os_found += re.findall(r\"\\(\\s*\\d+\\s*\\)\", lower)       # e.g. (22621)\n",
        "    os_found += re.findall(r\"\\b\\d{5}\\b\", lower)           # e.g. 22631\n",
        "\n",
        "    # spaCy mac product names (unchanged pattern from your other rules, harmless here)\n",
        "    mac_names = {\"ventura\", \"sonoma\", \"sequoia\", \"monterey\", \"big sur\", \"catalina\"}\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PRODUCT\" and ent.text.lower() in mac_names:\n",
        "            os_found.append(\"macos \" + ent.text.lower())\n",
        "\n",
        "    frame[\"os_versions\"] = sorted(set(s.strip() for s in os_found if s.strip()))\n",
        "\n",
        "    # 7. Auto-fail for unsupported OS (original rule)\n",
        "    if frame[\"unsupported_os\"]:\n",
        "        frame[\"auto_fail\"] = True\n",
        "\n",
        "    # 8. Final compliance/MIR/non-compliant logic (exclusive; unchanged)\n",
        "    if frame[\"auto_fail\"]:\n",
        "        frame[\"non_compliant\"] = True\n",
        "    elif frame[\"mobile_device_count\"] > 0 and frame[\"os_versions\"]:\n",
        "        frame[\"compliant\"] = True\n",
        "    elif not frame[\"mobile_device_count\"] and not frame[\"os_versions\"]:\n",
        "        frame[\"needs_more_info\"] = True\n",
        "    else:\n",
        "        frame[\"needs_more_info\"] = True\n",
        "\n",
        "    # Guarantee exclusivity\n",
        "    if frame[\"compliant\"]:\n",
        "        frame[\"needs_more_info\"] = frame[\"non_compliant\"] = False\n",
        "    elif frame[\"needs_more_info\"]:\n",
        "        frame[\"compliant\"] = frame[\"non_compliant\"] = False\n",
        "    else:\n",
        "        frame[\"compliant\"] = frame[\"needs_more_info\"] = False\n",
        "\n",
        "    # --- Reason assignment (unchanged) ---\n",
        "    if frame[\"compliant\"]:\n",
        "        frame[\"reason\"] = f\"Compliant: {frame['mobile_device_count']} mobile device(s) listed with supported OS version(s).\"\n",
        "    elif frame[\"auto_fail\"]:\n",
        "        frame[\"reason\"] = \"Auto-fail: Unsupported OS version detected for mobile/tablet device (e.g. Android ≤8 or iOS ≤12).\"\n",
        "    elif frame[\"no_devices\"]:\n",
        "        frame[\"reason\"] = \"Compliant: Applicant explicitly states no mobile or tablet devices are in use.\"\n",
        "    elif frame[\"mobile_device_count\"] == 0 and not frame[\"os_versions\"]:\n",
        "        frame[\"reason\"] = \"Needs more information: No device count or OS version found in the answer.\"\n",
        "    elif frame[\"mobile_device_count\"] > 0 and not frame[\"os_versions\"]:\n",
        "        frame[\"reason\"] = f\"Needs more information: {frame['mobile_device_count']} device(s) found, but no OS version provided.\"\n",
        "    elif not frame[\"mobile_device_count\"] and frame[\"os_versions\"]:\n",
        "        frame[\"reason\"] = f\"Needs more information: OS version(s) mentioned ({', '.join(frame['os_versions'])}), but no device count identified.\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: Answer incomplete or unclear regarding mobile devices and OS.\"\n",
        "\n",
        "    # 9. ---------------- EOL overlay (A2.6-scoped helpers) ----------------\n",
        "    # If any are EOL -> Fail and overrule; if near EOL -> append note; else leave as-is.\n",
        "    try:\n",
        "        candidates = _a26_eol_slug_and_cycle_candidates_mobile(text)\n",
        "        eol_hits, near_hits = [], []\n",
        "\n",
        "        for slug, ver in candidates:\n",
        "            status, date_str, days_left = _a26_check_eol_status(slug, ver)\n",
        "            label = f\"{slug} {ver}\".strip()\n",
        "            pretty = f\"{label} – EOL {date_str}\" if date_str else label\n",
        "            if status == \"eol\":\n",
        "                eol_hits.append(pretty)\n",
        "            elif status == \"near\":\n",
        "                near_hits.append(f\"{pretty} (in {days_left} days)\")\n",
        "\n",
        "        if eol_hits:\n",
        "            frame[\"auto_fail\"] = True\n",
        "            frame[\"non_compliant\"] = True\n",
        "            frame[\"compliant\"] = False\n",
        "            frame[\"needs_more_info\"] = False\n",
        "            frame[\"eol_hits\"] = eol_hits\n",
        "            # Overrule reason for clarity\n",
        "            frame[\"reason\"] = \"Fail: End-of-life mobile OS detected – \" + \"; \".join(eol_hits) + \".\"\n",
        "        elif near_hits:\n",
        "            frame[\"near_eol_hits\"] = near_hits\n",
        "            note = \" Note: Near end-of-life mobile OS – \" + \"; \".join(near_hits) + \".\"\n",
        "            frame[\"reason\"] = (frame[\"reason\"] + note).strip()\n",
        "\n",
        "    except Exception:\n",
        "        # If the EOL API lookup fails, keep the original label and reason.\n",
        "        pass\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# A2.7 – \"List the networks in scope\"\n",
        "# --------------------------------------------------------------------------- #\n",
        "#  Marking-guide highlights:\n",
        "#   • Expect short descriptive names (“Head Office LAN”, “Prod network”, “Guest Wi-Fi”).\n",
        "#   • Do NOT require IP ranges etc.\n",
        "#   • Home-worker locations are *not required*; if the answer only lists home wifi,\n",
        "#     prompt for clarification.\n",
        "#   • If unclear / blank → assessor must request more info.\n",
        "# --------------------------------------------------------------------------- #\n",
        "from typing import List, Dict\n",
        "import re\n",
        "\n",
        "try:\n",
        "    from text2num import text2num, NumberException\n",
        "except ImportError:\n",
        "    text2num = None\n",
        "\n",
        "def extract_all_numbers(text: str) -> List[int]:\n",
        "    nums = [int(x) for x in re.findall(r\"\\b\\d{1,4}\\b\", text)]\n",
        "    if text2num:\n",
        "        for m in re.finditer(\n",
        "            r'\\b(?:zero|one|two|three|four|five|six|seven|eight|nine|ten|'\n",
        "            r'eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|'\n",
        "            r'eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|'\n",
        "            r'eighty|ninety|hundred|thousand|and|-)+\\b(?:\\s+\\b(?:zero|one|'\n",
        "            r'two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|'\n",
        "            r'thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|'\n",
        "            r'twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|and|-)+\\b)*',\n",
        "            text, re.I):\n",
        "            try:\n",
        "                num = text2num(m.group(0))\n",
        "                nums.append(num)\n",
        "            except Exception:\n",
        "                continue\n",
        "    return nums\n",
        "\n",
        "def rule_a2_7(text: str) -> Dict[str, object]:\n",
        "    \"\"\"\n",
        "    Parse A2.7 (network scope listing): extracts numbers, locations, purpose, and compliance.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    lowered = text.lower()\n",
        "\n",
        "    frame = {\n",
        "        \"network_count\": 0,\n",
        "        \"network_locations\": [],\n",
        "        \"purpose\": [],\n",
        "        \"scope_unclear\": False,\n",
        "        \"needs_more_info\": False,\n",
        "        \"compliant\": False,   # Final compliance label\n",
        "        \"reason\": \"\"\n",
        "    }\n",
        "\n",
        "    # --- Extract numbers (digit and textual) ---\n",
        "    all_numbers = extract_all_numbers(text)\n",
        "    frame[\"network_count\"] = max(all_numbers) if all_numbers else 0\n",
        "\n",
        "    # --- Location extraction using spaCy and fallback ---\n",
        "    locations = set()\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in {\"GPE\", \"LOC\", \"FAC\", \"ORG\", \"FACILITY\", \"LOCATION\"}:\n",
        "            locations.add(ent.text.strip())\n",
        "\n",
        "    fallback_patterns = [\n",
        "        r\"\\b[A-Z][a-z]+\\s+(road|street|lane|office|building|suite|park)\\b\",\n",
        "        r\"\\b(head office|main office|branch office)\\b\",\n",
        "        r\"\\b[a-zA-Z]+,?\\s+[A-Z]{2,3}\\b\",\n",
        "        r\"\\b(uk|kenya|nairobi|aberdeen|scotland|england|wales|ireland|europe|africa)\\b\",\n",
        "        r\"\\bdatacentre\\b\", r\"\\bdatacenter\\b\"\n",
        "    ]\n",
        "    for pat in fallback_patterns:\n",
        "        for match in re.findall(pat, text, flags=re.I):\n",
        "            if isinstance(match, tuple):\n",
        "                locations.add(match[0].strip())\n",
        "            else:\n",
        "                locations.add(match.strip())\n",
        "    frame[\"network_locations\"] = sorted(locations)\n",
        "\n",
        "    # --- Purpose extraction ---\n",
        "    purpose_patterns = {\n",
        "        \"administrative\": r\"\\b(admin|administrative)\\b\",\n",
        "        \"development\": r\"\\b(dev|development|testing)\\b\",\n",
        "        \"home working\": r\"\\b(home|remote|wfh|work\\s+from\\s+home)\\b\",\n",
        "        \"vpn\": r\"\\bvpn\\b\",\n",
        "        \"cloud\": r\"\\b(azure|aws|gcp|cloud)\\b\",\n",
        "        \"datacentre\": r\"\\b(datacentre|datacenter)\\b\",\n",
        "    }\n",
        "    purposes = set()\n",
        "    for label, pattern in purpose_patterns.items():\n",
        "        if re.search(pattern, lowered):\n",
        "            purposes.add(label)\n",
        "    frame[\"purpose\"] = sorted(purposes)\n",
        "\n",
        "    # --- Unclear/needs more info logic ---\n",
        "    if (frame[\"network_count\"] == 0 and\n",
        "        not frame[\"network_locations\"] and\n",
        "        not frame[\"purpose\"]):\n",
        "        frame[\"scope_unclear\"] = True\n",
        "        frame[\"needs_more_info\"] = True\n",
        "\n",
        "    # --- Final compliance label ---\n",
        "    # Mark as compliant if at least one network described (by name/count) AND at least one location or purpose AND not scope_unclear\n",
        "    if (\n",
        "        (frame[\"network_count\"] > 0 or frame[\"network_locations\"] or frame[\"purpose\"]) and\n",
        "        not frame[\"scope_unclear\"] and\n",
        "        not frame[\"needs_more_info\"]\n",
        "    ):\n",
        "        frame[\"compliant\"] = True\n",
        "\n",
        "    # --- Reason assignment ---\n",
        "    if frame[\"compliant\"]:\n",
        "        parts = []\n",
        "        if frame[\"network_count\"]:\n",
        "            parts.append(f\"{frame['network_count']} network(s) listed\")\n",
        "        if frame[\"network_locations\"]:\n",
        "            parts.append(f\"location(s): {', '.join(frame['network_locations'])}\")\n",
        "        if frame[\"purpose\"]:\n",
        "            parts.append(f\"purpose(s): {', '.join(frame['purpose'])}\")\n",
        "        frame[\"reason\"] = \"Compliant: \" + \"; \".join(parts) + \".\"\n",
        "    elif frame[\"scope_unclear\"]:\n",
        "        frame[\"reason\"] = \"Needs more information: No clear network count, location, or purpose provided.\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: Partial scope information detected; clarification required.\"\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# A2.7.1 – \"List the networks in scope\"\n",
        "# --------------------------------------------------------------------------- #\n",
        "from typing import List, Dict\n",
        "import re\n",
        "\n",
        "try:\n",
        "    from text2num import text2num, NumberException\n",
        "except ImportError:\n",
        "    text2num = None\n",
        "\n",
        "def extract_all_numbers(text: str) -> List[int]:\n",
        "    nums = [int(x) for x in re.findall(r\"\\b\\d{1,4}\\b\", text)]\n",
        "    if text2num:\n",
        "        for m in re.finditer(\n",
        "            r'\\b(?:zero|one|two|three|four|five|six|seven|eight|nine|ten|'\n",
        "            r'eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|'\n",
        "            r'eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|'\n",
        "            r'eighty|ninety|hundred|thousand|and|-)+\\b(?:\\s+\\b(?:zero|one|'\n",
        "            r'two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|'\n",
        "            r'thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|'\n",
        "            r'twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|and|-)+\\b)*',\n",
        "            text, re.I):\n",
        "            try:\n",
        "                nums.append(text2num(m.group(0)))\n",
        "            except Exception:\n",
        "                continue\n",
        "    return nums\n",
        "\n",
        "def rule_a2_7_1(text: str) -> Dict[str, object]:\n",
        "    \"\"\"\n",
        "    Parse answer for home/remote worker count & handle clear “no”/none/zero (“nophrase”) as compliant.\n",
        "    \"\"\"\n",
        "\n",
        "    low  = text.lower().strip()\n",
        "    doc  = nlp(text)\n",
        "\n",
        "    frame: Dict[str, object] = {\n",
        "        \"home_worker_count\": 0,\n",
        "        \"hybrid_or_remote\":  False,\n",
        "        \"no_home_workers\":   False,\n",
        "        \"compliant\":         False,\n",
        "        \"needs_more_info\":   False,\n",
        "        \"non_compliant\":     False,\n",
        "        \"auto_fail\":         False,\n",
        "        \"reason\":            \"\"\n",
        "    }\n",
        "\n",
        "    # --- 1. No/none/nil/zero phrases (NOPHRASE LOGIC) ---\n",
        "    NOPHRASES = [\n",
        "        r\"^(none|0|zero|nil)\\s*$\",\n",
        "        r\"\\b(no|none|nil|0|zero)\\s+(home|remote)\\s*workers?\\b\",\n",
        "        r\"\\b(do not|don't|dont|never)\\s+(have|permit|allow|use|employ)\\b.*(home|remote)\\s*workers?\",\n",
        "        r\"\\b(no|none|nil|zero|0)\\s+(remote|home)?\\s*working\\b\",\n",
        "        r\"\\bno\\s+employees\\b\",\n",
        "        r\"\\bnot\\s+applicable\\b\",\n",
        "        r\"\\bn/a\\b\"\n",
        "    ]\n",
        "    if any(re.search(p, low) for p in NOPHRASES):\n",
        "        frame[\"no_home_workers\"] = True\n",
        "        frame[\"compliant\"] = True\n",
        "        return frame\n",
        "\n",
        "    # --- 2. Count extraction (if not an explicit nophrase) ---\n",
        "    nums = extract_all_numbers(low)\n",
        "    # fallback: also use single-word numbers from spaCy tokens\n",
        "    for tok in doc:\n",
        "        if tok.lower_ in {\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"}:\n",
        "            nums.append({\"zero\":0,\"one\":1,\"two\":2,\"three\":3,\"four\":4,\"five\":5,\"six\":6,\"seven\":7,\"eight\":8,\"nine\":9,\"ten\":10}[tok.lower_])\n",
        "\n",
        "    if nums:\n",
        "        frame[\"home_worker_count\"] = max(nums)\n",
        "        # detect remote/hybrid keywords\n",
        "        if re.search(r\"\\b(remote|home[- ]?working|wfh|telecommut(e|ing)|hybrid)\\b\", low):\n",
        "            frame[\"hybrid_or_remote\"] = True\n",
        "\n",
        "    # Any wording that implies remote work, even without a number\n",
        "    if re.search(r\"\\b(hybrid|remote|wfh|work\\s+from\\s+home|flexible\\s+working|vpn)\\b\", low):\n",
        "        frame[\"hybrid_or_remote\"] = True\n",
        "\n",
        "    # --- 3. Consistency & label logic ---\n",
        "    contradiction = frame[\"no_home_workers\"] and (\n",
        "        frame[\"home_worker_count\"] > 0 or frame[\"hybrid_or_remote\"]\n",
        "    )\n",
        "    if contradiction:\n",
        "        frame[\"non_compliant\"] = True\n",
        "    elif frame[\"no_home_workers\"]:\n",
        "        frame[\"compliant\"] = True\n",
        "    elif frame[\"home_worker_count\"] > 0 or frame[\"hybrid_or_remote\"]:\n",
        "        frame[\"compliant\"] = True\n",
        "    else:\n",
        "        frame[\"needs_more_info\"] = True\n",
        "        if not low:\n",
        "            frame[\"auto_fail\"] = True\n",
        "\n",
        "    # --- Reason assignment ---\n",
        "    if frame[\"non_compliant\"]:\n",
        "        frame[\"reason\"] = \"Non-compliant: Contradiction detected — answer states no home workers but also mentions remote/hybrid work or gives a count.\"\n",
        "    elif frame[\"compliant\"]:\n",
        "        if frame[\"no_home_workers\"]:\n",
        "            frame[\"reason\"] = \"Compliant: Applicant clearly states no home or remote workers.\"\n",
        "        elif frame[\"home_worker_count\"] > 0:\n",
        "            frame[\"reason\"] = f\"Compliant: {frame['home_worker_count']} home or remote workers identified.\"\n",
        "        elif frame[\"hybrid_or_remote\"]:\n",
        "            frame[\"reason\"] = \"Compliant: Remote or hybrid working arrangement described.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Compliant: Answer confirms acceptable remote work configuration.\"\n",
        "    elif frame[\"auto_fail\"]:\n",
        "        frame[\"reason\"] = \"Auto-fail: Blank or missing answer text.\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: No clear statement about home/remote workers or working arrangements.\"\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "#  A2.8 – Network-boundary equipment (firewalls / routers) checker\n",
        "#  • Extract make-and-model strings with a single regex bank + spaCy fallback\n",
        "#  • Distinguish between   →   hardware / virtual / “software-only” firewalls\n",
        "#  • Auto-fail if an EOL / unsupported device is mentioned (via static list or endoflife.date)\n",
        "#  • Final tri-state labels:   compliant / needs_more_info / non_compliant_auto_fail\n",
        "#  • Helpers/constants are namespaced for A2.8 to avoid collisions\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n",
        "\n",
        "# -----------------------------\n",
        "# Static unsupported examples (quick guards)\n",
        "# -----------------------------\n",
        "A28_UNSUPPORTED_MODELS = {\n",
        "    # Cisco (older / consumer / EOL-ish examples)\n",
        "    \"asa5505\", \"cisco 887\", \"ciscoc887\",\n",
        "    # Home / ISP kit\n",
        "    \"bt homehub 3\", \"smart hub 1\", \"bthub 1\", \"bt voyager 2100\",\n",
        "    \"thomson tg585\", \"d-link dir-615\", \"tp-link wr841n\",\n",
        "    # Old DrayTeks\n",
        "    \"draytek 2830\",\n",
        "    # Hypervisor version EOL (example)\n",
        "    \"esxi 6.5\",\n",
        "    # Ancient OS examples\n",
        "    \"windows xp\",\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Vendor/model regex patterns\n",
        "# -----------------------------\n",
        "A28_VENDOR_PATTERNS: List[str] = [\n",
        "    r\"(cisco\\s+\\w[\\w\\-]+)\",                           # Cisco\n",
        "    r\"(forti(?:gate|net)\\s+\\w[\\w\\-]+)\",               # Fortinet\n",
        "    r\"(meraki\\s+\\w[\\w\\-]+)\",                          # Meraki\n",
        "    r\"(draytek\\s+\\w[\\w\\-]+)\",                         # Draytek\n",
        "    r\"(palo[\\s\\-]*alto\\s+pa[\\-\\w]+)\",                 # Palo Alto PA-*\n",
        "    r\"(juniper\\s+\\w[\\w\\-]+)\",                         # Juniper\n",
        "    r\"(watchguard\\s+\\w[\\w\\-]+)\",                      # WatchGuard\n",
        "    r\"((?:sophos|barracuda)\\s+\\w[\\w\\-]+)\",            # Sophos/Barracuda\n",
        "    r\"((?:tp[\\-\\s]?link|asus|netgear)\\s+\\w[\\w\\-]+)\",  # TP-Link, ASUS, Netgear\n",
        "    r\"(\\w+\\s+(?:firewall|router|gateway|hub|modem)\\b[\\w\\-]*)\",  # Generic (e.g., \"sky broadband router\")\n",
        "    r\"(\\w+\\s+unifi\\s+\\w[\\w\\-]+)\",                     # UniFi\n",
        "    r\"(sonicwall\\s+\\w[\\w\\-]+)\",                       # SonicWall\n",
        "    r\"(linksys\\s+\\w[\\w\\-]+)\",                         # Linksys\n",
        "    r\"([\\w\\-]+(?:firewall|router|hub|gateway|modem)[\\w\\-]*)\",   # e.g., \"smart5ghub3\"\n",
        "]\n",
        "\n",
        "A28_SOFT_FW_REGEX = re.compile(\n",
        "    r\"(software|virtual|cloud[- ]?(based)?)\\s+(firewall|gateway)|\"\n",
        "    r\"(windows defender|bitdefender|intune|gravityzone|untangle|pfsense|mcafee|azure (virtual )?firewall|windows firewall|sophos endpoint|gravityzone)\",\n",
        "    re.I,\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# EOL overlay helpers (endoflife.date) — A2.8 namespaced\n",
        "# =============================================================================\n",
        "import requests\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "A28_NEAR_EOL_DAYS = 180  # adjust window for “near EOL” notes\n",
        "\n",
        "# Known product slugs on endoflife.date relevant to network edge / firewalls.\n",
        "# (If a slug is wrong or missing, lookups will return “unknown” and won’t change labels.)\n",
        "_A28_EOLDATE_KNOWN_SLUGS = {\n",
        "    # Firewall OS / network OS\n",
        "    \"fortios\": \"fortios\",\n",
        "    \"pan-os\": \"pan-os\", \"panos\": \"pan-os\", \"palo alto pan-os\": \"pan-os\",\n",
        "    \"junos\": \"junos\", \"juniper junos\": \"junos\",\n",
        "    \"routeros\": \"mikrotik-routeros\", \"mikrotik routeros\": \"mikrotik-routeros\",\n",
        "    \"pfsense\": \"pfsense\",\n",
        "    \"opnsense\": \"opnsense\",\n",
        "    \"vyos\": \"vyos\",\n",
        "    \"untangle\": \"untangle-ngfw\", \"untangle ngfw\": \"untangle-ngfw\",\n",
        "    \"sonicos\": \"sonicos\", \"sonicwall os\": \"sonicos\",\n",
        "    \"watchguard fireware\": \"watchguard-fireware\", \"fireware\": \"watchguard-fireware\",\n",
        "    \"sophos utm\": \"sophos-utm\",\n",
        "    \"sophos xg\": \"sophos-firewall\", \"sophos firewall\": \"sophos-firewall\", \"sfos\": \"sophos-firewall\",\n",
        "    # Cisco families\n",
        "    \"ios xe\": \"cisco-ios-xe\", \"ios-xe\": \"cisco-ios-xe\", \"cisco ios xe\": \"cisco-ios-xe\",\n",
        "    \"asa\": \"cisco-asa\", \"asa os\": \"cisco-asa\", \"cisco asa\": \"cisco-asa\",\n",
        "    # Hypervisor (sometimes used for virtual edge)\n",
        "    \"esxi\": \"vmware-esxi\", \"vmware esxi\": \"vmware-esxi\",\n",
        "    # Ubiquiti / Unifi software (mgmt app – not a firewall itself, but sometimes cited)\n",
        "    \"unifi network application\": \"unifi-network-application\",\n",
        "    \"edgeos\": \"ubiquiti-edgeos\",\n",
        "}\n",
        "\n",
        "def _a28_eol_resolve_slug(name: str) -> str | None:\n",
        "    if not name:\n",
        "        return None\n",
        "    h = name.lower().strip()\n",
        "    if h in _A28_EOLDATE_KNOWN_SLUGS:\n",
        "        return _A28_EOLDATE_KNOWN_SLUGS[h]\n",
        "    # fuzzy contains\n",
        "    for k, v in _A28_EOLDATE_KNOWN_SLUGS.items():\n",
        "        if k in h:\n",
        "            return v\n",
        "    return None\n",
        "\n",
        "def _a28_eol_fetch_cycles(slug: str):\n",
        "    \"\"\"Cache product cycle JSON per process.\"\"\"\n",
        "    if not hasattr(_a28_eol_fetch_cycles, \"_cache\"):\n",
        "        _a28_eol_fetch_cycles._cache = {}\n",
        "    cache = _a28_eol_fetch_cycles._cache\n",
        "    if slug in cache:\n",
        "        return cache[slug]\n",
        "    url = f\"https://endoflife.date/api/v1/products/{slug}.json\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=8)\n",
        "        if r.ok:\n",
        "            data = r.json()\n",
        "            cache[slug] = data\n",
        "            return data\n",
        "    except Exception:\n",
        "        pass\n",
        "    cache[slug] = None\n",
        "    return None\n",
        "\n",
        "def _a28_norm(s: str) -> str:\n",
        "    return (s or \"\").lower().replace(\" \", \"\").strip()\n",
        "\n",
        "def _a28_eol_best_match(cycles: list, version_hint: str):\n",
        "    \"\"\"Loose match: exact, contains either way, then prefix (e.g., 17 -> 17.9).\"\"\"\n",
        "    if not cycles or not version_hint:\n",
        "        return None\n",
        "    q = version_hint.strip().lower()\n",
        "    qn = _a28_norm(q)\n",
        "    for c in cycles:\n",
        "        cyc = str(c.get(\"cycle\") or c.get(\"releaseCycle\") or \"\")\n",
        "        cn = _a28_norm(cyc)\n",
        "        if not cn:\n",
        "            continue\n",
        "        if qn == cn or q in cyc.lower() or cyc.lower() in q:\n",
        "            return c\n",
        "        if cyc.lower().startswith(q):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def _a28_eol_status(entry: dict, today=None) -> Tuple[str, str, int]:\n",
        "    \"\"\"\n",
        "    Returns: (status, eol_date_str, days_left)\n",
        "      status ∈ {\"eol\", \"near\", \"ok\", \"unknown\"}\n",
        "    \"\"\"\n",
        "    if not entry:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "    eol = entry.get(\"eol\") or entry.get(\"support\") or entry.get(\"discontinued\")\n",
        "    if not eol or isinstance(eol, bool):\n",
        "        return (\"unknown\", \"\", 0)\n",
        "    try:\n",
        "        d = datetime.fromisoformat(str(eol)).replace(tzinfo=timezone.utc)\n",
        "        today = today or datetime.now(timezone.utc)\n",
        "        days = (d - today).days\n",
        "        if days < 0:\n",
        "            return (\"eol\", d.date().isoformat(), days)\n",
        "        if days <= A28_NEAR_EOL_DAYS:\n",
        "            return (\"near\", d.date().isoformat(), days)\n",
        "        return (\"ok\", d.date().isoformat(), days)\n",
        "    except Exception:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "# --- Extract (product_slug_name, version) pairs from free text ---\n",
        "def _a28_eol_candidates_from_text(text: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Recognize common network/firewall OS names + versions in free text.\n",
        "    If only a product is mentioned without a version, we still return it with \"\" (unknown),\n",
        "    which will not overrule compliance, but could match if the API has a default cycle.\n",
        "    \"\"\"\n",
        "    t = text.lower()\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "\n",
        "    # FortiOS\n",
        "    for m in re.finditer(r\"\\bforti\\s*os\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"fortios\", m.group(1)))\n",
        "    if re.search(r\"\\bfortios\\b|\\bforti\\s*os\\b|\\bforti(?:gate|net)\\b\", t) and not any(p[0] == \"fortios\" for p in pairs):\n",
        "        pairs.append((\"fortios\", \"\"))\n",
        "\n",
        "    # PAN-OS\n",
        "    for m in re.finditer(r\"\\bpan[-\\s]?os\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"pan-os\", m.group(1)))\n",
        "    if re.search(r\"\\bpan[-\\s]?os\\b|\\bpalo\\s*alto\\b\", t) and not any(p[0] == \"pan-os\" for p in pairs):\n",
        "        pairs.append((\"pan-os\", \"\"))\n",
        "\n",
        "    # Junos\n",
        "    for m in re.finditer(r\"\\bjunos\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"junos\", m.group(1)))\n",
        "    if re.search(r\"\\bjunos\\b|\\bjuniper\\b\", t) and not any(p[0] == \"junos\" for p in pairs):\n",
        "        pairs.append((\"junos\", \"\"))\n",
        "\n",
        "    # Cisco ASA OS\n",
        "    for m in re.finditer(r\"\\basa(?:\\s*os)?\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"cisco-asa\", m.group(1)))\n",
        "    if re.search(r\"\\bcisco\\s+asa\\b|\\basa\\b\", t) and not any(p[0] == \"cisco-asa\" for p in pairs):\n",
        "        pairs.append((\"cisco-asa\", \"\"))\n",
        "\n",
        "    # Cisco IOS XE\n",
        "    for m in re.finditer(r\"\\b(?:ios[\\s\\-]?xe|cisco\\s+ios\\s*xe)\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"cisco-ios-xe\", m.group(1)))\n",
        "    if re.search(r\"\\bios[\\s\\-]?xe\\b|\\bcisco\\s+ios\\s*xe\\b\", t) and not any(p[0] == \"cisco-ios-xe\" for p in pairs):\n",
        "        pairs.append((\"cisco-ios-xe\", \"\"))\n",
        "\n",
        "    # SonicOS\n",
        "    for m in re.finditer(r\"\\bsonic\\s*os\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"sonicos\", m.group(1)))\n",
        "    if re.search(r\"\\bsonicwall\\b|\\bsonic\\s*os\\b\", t) and not any(p[0] == \"sonicos\" for p in pairs):\n",
        "        pairs.append((\"sonicos\", \"\"))\n",
        "\n",
        "    # WatchGuard Fireware\n",
        "    for m in re.finditer(r\"\\bfireware\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"watchguard-fireware\", m.group(1)))\n",
        "    if re.search(r\"\\bwatchguard\\b|\\bfireware\\b\", t) and not any(p[0] == \"watchguard-fireware\" for p in pairs):\n",
        "        pairs.append((\"watchguard-fireware\", \"\"))\n",
        "\n",
        "    # Sophos\n",
        "    for m in re.finditer(r\"\\bsfos\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"sophos-firewall\", m.group(1)))\n",
        "    if re.search(r\"\\bsophos\\s+(?:xg|firewall)\\b|\\bsfos\\b\", t) and not any(p[0] == \"sophos-firewall\" for p in pairs):\n",
        "        pairs.append((\"sophos-firewall\", \"\"))\n",
        "    if re.search(r\"\\bsophos\\s+utm\\b\", t):\n",
        "        for m in re.finditer(r\"\\butm\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "            pairs.append((\"sophos-utm\", m.group(1)))\n",
        "        if not any(p[0] == \"sophos-utm\" for p in pairs):\n",
        "            pairs.append((\"sophos-utm\", \"\"))\n",
        "\n",
        "    # pfSense / OPNsense / VyOS / Untangle\n",
        "    for slug, key in ((\"pfsense\", \"pfsense\"), (\"opnsense\", \"opnsense\"), (\"vyos\", \"vyos\"), (\"untangle-ngfw\", \"untangle\")):\n",
        "        for m in re.finditer(rf\"\\b{key}\\b(?:\\s*(\\d+(?:\\.\\d+)*))?\", t):\n",
        "            ver = m.group(1) or \"\"\n",
        "            pairs.append((slug, ver))\n",
        "\n",
        "    # MikroTik RouterOS\n",
        "    for m in re.finditer(r\"\\brouter\\s*os\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"mikrotik-routeros\", m.group(1)))\n",
        "    if re.search(r\"\\bmikro\\s*tik\\b|\\brouter\\s*os\\b\", t) and not any(p[0] == \"mikrotik-routeros\" for p in pairs):\n",
        "        pairs.append((\"mikrotik-routeros\", \"\"))\n",
        "\n",
        "    # VMware ESXi (for virtual appliances)\n",
        "    for m in re.finditer(r\"\\besxi\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"vmware-esxi\", m.group(1)))\n",
        "\n",
        "    # UniFi / EdgeOS (best-effort)\n",
        "    for m in re.finditer(r\"\\bunifi\\s+network\\s+application\\s*(\\d+(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"unifi-network-application\", m.group(1)))\n",
        "    if re.search(r\"\\bedgeos\\b\", t):\n",
        "        pairs.append((\"ubiquiti-edgeos\", \"\"))\n",
        "\n",
        "    # Deduplicate\n",
        "    seen = set()\n",
        "    out: List[Tuple[str, str]] = []\n",
        "    for p in pairs:\n",
        "        if p not in seen:\n",
        "            seen.add(p)\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "def _a28_eol_check(slug_name: str, version_hint: str) -> Tuple[str, str, int, str]:\n",
        "    \"\"\"\n",
        "    Resolve slug, fetch cycles, attempt match, and classify.\n",
        "    Returns: (status, eol_date, days_left, resolved_slug)\n",
        "    \"\"\"\n",
        "    slug = _a28_eol_resolve_slug(slug_name) or slug_name\n",
        "    cycles = _a28_eol_fetch_cycles(slug)\n",
        "    if not cycles:\n",
        "        return (\"unknown\", \"\", 0, slug)\n",
        "    entry = _a28_eol_best_match(cycles, version_hint)\n",
        "    status, eol_date, days = _a28_eol_status(entry)\n",
        "    return (status, eol_date, days, slug)\n",
        "\n",
        "# =============================================================================\n",
        "# Main rule (original logic kept; EOL overlay added)\n",
        "# =============================================================================\n",
        "def rule_a2_8(text: str) -> Dict[str, object]:\n",
        "    low = text.lower()\n",
        "    frame: Dict[str, object] = {\n",
        "        \"device_models\":          [],   # unique make-and-model strings\n",
        "        \"software_firewall_only\": False,\n",
        "        \"unsupported_device_flag\": False,\n",
        "        \"auto_fail\":              False,\n",
        "        \"compliant\":              False,\n",
        "        \"needs_more_info\":        False,\n",
        "        \"reason\":                 \"\",\n",
        "        # EOL diagnostics (optional)\n",
        "        \"eol_hits\":               [],\n",
        "        \"near_eol_hits\":          [],\n",
        "    }\n",
        "\n",
        "    found: List[str] = []\n",
        "    for rx in A28_VENDOR_PATTERNS:\n",
        "        found.extend(re.findall(rx, text, flags=re.I))\n",
        "\n",
        "    # spaCy fallback for PRODUCT/ORG in sentences mentioning firewall/router/etc.\n",
        "    for ent in nlp(text).ents:\n",
        "        if ent.label_ in {\"PRODUCT\", \"ORG\"}:\n",
        "            sent = ent.sent.text.lower()\n",
        "            if any(w in sent for w in (\"firewall\", \"router\", \"gateway\", \"hub\", \"switch\", \"modem\")):\n",
        "                found.append(ent.text)\n",
        "\n",
        "    # Normalize models\n",
        "    found = {re.sub(r\"\\s{2,}\", \" \", f.strip(\" .,-:\")) for f in found if f.strip()}\n",
        "    frame[\"device_models\"] = sorted(found)\n",
        "\n",
        "    # Software-only / virtual mention?\n",
        "    frame[\"software_firewall_only\"] = bool(A28_SOFT_FW_REGEX.search(low))\n",
        "\n",
        "    # Quick static unsupported/EOL triggers\n",
        "    for bad in A28_UNSUPPORTED_MODELS:\n",
        "        if bad in low:\n",
        "            frame[\"unsupported_device_flag\"] = True\n",
        "            frame[\"auto_fail\"] = True\n",
        "            break\n",
        "\n",
        "    # Labeling (original behaviour)\n",
        "    if not frame[\"device_models\"]:\n",
        "        if not re.search(r\"\\b(firewall|router|gateway|hub|modem)\\b\", low):\n",
        "            frame[\"needs_more_info\"] = True\n",
        "    else:\n",
        "        if frame[\"software_firewall_only\"]:\n",
        "            frame[\"needs_more_info\"] = True\n",
        "        else:\n",
        "            frame[\"compliant\"] = True\n",
        "\n",
        "    # If auto_fail via static list, override now\n",
        "    if frame[\"auto_fail\"]:\n",
        "        frame[\"compliant\"] = False\n",
        "        frame[\"needs_more_info\"] = False\n",
        "\n",
        "    # --- Reason (pre-EOL overlay) ---\n",
        "    if frame[\"auto_fail\"]:\n",
        "        frame[\"reason\"] = \"Auto-fail: Unsupported or end-of-life firewall device detected in response.\"\n",
        "    elif frame[\"compliant\"]:\n",
        "        frame[\"reason\"] = (\n",
        "            \"Compliant: Supported hardware firewall device(s) identified — \"\n",
        "            + \", \".join(frame[\"device_models\"]) + \".\"\n",
        "        )\n",
        "    elif frame[\"software_firewall_only\"]:\n",
        "        frame[\"reason\"] = (\n",
        "            \"Needs more information: Only software or cloud-based firewall mentioned; \"\n",
        "            \"no hardware model identified.\"\n",
        "        )\n",
        "    elif not frame[\"device_models\"]:\n",
        "        frame[\"reason\"] = \"Needs more information: No identifiable firewall, router, or gateway device model provided.\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: Hardware firewall model unclear or incomplete.\"\n",
        "\n",
        "    # =============================================================================\n",
        "    # EOL overlay via endoflife.date (augments/overrides AFTER original logic)\n",
        "    #   • Parse network OS (FortiOS, PAN-OS, JUNOS, ASA OS, RouterOS, etc.)\n",
        "    #   • If any EOL → flip to auto-fail and override reason\n",
        "    #   • If any near-EOL → append note but keep label\n",
        "    # =============================================================================\n",
        "    try:\n",
        "        candidates = _a28_eol_candidates_from_text(text)\n",
        "        eol_hits, near_hits = [], []\n",
        "\n",
        "        for prod_key, ver in candidates:\n",
        "            status, eol_date, days_left, resolved_slug = _a28_eol_check(prod_key, ver)\n",
        "            label = f\"{resolved_slug} {ver}\".strip()\n",
        "            if status == \"eol\":\n",
        "                eol_hits.append(f\"{label} – EOL {eol_date}\")\n",
        "            elif status == \"near\":\n",
        "                near_hits.append(f\"{label} – EOL {eol_date} (in {days_left} days)\")\n",
        "\n",
        "        if eol_hits:\n",
        "            frame[\"auto_fail\"] = True\n",
        "            frame[\"compliant\"] = False\n",
        "            frame[\"needs_more_info\"] = False\n",
        "            frame[\"eol_hits\"] = eol_hits\n",
        "            frame[\"reason\"] = \"Auto-fail: End-of-life network OS detected — \" + \"; \".join(eol_hits) + \".\"\n",
        "        elif near_hits:\n",
        "            frame[\"near_eol_hits\"] = near_hits\n",
        "            frame[\"reason\"] = frame[\"reason\"].rstrip(\".\") + \". Note: Near end-of-life — \" + \"; \".join(near_hits) + \".\"\n",
        "\n",
        "    except Exception:\n",
        "        # If EOL lookups fail, keep original label/reason\n",
        "        pass\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------ #\n",
        "#  A2.9 – “List all 3rd‑party Cloud Services in use”\n",
        "# ------------------------------------------------------------------ #\n",
        "def rule_a2_9(text: str) -> Dict[str, object]:\n",
        "    doc = nlp(text)\n",
        "    low = text.lower()\n",
        "\n",
        "    frame: Dict[str, object] = {\n",
        "        \"cloud_services\":     [],               # normalised names\n",
        "        \"service_types\":      {\"saas\": [], \"paas\": [], \"iaas\": []},\n",
        "        \"missing_cloud_flag\": False,\n",
        "        \"compliant\":          False,\n",
        "        \"needs_more_info\":    False,\n",
        "        \"non_compliant_missing\": False,\n",
        "        \"reason\":             \"\"\n",
        "    }\n",
        "\n",
        "    KNOWN_CLOUD = {\n",
        "        # -------- SaaS --------\n",
        "        \"microsoft 365\":     \"saas\",\n",
        "        \"ms365\":             \"saas\",\n",
        "        \"office 365\":        \"saas\",\n",
        "        \"microsoft onedrive\":\"saas\",\n",
        "        \"onedrive\":          \"saas\",\n",
        "        \"google workspace\":  \"saas\",\n",
        "        \"google drive\":      \"saas\",\n",
        "        \"gdrive\":            \"saas\",\n",
        "        \"dropbox\":           \"saas\",\n",
        "        \"exchange online\":   \"saas\",\n",
        "        \"outlook online\":    \"saas\",\n",
        "        \"salesforce\":        \"saas\",\n",
        "        \"slack\":             \"saas\",\n",
        "        \"zoom\":              \"saas\",\n",
        "        \"github\":            \"saas\",\n",
        "        \"gitlab\":            \"saas\",\n",
        "        \"jira\":              \"saas\",\n",
        "        \"zendesk\":           \"saas\",\n",
        "        \"hubspot\":           \"saas\",\n",
        "        \"xero\":              \"saas\",\n",
        "        # -------- PaaS --------\n",
        "        \"azure app service\":         \"paas\",\n",
        "        \"aws elastic beanstalk\":     \"paas\",\n",
        "        \"heroku\":                    \"paas\",\n",
        "        \"google app engine\":         \"paas\",\n",
        "        \"oracle cloud functions\":    \"paas\",\n",
        "        \"cloud run\":                 \"paas\",\n",
        "        # -------- IaaS --------\n",
        "        \"azure\":             \"iaas\",\n",
        "        \"azure ad\":          \"iaas\",\n",
        "        \"aws\":               \"iaas\",\n",
        "        \"amazon web services\":\"iaas\",\n",
        "        \"amazon ec2\":        \"iaas\",\n",
        "        \"s3\":                \"iaas\",\n",
        "        \"gcp\":               \"iaas\",\n",
        "        \"google cloud platform\":\"iaas\",\n",
        "        \"google compute engine\":\"iaas\",\n",
        "        \"ibm cloud\":         \"iaas\",\n",
        "        \"oracle cloud\":      \"iaas\",\n",
        "        \"digitalocean\":      \"iaas\",\n",
        "        \"linode\":            \"iaas\",\n",
        "        \"rackspace\":         \"iaas\",\n",
        "        \"vultr\":             \"iaas\",\n",
        "        \"cloud storage\":     \"iaas\",\n",
        "    }\n",
        "\n",
        "    # --- 0. Immediate compliance if 'attached' ---\n",
        "    if \"attached\" in low:\n",
        "        frame[\"compliant\"] = True\n",
        "        return frame\n",
        "\n",
        "    # --- 1a. regex scan over text ---\n",
        "    matched = []\n",
        "    low_nospaces = re.sub(r'\\s+', '', low)  # 'microsoft one drive' -> 'microsoftonedrive'\n",
        "    for svc in KNOWN_CLOUD:\n",
        "        svc_nospaces = svc.replace(\" \", \"\")\n",
        "        if svc in low or svc_nospaces in low_nospaces:\n",
        "            matched.append(svc)\n",
        "\n",
        "    # --- spaCy fallback: add any ORG/PRODUCT entity with cloud-ish keywords ---\n",
        "    cloud_fallback_keywords = (\n",
        "        \"cloud\", \"drive\", \"storage\", \"online\", \"365\", \"gcp\", \"aws\", \"azure\", \"exchange\",\n",
        "        \"service\", \"apps\", \"saas\", \"paas\", \"iaas\", \".com\"\n",
        "    )\n",
        "    for ent in doc.ents:\n",
        "        t = ent.text.lower().strip()\n",
        "        # Only add if not already matched (ignore case)\n",
        "        if ent.label_ in {\"ORG\", \"PRODUCT\"} and t not in [m.lower() for m in matched]:\n",
        "            if any(kw in t for kw in cloud_fallback_keywords):\n",
        "                matched.append(ent.text.strip())\n",
        "\n",
        "    # --- Remove duplicates (case-insensitive), preserve first occurrence's spelling ---\n",
        "    seen = set()\n",
        "    cloud_services = []\n",
        "    for s in matched:\n",
        "        key = s.lower()\n",
        "        if key not in seen:\n",
        "            cloud_services.append(s)\n",
        "            seen.add(key)\n",
        "    frame[\"cloud_services\"] = cloud_services\n",
        "\n",
        "    # --- 2a. explicit labels in applicant answer ---\n",
        "    if \"saas\" in low:\n",
        "        frame[\"service_types\"][\"saas\"] += frame[\"cloud_services\"]\n",
        "    if \"paas\" in low:\n",
        "        frame[\"service_types\"][\"paas\"] += frame[\"cloud_services\"]\n",
        "    if \"iaas\" in low:\n",
        "        frame[\"service_types\"][\"iaas\"] += frame[\"cloud_services\"]\n",
        "\n",
        "    # --- 2b. fall back to dictionary mapping ---\n",
        "    for svc in frame[\"cloud_services\"]:\n",
        "        svc_type = KNOWN_CLOUD.get(svc.lower())\n",
        "        if svc_type:\n",
        "            frame[\"service_types\"][svc_type].append(svc)\n",
        "\n",
        "    # dedupe lists\n",
        "    for k in frame[\"service_types\"]:\n",
        "        frame[\"service_types\"][k] = sorted(set(frame[\"service_types\"][k]))\n",
        "\n",
        "    # --- 3. Final compliance logic ---\n",
        "    if frame[\"cloud_services\"]:\n",
        "        frame[\"compliant\"] = True\n",
        "    else:\n",
        "        # blank / “none” / suspicious negation\n",
        "        if re.search(r\"\\b(none?|n/a|no cloud|not applicable)\\b\", low):\n",
        "            frame[\"non_compliant_missing\"] = True\n",
        "        else:\n",
        "            frame[\"needs_more_info\"] = True\n",
        "        frame[\"missing_cloud_flag\"] = True\n",
        "\n",
        "    # --- Reason assignment ---\n",
        "    if frame[\"compliant\"]:\n",
        "        svc_list = \", \".join(frame[\"cloud_services\"])\n",
        "        type_summary = []\n",
        "        for k in [\"saas\", \"paas\", \"iaas\"]:\n",
        "            if frame[\"service_types\"][k]:\n",
        "                type_summary.append(f\"{k.upper()}: {', '.join(frame['service_types'][k])}\")\n",
        "        frame[\"reason\"] = f\"Compliant: Cloud services listed – {svc_list}. Types: {'; '.join(type_summary)}.\"\n",
        "    elif frame[\"non_compliant_missing\"]:\n",
        "        frame[\"reason\"] = \"Non-compliant: Applicant states no cloud services in use, which contradicts expected usage.\"\n",
        "    elif frame[\"needs_more_info\"]:\n",
        "        frame[\"reason\"] = \"Needs more information: No clear cloud services identified; response is vague or incomplete.\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: Cloud service listing unclear or improperly formatted.\"\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# A4.12 — “Software firewall availability” checker (UPDATED with EOL overlay)\n",
        "# Namespaced helpers/constants to avoid collisions with other rules\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "from typing import Dict, List, Tuple\n",
        "import re\n",
        "\n",
        "# =========================\n",
        "# EOL helpers (endoflife.date) — A4.12 namespace\n",
        "# =========================\n",
        "import requests\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "A412_NEAR_EOL_DAYS = 180  # tweak the “near EOL” window for notes\n",
        "\n",
        "# Map product name → endoflife.date slug (client OS & common endpoint/server OS that might appear here)\n",
        "_A412_EOL_SLUGS = {\n",
        "    # Windows client\n",
        "    \"windows\": \"windows\",\n",
        "    \"windows 10\": \"windows\",\n",
        "    \"windows 11\": \"windows\",\n",
        "    # macOS\n",
        "    \"macos\": \"macos\",\n",
        "    \"os x\": \"macos\",\n",
        "    # Apple mobile/tablet\n",
        "    \"ios\": \"ios\",\n",
        "    \"ipados\": \"ipados\",\n",
        "    # Android / ChromeOS\n",
        "    \"android\": \"android\",\n",
        "    \"chromeos\": \"chromeos\",\n",
        "    \"chromebook\": \"chromeos\",\n",
        "    # Linux distros (desktop/server)\n",
        "    \"ubuntu\": \"ubuntu\",\n",
        "    \"debian\": \"debian\",\n",
        "    \"fedora\": \"fedora\",\n",
        "    \"centos\": \"centos\",\n",
        "    \"redhat\": \"rhel\",\n",
        "    \"red hat\": \"rhel\",\n",
        "    \"red hat enterprise linux\": \"rhel\",\n",
        "    \"rhel\": \"rhel\",\n",
        "    \"arch\": \"archlinux\", \"arch linux\": \"archlinux\",\n",
        "    \"kali\": \"kali\", \"kali linux\": \"kali\",\n",
        "    \"parrot\": \"parrot\", \"parrot os\": \"parrot\",\n",
        "    \"raspbian\": \"raspberry-pi-os\", \"raspberry pi os\": \"raspberry-pi-os\",\n",
        "    \"opensuse\": \"opensuse\", \"open suse\": \"opensuse\",\n",
        "    # (If they mention server OS here, we’ll still evaluate)\n",
        "    \"windows server\": \"windows-server\",\n",
        "}\n",
        "\n",
        "# macOS codenames → help us surface explicit “macOS <code>” candidates\n",
        "_A412_MAC_CODENAMES = {\n",
        "    \"catalina\", \"big sur\", \"monterey\", \"ventura\", \"sonoma\", \"sequoia\"\n",
        "}\n",
        "\n",
        "def _a412_eol_resolve_slug(name: str) -> str | None:\n",
        "    if not name:\n",
        "        return None\n",
        "    h = name.lower().strip()\n",
        "    if h in _A412_EOL_SLUGS:\n",
        "        return _A412_EOL_SLUGS[h]\n",
        "    # fuzzy contains\n",
        "    for k, v in _A412_EOL_SLUGS.items():\n",
        "        if k in h:\n",
        "            return v\n",
        "    return None\n",
        "\n",
        "def _a412_eol_fetch_cycles(slug: str):\n",
        "    \"\"\"Fetch & cache product cycles from endoflife.date.\"\"\"\n",
        "    if not hasattr(_a412_eol_fetch_cycles, \"_cache\"):\n",
        "        _a412_eol_fetch_cycles._cache = {}\n",
        "    cache = _a412_eol_fetch_cycles._cache\n",
        "    if slug in cache:\n",
        "        return cache[slug]\n",
        "    url = f\"https://endoflife.date/api/v1/products/{slug}.json\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=8)\n",
        "        if r.ok:\n",
        "            cache[slug] = r.json()\n",
        "            return cache[slug]\n",
        "    except Exception:\n",
        "        pass\n",
        "    cache[slug] = None\n",
        "    return None\n",
        "\n",
        "def _a412_norm(s: str) -> str:\n",
        "    return (s or \"\").lower().replace(\" \", \"\").strip()\n",
        "\n",
        "def _a412_eol_best_match(cycles: list, version_hint: str):\n",
        "    \"\"\"Loose match between version_hint and a cycle entry.\"\"\"\n",
        "    if not cycles or not version_hint:\n",
        "        return None\n",
        "    q = version_hint.strip().lower()\n",
        "    qn = _a412_norm(q)\n",
        "    for c in cycles:\n",
        "        cyc = str(c.get(\"cycle\") or c.get(\"releaseCycle\") or \"\")\n",
        "        cn = _a412_norm(cyc)\n",
        "        if not cn:\n",
        "            continue\n",
        "        if qn == cn or q in cyc.lower() or cyc.lower() in q:\n",
        "            return c\n",
        "        if cyc.lower().startswith(q):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def _a412_eol_status(entry: dict, today=None) -> Tuple[str, str, int]:\n",
        "    \"\"\"\n",
        "    Returns: (status, eol_date_str, days_left)\n",
        "      status ∈ {\"eol\", \"near\", \"ok\", \"unknown\"}\n",
        "    \"\"\"\n",
        "    if not entry:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "    eol = entry.get(\"eol\") or entry.get(\"support\") or entry.get(\"discontinued\")\n",
        "    if not eol or isinstance(eol, bool):\n",
        "        return (\"unknown\", \"\", 0)\n",
        "    try:\n",
        "        d = datetime.fromisoformat(str(eol)).replace(tzinfo=timezone.utc)\n",
        "        today = today or datetime.now(timezone.utc)\n",
        "        days = (d - today).days\n",
        "        if days < 0:\n",
        "            return (\"eol\", d.date().isoformat(), days)\n",
        "        if days <= A412_NEAR_EOL_DAYS:\n",
        "            return (\"near\", d.date().isoformat(), days)\n",
        "        return (\"ok\", d.date().isoformat(), days)\n",
        "    except Exception:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "def _a412_candidate_pairs_from_text(text: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Return (slug-ish name, version_hint) pairs from free text.\n",
        "    We keep it liberal: Windows 10/11 + 22H2, macOS codename/number,\n",
        "    iOS/iPadOS/Android versions, ChromeOS, Ubuntu/Debian/etc.\n",
        "    \"\"\"\n",
        "    t = text.lower()\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "\n",
        "    # Windows 10/11 with optional channel (e.g., 21H2/22H2)\n",
        "    for m in re.finditer(r\"\\bwindows\\s+(10|11)\\s*(\\d{2}h\\d)?\\b\", t):\n",
        "        base = m.group(1)\n",
        "        ch = m.group(2) or \"\"\n",
        "        pairs.append((\"windows\", (base + (\" \" + ch.upper() if ch else \"\")).strip()))\n",
        "    # Bare \"windows 10/11\" if not already captured\n",
        "    if re.search(r\"\\bwindows\\s+10\\b\", t) and not any(p[1].startswith(\"10\") for p in pairs):\n",
        "        pairs.append((\"windows\", \"10\"))\n",
        "    if re.search(r\"\\bwindows\\s+11\\b\", t) and not any(p[1].startswith(\"11\") for p in pairs):\n",
        "        pairs.append((\"windows\", \"11\"))\n",
        "\n",
        "    # macOS by number (13.6) and codename\n",
        "    for m in re.finditer(r\"\\bmac\\s?os\\s*(\\d{2}(?:\\.\\d+){0,2})\\b\", t):\n",
        "        pairs.append((\"macos\", m.group(1)))\n",
        "    for code in _A412_MAC_CODENAMES:\n",
        "        if re.search(rf\"\\b{code}\\b\", t):\n",
        "            pairs.append((\"macos\", code))\n",
        "\n",
        "    # iOS / iPadOS\n",
        "    for m in re.finditer(r\"\\b(iOS|iPadOS)\\s*(\\d{1,2}(?:\\.\\d+){0,2})\\b\", t, flags=re.I):\n",
        "        pairs.append((m.group(1).lower(), m.group(2)))\n",
        "\n",
        "    # Android (major.minor)\n",
        "    for m in re.finditer(r\"\\bandroid\\s*(\\d{1,2}(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"android\", m.group(1)))\n",
        "    if re.search(r\"\\bandroid\\b\", t) and not any(p[0] == \"android\" for p in pairs):\n",
        "        pairs.append((\"android\", \"\"))\n",
        "\n",
        "    # ChromeOS\n",
        "    for m in re.finditer(r\"\\bchrome\\s?os\\s*(\\d+(?:\\.\\d+)*)?\\b\", t):\n",
        "        pairs.append((\"chromeos\", (m.group(1) or \"\").strip()))\n",
        "    if re.search(r\"\\bchromebook\\b\", t) and not any(p[0] == \"chromeos\" for p in pairs):\n",
        "        pairs.append((\"chromeos\", \"\"))\n",
        "\n",
        "    # Linux desktop/server distros\n",
        "    for name, pat in [\n",
        "        (\"ubuntu\", r\"\\bubuntu\\s+(\\d{2}\\.\\d{2})\\b\"),\n",
        "        (\"debian\", r\"\\bdebian\\s+(\\d{1,2})\\b\"),\n",
        "        (\"fedora\", r\"\\bfedora\\s+(\\d{1,3})\\b\"),\n",
        "        (\"centos\", r\"\\bcentos\\s+(\\d{1,2})\\b\"),\n",
        "        (\"rhel\", r\"\\b(?:rhel|red\\s*hat(?:\\s+enterprise)?\\s+linux)\\s+(\\d{1,2})\\b\"),\n",
        "        (\"archlinux\", r\"\\barch(?:\\s+linux)?\\b\"),\n",
        "        (\"kali\", r\"\\bkali(?:\\s+linux)?\\b\"),\n",
        "        (\"parrot\", r\"\\bparrot(?:\\s+os)?\\b\"),\n",
        "        (\"raspberry-pi-os\", r\"\\b(?:raspbian|raspberry\\s*pi\\s*os)\\s*(\\d{1,2})?\\b\"),\n",
        "        (\"opensuse\", r\"\\bopen\\s*suse\\b|\\bopensuse\\b\"),\n",
        "    ]:\n",
        "        m = re.search(pat, t)\n",
        "        if m:\n",
        "            ver = \"\"\n",
        "            if m.groups():\n",
        "                # use first capturing group if numeric found\n",
        "                g = next((g for g in m.groups() if g), \"\")\n",
        "                ver = g.strip()\n",
        "            pairs.append((name, ver))\n",
        "\n",
        "    # Windows Server (if people mention it here)\n",
        "    for m in re.finditer(r\"\\bwindows\\s+server\\s+((?:20)?\\d{2})(?:\\s*r2)?\\b\", t):\n",
        "        pairs.append((\"windows-server\", m.group(1)))\n",
        "\n",
        "    # Deduplicate pairs in-order\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for p in pairs:\n",
        "        if p not in seen:\n",
        "            seen.add(p)\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "def _a412_eol_check(product_hint: str, version_hint: str) -> Tuple[str, str, int, str]:\n",
        "    \"\"\"\n",
        "    Resolve product slug → fetch cycles → best-match cycle → compute status.\n",
        "    Returns: (status, eol_date, days_left, slug)\n",
        "    \"\"\"\n",
        "    slug = _a412_eol_resolve_slug(product_hint) or product_hint\n",
        "    cycles = _a412_eol_fetch_cycles(slug)\n",
        "    if not cycles:\n",
        "        return (\"unknown\", \"\", 0, slug)\n",
        "    entry = _a412_eol_best_match(cycles, version_hint)\n",
        "    status, eol_date, days = _a412_eol_status(entry)\n",
        "    return (status, eol_date, days, slug)\n",
        "\n",
        "# =========================\n",
        "# Main rule (original logic + EOL overlay)\n",
        "# =========================\n",
        "def rule_a4_12(text):\n",
        "    doc = nlp(text)\n",
        "    lowered = text.lower()\n",
        "\n",
        "    frame = {\n",
        "        \"mentions_os\": [],\n",
        "        \"firewall_not_available_claim\": False,\n",
        "        \"unsupported_os_claim\": False,\n",
        "        \"contradiction_detected\": False,\n",
        "        \"compliant\": False,\n",
        "        \"non_compliant\": False,\n",
        "        \"needs_more_info\": False,\n",
        "        # --- new fields (EOL overlay) ---\n",
        "        \"auto_fail\": False,\n",
        "        \"eol_hits\": [],\n",
        "        \"near_eol_hits\": [],\n",
        "        \"reason\": \"\"\n",
        "    }\n",
        "\n",
        "    # Known OS and related product names (expanded)\n",
        "    known_os = [\n",
        "        \"windows 10\", \"windows 11\", \"windows\",\n",
        "        \"macos\", \"os x\",\n",
        "        \"ubuntu\", \"debian\", \"linux\", \"redhat\", \"red hat\", \"rhel\",\n",
        "        \"centos\", \"arch\", \"fedora\", \"kali\", \"parrot\",\n",
        "        \"embedded linux\", \"unix\", \"custom os\", \"bespoke\", \"custom build\",\n",
        "        \"android\", \"ios\", \"ipados\", \"raspbian\", \"raspberry pi os\", \"chromeos\", \"chromebook\",\n",
        "        \"windows server\"\n",
        "    ]\n",
        "    known_os = [k.lower() for k in known_os]\n",
        "\n",
        "    # Extract OS/product mentions using spaCy\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in (\"PRODUCT\", \"ORG\", \"NORP\"):\n",
        "            val = ent.text.lower()\n",
        "            if val not in frame[\"mentions_os\"]:\n",
        "                frame[\"mentions_os\"].append(val)\n",
        "\n",
        "    # Also match any from the known_os list (string match fallback)\n",
        "    for os_name in known_os:\n",
        "        if os_name in lowered and os_name not in frame[\"mentions_os\"]:\n",
        "            frame[\"mentions_os\"].append(os_name)\n",
        "\n",
        "    # Detect claim that software firewall is unavailable/missing\n",
        "    if re.search(r\"(no (built[- ]?in |default )?software firewall|not (have|include|contain) software firewall|firewall (not|never) available|cannot.*firewall)\", lowered):\n",
        "        frame[\"firewall_not_available_claim\"] = True\n",
        "\n",
        "    # Detect claim of unsupported or custom/uncommon OS\n",
        "    if re.search(r\"(embedded linux|bespoke|custom os|bare metal|minimal linux|no gui|headless|custom build)\", lowered):\n",
        "        frame[\"unsupported_os_claim\"] = True\n",
        "\n",
        "    # Contradiction logic: claims common OS lacks firewall (which is false)\n",
        "    contradiction_oses = {\"windows\", \"windows 10\", \"windows 11\", \"macos\", \"os x\", \"ubuntu\", \"debian\", \"fedora\", \"arch\", \"centos\", \"redhat\"}\n",
        "    for os in frame[\"mentions_os\"]:\n",
        "        if os in contradiction_oses and frame[\"firewall_not_available_claim\"]:\n",
        "            frame[\"contradiction_detected\"] = True\n",
        "            frame[\"non_compliant\"] = True\n",
        "\n",
        "    # Needs more info if claim is made but OS not named or is too vague\n",
        "    if frame[\"firewall_not_available_claim\"] and not frame[\"mentions_os\"]:\n",
        "        frame[\"needs_more_info\"] = True\n",
        "\n",
        "    # If claim is made, OS is *not* a real exception, but no contradiction: non-compliance\n",
        "    if frame[\"firewall_not_available_claim\"] and not frame[\"unsupported_os_claim\"] and not frame[\"contradiction_detected\"]:\n",
        "        frame[\"non_compliant\"] = True\n",
        "\n",
        "    # If they list only custom OS (real exceptions): compliant\n",
        "    if frame[\"firewall_not_available_claim\"] and frame[\"unsupported_os_claim\"]:\n",
        "        frame[\"compliant\"] = True\n",
        "\n",
        "    # If applicant simply lists only OSes where a firewall is always available, and doesn't claim otherwise: compliant\n",
        "    if (not frame[\"firewall_not_available_claim\"]\n",
        "        and all(os in contradiction_oses for os in frame[\"mentions_os\"])\n",
        "        and frame[\"mentions_os\"]):\n",
        "        frame[\"compliant\"] = True\n",
        "\n",
        "    # If nothing is mentioned, needs more info\n",
        "    if not frame[\"mentions_os\"] and not frame[\"firewall_not_available_claim\"]:\n",
        "        frame[\"needs_more_info\"] = True\n",
        "\n",
        "    # --- Reason assignment (pre-EOL overlay) ---\n",
        "    if frame[\"compliant\"]:\n",
        "        if frame[\"unsupported_os_claim\"]:\n",
        "            frame[\"reason\"] = \"Compliant: Software firewall not available on unsupported or custom OS.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = f\"Compliant: Listed OS(es) ({', '.join(frame['mentions_os'])}) are known to include built-in firewalls and no claim was made otherwise.\"\n",
        "    elif frame[\"non_compliant\"]:\n",
        "        if frame[\"contradiction_detected\"]:\n",
        "            frame[\"reason\"] = f\"Non-compliant: Claimed firewall not available on OS ({', '.join(frame['mentions_os'])}) known to include one.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Non-compliant: Claimed lack of firewall on standard OS without justification.\"\n",
        "    elif frame[\"needs_more_info\"]:\n",
        "        if frame[\"firewall_not_available_claim\"] and not frame[\"mentions_os\"]:\n",
        "            frame[\"reason\"] = \"Needs more information: Applicant claimed firewall is unavailable but did not specify which OS.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Needs more information: No OS or firewall-related details provided.\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: Incomplete or unclear firewall availability explanation.\"\n",
        "\n",
        "    # =========================\n",
        "    # EOL overlay (runs last)\n",
        "    #   • If any EOL → mark auto_fail + non_compliant, override reason\n",
        "    #   • If any near EOL → append note to reason\n",
        "    # =========================\n",
        "    try:\n",
        "        candidates = _a412_candidate_pairs_from_text(text)\n",
        "        eol_hits, near_hits = [], []\n",
        "\n",
        "        for prod_key, ver in candidates:\n",
        "            status, eol_date, days_left, slug = _a412_eol_check(prod_key, ver)\n",
        "            label = f\"{slug} {ver}\".strip()\n",
        "            if status == \"eol\":\n",
        "                eol_hits.append(f\"{label} – EOL {eol_date}\")\n",
        "            elif status == \"near\":\n",
        "                near_hits.append(f\"{label} – EOL {eol_date} (in {days_left} days)\")\n",
        "\n",
        "        if eol_hits:\n",
        "            frame[\"auto_fail\"] = True\n",
        "            frame[\"non_compliant\"] = True\n",
        "            frame[\"compliant\"] = False\n",
        "            frame[\"needs_more_info\"] = False\n",
        "            frame[\"eol_hits\"] = eol_hits\n",
        "            frame[\"reason\"] = \"Auto-fail: End-of-life OS detected — \" + \"; \".join(eol_hits) + \".\"\n",
        "        elif near_hits:\n",
        "            frame[\"near_eol_hits\"] = near_hits\n",
        "            frame[\"reason\"] = frame[\"reason\"].rstrip(\".\") + \". Note: Near end-of-life — \" + \"; \".join(near_hits) + \".\"\n",
        "\n",
        "    except Exception:\n",
        "        # If EOL lookups fail, we keep the original label/reason intact.\n",
        "        pass\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# A6.2.1 — Browser inventory checker (UPDATED with EOL overlay via endoflife.date)\n",
        "# Namespaced helpers/constants only; functionality unchanged\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n",
        "\n",
        "# =========================\n",
        "# EOL helpers (endoflife.date) — A6.2.1 namespace\n",
        "# =========================\n",
        "import requests\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "A621_NEAR_EOL_DAYS = 180  # tweak as needed\n",
        "\n",
        "# Map common names → endoflife.date slugs\n",
        "_A621_EBROWSER_SLUGS = {\n",
        "    \"chrome\": \"chrome\",\n",
        "    \"chromium\": \"chromium\",\n",
        "    \"edge\": \"edge\",\n",
        "    \"firefox\": \"firefox\",\n",
        "    \"firefox esr\": \"firefox-esr\",\n",
        "    \"safari\": \"safari\",\n",
        "    \"opera\": \"opera\",\n",
        "    \"brave\": \"brave\",\n",
        "    \"vivaldi\": \"vivaldi\",\n",
        "    \"samsung internet\": \"samsung-internet\",\n",
        "}\n",
        "\n",
        "# synonym patterns that resolve to the above slugs\n",
        "_A621_EBROWSER_PATTERNS = [\n",
        "    (r\"samsung\\s+internet\", \"samsung internet\"),\n",
        "    (r\"firefox\\s+esr|esr\\s+firefox\", \"firefox esr\"),\n",
        "    (r\"google\\s*chrome|chrome\", \"chrome\"),\n",
        "    (r\"microsoft\\s*edge|edge\", \"edge\"),\n",
        "    (r\"mozilla\\s*firefox|firefox\", \"firefox\"),\n",
        "    (r\"safari\", \"safari\"),\n",
        "    (r\"opera\", \"opera\"),\n",
        "    (r\"chromium\", \"chromium\"),\n",
        "    (r\"brave\", \"brave\"),\n",
        "    (r\"vivaldi\", \"vivaldi\"),\n",
        "]\n",
        "\n",
        "def _a621_eol_slug_for_browser(name: str) -> str | None:\n",
        "    if not name:\n",
        "        return None\n",
        "    n = name.lower().strip()\n",
        "    return _A621_EBROWSER_SLUGS.get(n)\n",
        "\n",
        "def _a621_eol_fetch_cycles(slug: str):\n",
        "    \"\"\"Fetch & cache product cycles from endoflife.date.\"\"\"\n",
        "    if not hasattr(_a621_eol_fetch_cycles, \"_cache\"):\n",
        "        _a621_eol_fetch_cycles._cache = {}\n",
        "    cache = _a621_eol_fetch_cycles._cache\n",
        "    if slug in cache:\n",
        "        return cache[slug]\n",
        "    url = f\"https://endoflife.date/api/v1/products/{slug}.json\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=8)\n",
        "        if r.ok:\n",
        "            cache[slug] = r.json()\n",
        "            return cache[slug]\n",
        "    except Exception:\n",
        "        pass\n",
        "    cache[slug] = None\n",
        "    return None\n",
        "\n",
        "def _a621_norm(s: str) -> str:\n",
        "    return (s or \"\").lower().replace(\" \", \"\").strip()\n",
        "\n",
        "def _a621_eol_best_match(cycles: list, version_hint: str):\n",
        "    \"\"\"Loose match between version_hint and a cycle entry.\"\"\"\n",
        "    if not cycles or not version_hint:\n",
        "        return None\n",
        "    q = str(version_hint).strip().lower()\n",
        "    qn = _a621_norm(q)\n",
        "    # exact / contains / prefix\n",
        "    for c in cycles:\n",
        "        cyc = str(c.get(\"cycle\") or c.get(\"releaseCycle\") or \"\")\n",
        "        cl = cyc.lower()\n",
        "        cn = _a621_norm(cyc)\n",
        "        if not cn:\n",
        "            continue\n",
        "        if qn == cn or q in cl or cl in q:\n",
        "            return c\n",
        "        if cl.startswith(q):\n",
        "            return c\n",
        "    # numeric-major fallback (e.g., 120 -> 120)\n",
        "    try:\n",
        "        qmaj = q.split(\".\")[0]\n",
        "        if qmaj:\n",
        "            for c in cycles:\n",
        "                cyc = str(c.get(\"cycle\") or \"\")\n",
        "                if str(cyc).split(\".\")[0] == qmaj:\n",
        "                    return c\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def _a621_eol_status(entry: dict, today=None) -> Tuple[str, str, int]:\n",
        "    \"\"\"\n",
        "    Returns: (status, eol_date_str, days_left)\n",
        "      status ∈ {\"eol\", \"near\", \"ok\", \"unknown\"}\n",
        "    \"\"\"\n",
        "    if not entry:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "    eol = entry.get(\"eol\") or entry.get(\"support\") or entry.get(\"discontinued\")\n",
        "    if not eol or isinstance(eol, bool):\n",
        "        return (\"unknown\", \"\", 0)\n",
        "    try:\n",
        "        d = datetime.fromisoformat(str(eol)).replace(tzinfo=timezone.utc)\n",
        "        today = today or datetime.now(timezone.utc)\n",
        "        days = (d - today).days\n",
        "        if days < 0:\n",
        "            return (\"eol\", d.date().isoformat(), days)\n",
        "        if days <= A621_NEAR_EOL_DAYS:\n",
        "            return (\"near\", d.date().isoformat(), days)\n",
        "        return (\"ok\", d.date().isoformat(), days)\n",
        "    except Exception:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "def _a621_candidate_browser_pairs(text: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Extract (normalized_browser_name, version_hint) from free text.\n",
        "    Handles \"v120\", \"version 120.0.6099\", \"firefox esr 115\", etc.\n",
        "    \"\"\"\n",
        "    t = text.lower()\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "\n",
        "    # With \"version\"/\"v\"\n",
        "    rx_with_kw = re.compile(\n",
        "        r\"(?P<brw>{})[^\\w\\d]{{0,12}}(?:v(?:ersion)?)?[^\\w\\d]{{0,6}}(?P<ver>\\d+(?:\\.\\d+){{0,3}})\".format(\n",
        "            \"|\".join(p for p, _ in _A621_EBROWSER_PATTERNS)\n",
        "        ),\n",
        "        re.I,\n",
        "    )\n",
        "    # Bare number after name\n",
        "    rx_loose = re.compile(\n",
        "        r\"(?P<brw>{})[^\\w\\d]{{0,12}}(?P<ver>\\d+(?:\\.\\d+){{0,3}})\".format(\n",
        "            \"|\".join(p for p, _ in _A621_EBROWSER_PATTERNS)\n",
        "        ),\n",
        "        re.I,\n",
        "    )\n",
        "    # ESR without explicit number (we'll still record empty version so EOL = unknown)\n",
        "    if re.search(r\"\\bfirefox\\b.*\\besr\\b|\\besr\\b.*\\bfirefox\\b\", t):\n",
        "        pairs.append((\"firefox esr\", \"\"))\n",
        "\n",
        "    def norm_name(raw: str) -> str | None:\n",
        "        r = raw.lower().strip()\n",
        "        for pat, name in _A621_EBROWSER_PATTERNS:\n",
        "            if re.fullmatch(pat, r, flags=re.I):\n",
        "                return name\n",
        "            if re.search(rf\"^{pat}$\", r, flags=re.I):\n",
        "                return name\n",
        "        # last-resort contains\n",
        "        for pat, name in _A621_EBROWSER_PATTERNS:\n",
        "            if re.search(pat, r, flags=re.I):\n",
        "                return name\n",
        "        return None\n",
        "\n",
        "    for m in rx_with_kw.finditer(t):\n",
        "        brw = norm_name(m.group(\"brw\")) or m.group(\"brw\").lower()\n",
        "        ver = m.group(\"ver\")\n",
        "        if brw:\n",
        "            pairs.append((brw, ver))\n",
        "\n",
        "    for m in rx_loose.finditer(t):\n",
        "        brw = norm_name(m.group(\"brw\")) or m.group(\"brw\").lower()\n",
        "        ver = m.group(\"ver\")\n",
        "        if brw and (brw, ver) not in pairs:\n",
        "            pairs.append((brw, ver))\n",
        "\n",
        "    # If browser mentioned without version, add with empty version\n",
        "    for pat, name in _A621_EBROWSER_PATTERNS:\n",
        "        if re.search(rf\"\\b{pat}\\b\", t, flags=re.I) and not any(n == name for n, _ in pairs):\n",
        "            pairs.append((name, \"\"))\n",
        "\n",
        "    # dedupe (preserve order)\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for p in pairs:\n",
        "        if p not in seen:\n",
        "            seen.add(p)\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "def _a621_eol_check_browser(name: str, version: str) -> Tuple[str, str, int, str]:\n",
        "    \"\"\"\n",
        "    Resolve browser slug → fetch cycles → best-match cycle → compute status.\n",
        "    Returns: (status, eol_date, days_left, slug)\n",
        "    \"\"\"\n",
        "    slug = _a621_eol_slug_for_browser(name) or name\n",
        "    cycles = _a621_eol_fetch_cycles(slug)\n",
        "    if not cycles:\n",
        "        return (\"unknown\", \"\", 0, slug)\n",
        "    entry = _a621_eol_best_match(cycles, version)\n",
        "    status, eol_date, days = _a621_eol_status(entry)\n",
        "    return (status, eol_date, days, slug)\n",
        "\n",
        "# =========================\n",
        "# Main rule (original logic + EOL overlay)\n",
        "# =========================\n",
        "def rule_a6_2_1(text: str) -> dict:\n",
        "    \"\"\"\n",
        "    Extracts browsers and versions, being lenient: any number after a browser is treated as version.\n",
        "    - If 'attached' is present: compliant.\n",
        "    - Legacy browsers are non-compliant.\n",
        "    - Outdated versions as per guidance are non-compliant.\n",
        "    - EOL overlay: if any browser version is EOL per endoflife.date → auto_fail + non_compliant.\n",
        "    - Otherwise, needs_more_info if no browser+version.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    lowered = text.lower()\n",
        "    frame = {\n",
        "        \"browsers_listed\":      [],\n",
        "        \"versions_provided\":    False,\n",
        "        \"unsupported_browser\":  False,\n",
        "        \"outdated_version\":     False,\n",
        "        \"compliant\":            False,\n",
        "        \"needs_more_info\":      False,\n",
        "        \"non_compliant\":        False,\n",
        "        # --- EOL overlay fields ---\n",
        "        \"auto_fail\":            False,\n",
        "        \"eol_hits\":             [],\n",
        "        \"near_eol_hits\":        [],\n",
        "        \"reason\":               \"\"\n",
        "    }\n",
        "\n",
        "    if \"attached\" in lowered:\n",
        "        frame[\"compliant\"] = True\n",
        "        return frame\n",
        "\n",
        "    browser_names = [\n",
        "        \"chrome\", \"edge\", \"firefox\", \"safari\", \"opera\", \"chromium\", \"brave\", \"samsung internet\", \"vivaldi\"\n",
        "    ]\n",
        "    browser_rx = r\"|\".join([\n",
        "        r\"(?:samsung\\s+internet)\", r\"(?:firefox\\s+esr)\", \"chrome\", \"edge\", \"firefox\", \"safari\", \"opera\", \"chromium\", \"brave\", \"vivaldi\"\n",
        "    ])\n",
        "    version_pat = re.compile(\n",
        "        rf\"({browser_rx})[^\\w\\d]{{0,10}}v?(ersion)?[^\\w\\d]{{0,5}}([0-9]+(?:\\.[0-9]+){{0,3}})\",\n",
        "        re.I\n",
        "    )\n",
        "    loose_pat = re.compile(\n",
        "        rf\"({browser_rx})[^\\w\\d]{{0,10}}([0-9]+(?:\\.[0-9]+){{0,3}})\",\n",
        "        re.I\n",
        "    )\n",
        "\n",
        "    # Minimum supported majors (keep your thresholds)\n",
        "    min_supported_major = {\n",
        "        \"chrome\":   110,\n",
        "        \"edge\":     110,\n",
        "        \"firefox\":  110,\n",
        "        \"safari\":   15,\n",
        "        \"opera\":    95,\n",
        "        \"chromium\": 110,\n",
        "        \"brave\":    110,\n",
        "        \"samsung internet\": 22,\n",
        "        \"vivaldi\":  5,   # rough baseline; Vivaldi versions are smaller numerically\n",
        "    }\n",
        "    legacy_list = [\n",
        "        \"internet explorer\", \"ie6\", \"ie7\", \"ie8\", \"ie9\", \"ie10\", \"ie11\",\n",
        "        \"netscape\", \"uc browser\", \"konqueror\", \"maxthon\", \"waterfox classic\"\n",
        "    ]\n",
        "\n",
        "    found_versions = []\n",
        "    found_browsers = set()\n",
        "    outdated = False\n",
        "\n",
        "    for m in version_pat.finditer(lowered):\n",
        "        browser = m.group(1).strip().lower()\n",
        "        # normalize firefox esr → firefox for min-version check; keep name for list\n",
        "        min_key = \"firefox\" if \"firefox\" in browser else browser\n",
        "        version = m.group(3)\n",
        "        found_browsers.add(browser)\n",
        "        found_versions.append((browser, version))\n",
        "        try:\n",
        "            major = int(version.split(\".\")[0])\n",
        "            if major < min_supported_major.get(min_key, 0):\n",
        "                outdated = True\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    for m in loose_pat.finditer(lowered):\n",
        "        browser = m.group(1).strip().lower()\n",
        "        min_key = \"firefox\" if \"firefox\" in browser else browser\n",
        "        version = m.group(2)\n",
        "        if (browser, version) not in found_versions:\n",
        "            found_browsers.add(browser)\n",
        "            found_versions.append((browser, version))\n",
        "            try:\n",
        "                major = int(version.split(\".\")[0])\n",
        "                if major < min_supported_major.get(min_key, 0):\n",
        "                    outdated = True\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # Pick up browser mentions without versions (names only)\n",
        "    for browser in browser_names:\n",
        "        if re.search(rf\"\\b{browser}\\b\", lowered) and browser not in found_browsers:\n",
        "            found_browsers.add(browser)\n",
        "\n",
        "    # spaCy PRODUCT entities as a fallback\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PRODUCT\":\n",
        "            prod = ent.text.lower()\n",
        "            if prod in browser_names and prod not in found_browsers:\n",
        "                found_browsers.add(prod)\n",
        "\n",
        "    if any(legacy in lowered for legacy in legacy_list):\n",
        "        frame[\"unsupported_browser\"] = True\n",
        "\n",
        "    # Normalize “firefox esr” in the list if present\n",
        "    normalized_list = set()\n",
        "    for b in found_browsers:\n",
        "        if re.search(r\"firefox\\s+esr\", b):\n",
        "            normalized_list.add(\"firefox esr\")\n",
        "        else:\n",
        "            normalized_list.add(b)\n",
        "    frame[\"browsers_listed\"] = sorted(normalized_list)\n",
        "\n",
        "    frame[\"versions_provided\"] = bool(found_versions)\n",
        "    frame[\"outdated_version\"] = outdated\n",
        "\n",
        "    # If any version pattern appears anywhere and any browser is present, treat as version provided\n",
        "    if not frame[\"versions_provided\"] and frame[\"browsers_listed\"]:\n",
        "        if re.search(r\"\\b\\d+(\\.\\d+){1,3}\\b\", lowered):\n",
        "            frame[\"versions_provided\"] = True\n",
        "\n",
        "    # Core label logic stays the same\n",
        "    if frame[\"unsupported_browser\"] or frame[\"outdated_version\"]:\n",
        "        frame[\"non_compliant\"] = True\n",
        "    elif frame[\"browsers_listed\"] and frame[\"versions_provided\"]:\n",
        "        frame[\"compliant\"] = True\n",
        "    else:\n",
        "        frame[\"needs_more_info\"] = True\n",
        "\n",
        "    # --- Reason assignment (pre-EOL overlay) ---\n",
        "    if frame[\"compliant\"]:\n",
        "        frame[\"reason\"] = f\"Compliant: Supported browser(s) listed ({', '.join(frame['browsers_listed'])}) with valid version(s).\"\n",
        "    elif frame[\"non_compliant\"]:\n",
        "        if frame[\"unsupported_browser\"]:\n",
        "            frame[\"reason\"] = \"Non-compliant: Legacy or unsupported browser mentioned (e.g. Internet Explorer).\"\n",
        "        elif frame[\"outdated_version\"]:\n",
        "            frame[\"reason\"] = \"Non-compliant: Browser version is below the minimum supported level.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Non-compliant: Unsupported configuration detected.\"\n",
        "    elif frame[\"needs_more_info\"]:\n",
        "        if not frame[\"browsers_listed\"]:\n",
        "            frame[\"reason\"] = \"Needs more information: No identifiable browser found in the response.\"\n",
        "        elif not frame[\"versions_provided\"]:\n",
        "            frame[\"reason\"] = \"Needs more information: Browser(s) listed without any version information.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Needs more information: Partial browser/version information; unclear compliance.\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: Incomplete browser information.\"\n",
        "\n",
        "    # =========================\n",
        "    # EOL overlay (runs last)\n",
        "    #   • If any EOL → set auto_fail + non_compliant and override reason\n",
        "    #   • If any near-EOL → append note to reason\n",
        "    # =========================\n",
        "    try:\n",
        "        candidates = _a621_candidate_browser_pairs(text)\n",
        "        eol_hits, near_hits = [], []\n",
        "\n",
        "        for brw, ver in candidates:\n",
        "            slug = _a621_eol_slug_for_browser(brw)\n",
        "            if not slug:\n",
        "                continue\n",
        "            status, eol_date, days_left, _ = _a621_eol_check_browser(brw, ver)\n",
        "            label = f\"{slug} {ver}\".strip()\n",
        "            if status == \"eol\":\n",
        "                eol_hits.append(f\"{label} – EOL {eol_date}\")\n",
        "            elif status == \"near\":\n",
        "                near_hits.append(f\"{label} – EOL {eol_date} (in {days_left} days)\")\n",
        "\n",
        "        if eol_hits:\n",
        "            frame[\"auto_fail\"] = True\n",
        "            frame[\"non_compliant\"] = True\n",
        "            frame[\"compliant\"] = False\n",
        "            frame[\"needs_more_info\"] = False\n",
        "            frame[\"eol_hits\"] = eol_hits\n",
        "            frame[\"reason\"] = \"Auto-fail: End-of-life browser version detected — \" + \"; \".join(eol_hits) + \".\"\n",
        "        elif near_hits:\n",
        "            frame[\"near_eol_hits\"] = near_hits\n",
        "            frame[\"reason\"] = frame[\"reason\"].rstrip(\".\") + \". Note: Near end-of-life — \" + \"; \".join(near_hits) + \".\"\n",
        "\n",
        "    except Exception:\n",
        "        # If EOL lookups fail, keep original label/reason.\n",
        "        pass\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# A6.2.2 — Malware-protection inventory checker (UPDATED with EOL overlay)\n",
        "# Namespaced helpers/constants only; functionality unchanged\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n",
        "\n",
        "# =========================\n",
        "# EOL helpers (endoflife.date) — A6.2.2 namespace\n",
        "# =========================\n",
        "import requests\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "A622_NEAR_EOL_DAYS = 180  # tweak the “near EOL” note window\n",
        "\n",
        "# Map product name → endoflife.date slug (platforms + some AV engines)\n",
        "# Unknown slugs simply resolve to \"unknown\" and won’t affect labels.\n",
        "_A622_EOLDATE_SLUGS = {\n",
        "    # Operating systems (often mentioned alongside tools)\n",
        "    \"windows\": \"windows\",\n",
        "    \"windows 10\": \"windows\",\n",
        "    \"windows 11\": \"windows\",\n",
        "    \"windows server\": \"windows-server\",\n",
        "    \"macos\": \"macos\",\n",
        "    \"os x\": \"macos\",\n",
        "    \"ubuntu\": \"ubuntu\",\n",
        "    \"debian\": \"debian\",\n",
        "    \"rhel\": \"rhel\",\n",
        "    \"red hat\": \"rhel\",\n",
        "    \"red hat enterprise linux\": \"rhel\",\n",
        "    \"centos\": \"centos\",\n",
        "    \"rocky linux\": \"rocky-linux\",\n",
        "    \"almalinux\": \"almalinux-os\",\n",
        "    \"oracle linux\": \"oracle-linux\",\n",
        "    \"opensuse\": \"opensuse\",\n",
        "    \"arch linux\": \"archlinux\",\n",
        "    \"kali\": \"kali\",\n",
        "    \"parrot\": \"parrot\",\n",
        "    # Browsers sometimes appear here too (keep supported)\n",
        "    \"chrome\": \"chrome\",\n",
        "    \"edge\": \"edge\",\n",
        "    \"firefox\": \"firefox\",\n",
        "    \"firefox esr\": \"firefox-esr\",\n",
        "    \"safari\": \"safari\",\n",
        "    # AV / endpoint security engines (subset known on endoflife.date)\n",
        "    \"clamav\": \"clamav\",\n",
        "    \"microsoft defender\": \"microsoft-defender-antivirus\",\n",
        "    \"windows defender\": \"microsoft-defender-antivirus\",\n",
        "    \"defender for endpoint\": \"microsoft-defender-for-endpoint\",\n",
        "}\n",
        "\n",
        "def _a622_eol_resolve_slug(name: str) -> str | None:\n",
        "    if not name:\n",
        "        return None\n",
        "    key = name.lower().strip()\n",
        "    if key in _A622_EOLDATE_SLUGS:\n",
        "        return _A622_EOLDATE_SLUGS[key]\n",
        "    # fuzzy fallback\n",
        "    for k, v in _A622_EOLDATE_SLUGS.items():\n",
        "        if k in key:\n",
        "            return v\n",
        "    return None\n",
        "\n",
        "def _a622_eol_fetch_cycles(slug: str):\n",
        "    \"\"\"Fetch & cache product cycles from endoflife.date.\"\"\"\n",
        "    if not hasattr(_a622_eol_fetch_cycles, \"_cache\"):\n",
        "        _a622_eol_fetch_cycles._cache = {}\n",
        "    cache = _a622_eol_fetch_cycles._cache\n",
        "    if slug in cache:\n",
        "        return cache[slug]\n",
        "    url = f\"https://endoflife.date/api/v1/products/{slug}.json\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=8)\n",
        "        if r.ok:\n",
        "            cache[slug] = r.json()\n",
        "            return cache[slug]\n",
        "    except Exception:\n",
        "        pass\n",
        "    cache[slug] = None\n",
        "    return None\n",
        "\n",
        "def _a622_norm(s: str) -> str:\n",
        "    return (s or \"\").lower().replace(\" \", \"\").strip()\n",
        "\n",
        "def _a622_eol_best_match(cycles: list, version_hint: str):\n",
        "    \"\"\"Loose match between version_hint and a cycle entry.\"\"\"\n",
        "    if not cycles or not version_hint:\n",
        "        return None\n",
        "    q = str(version_hint).strip().lower()\n",
        "    qn = _a622_norm(q)\n",
        "    for c in cycles:\n",
        "        cyc = str(c.get(\"cycle\") or c.get(\"releaseCycle\") or \"\")\n",
        "        cl = cyc.lower()\n",
        "        cn = _a622_norm(cyc)\n",
        "        if not cn:\n",
        "            continue\n",
        "        if qn == cn or q in cl or cl in q:\n",
        "            return c\n",
        "        if cl.startswith(q):\n",
        "            return c\n",
        "    # numeric-major fallback (e.g., 115 -> 115.x)\n",
        "    try:\n",
        "        qmaj = q.split(\".\")[0]\n",
        "        if qmaj:\n",
        "            for c in cycles:\n",
        "                cyc = str(c.get(\"cycle\") or \"\")\n",
        "                if str(cyc).split(\".\")[0] == qmaj:\n",
        "                    return c\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def _a622_eol_status(entry: dict, today=None) -> Tuple[str, str, int]:\n",
        "    \"\"\"\n",
        "    Returns: (status, eol_date_str, days_left)\n",
        "      status ∈ {\"eol\", \"near\", \"ok\", \"unknown\"}\n",
        "    \"\"\"\n",
        "    if not entry:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "    eol = entry.get(\"eol\") or entry.get(\"support\") or entry.get(\"discontinued\")\n",
        "    if not eol or isinstance(eol, bool):\n",
        "        return (\"unknown\", \"\", 0)\n",
        "    try:\n",
        "        d = datetime.fromisoformat(str(eol)).replace(tzinfo=timezone.utc)\n",
        "        today = today or datetime.now(timezone.utc)\n",
        "        days = (d - today).days\n",
        "        if days < 0:\n",
        "            return (\"eol\", d.date().isoformat(), days)\n",
        "        if days <= A622_NEAR_EOL_DAYS:\n",
        "            return (\"near\", d.date().isoformat(), days)\n",
        "        return (\"ok\", d.date().isoformat(), days)\n",
        "    except Exception:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "# Extract EOL candidates from malware tool text:\n",
        "#  • AV tools (subset) with adjacent versions\n",
        "#  • OS/platform mentions (Windows/macOS/Linux) with versions / years\n",
        "def _a622_eol_candidates_from_text(text: str) -> List[Tuple[str, str]]:\n",
        "    t = text.lower()\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "\n",
        "    # OS/platforms\n",
        "    for m in re.finditer(r\"\\bwindows\\s+(10|11)\\s*(\\d{2}h\\d)?\\b\", t):\n",
        "        ver = (m.group(1) + (\" \" + (m.group(2) or \"\").upper()).strip()).strip()\n",
        "        pairs.append((\"windows\", ver))\n",
        "    if re.search(r\"\\bwindows\\s+10\\b\", t) and not any(p[0] == \"windows\" and p[1].startswith(\"10\") for p in pairs):\n",
        "        pairs.append((\"windows\", \"10\"))\n",
        "    if re.search(r\"\\bwindows\\s+11\\b\", t) and not any(p[0] == \"windows\" and p[1].startswith(\"11\") for p in pairs):\n",
        "        pairs.append((\"windows\", \"11\"))\n",
        "\n",
        "    for m in re.finditer(r\"\\bwindows\\s+server\\s+((?:20)?\\d{2})(?:\\s*r2)?\\b\", t):\n",
        "        pairs.append((\"windows server\", m.group(1)))\n",
        "\n",
        "    for m in re.finditer(r\"\\bmac\\s?os\\s*(\\d{2}(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"macos\", m.group(1)))\n",
        "\n",
        "    for name, pat in [\n",
        "        (\"ubuntu\", r\"\\bubuntu\\s+(\\d{2}\\.\\d{2})\\b\"),\n",
        "        (\"debian\", r\"\\bdebian\\s+(\\d{1,2})\\b\"),\n",
        "        (\"rhel\", r\"\\b(?:rhel|red\\s*hat(?:\\s+enterprise)?\\s+linux)\\s+(\\d{1,2})\\b\"),\n",
        "        (\"centos\", r\"\\bcentos\\s+(\\d{1,2})\\b\"),\n",
        "        (\"rocky linux\", r\"\\brocky\\s+linux\\s+(\\d{1,2})\\b\"),\n",
        "        (\"almalinux\", r\"\\balma\\s*linux\\s+(\\d{1,2})\\b\"),\n",
        "        (\"oracle linux\", r\"\\boracle\\s+linux\\s+(\\d{1,2})\\b\"),\n",
        "        (\"opensuse\", r\"\\bopen\\s*suse\\b|\\bopensuse\\b\"),\n",
        "    ]:\n",
        "        m = re.search(pat, t)\n",
        "        if m:\n",
        "            ver = \"\"\n",
        "            if m.groups():\n",
        "                g = next((g for g in m.groups() if g), \"\")\n",
        "                ver = g.strip()\n",
        "            pairs.append((name, ver))\n",
        "\n",
        "    # AV engines we can try to EOL-check\n",
        "    av_patterns = [\n",
        "        (r\"\\bclamav\\b\", \"clamav\"),\n",
        "        (r\"\\bwindows\\s+defender\\b|\\bmicrosoft\\s+defender\\b\", \"microsoft defender\"),\n",
        "        (r\"\\bdefender\\s+for\\s+endpoint\\b\", \"defender for endpoint\"),\n",
        "    ]\n",
        "    # version tokens following the product (v / version / plain number)\n",
        "    rx_version_after = r\"[^\\w\\d]{0,10}(?:v(?:ersion)?)?[^\\w\\d]{0,5}(\\d+(?:\\.\\d+){0,3})\"\n",
        "    for pat, canon in av_patterns:\n",
        "        for m in re.finditer(pat + rx_version_after, t, flags=re.I):\n",
        "            pairs.append((canon, m.group(1)))\n",
        "        # also add without version so we at least try (will be \"unknown\" if no cycle)\n",
        "        if re.search(pat, t, flags=re.I) and not any(p[0] == canon for p in pairs):\n",
        "            pairs.append((canon, \"\"))\n",
        "\n",
        "    # Deduplicate while preserving order\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for p in pairs:\n",
        "        if p not in seen:\n",
        "            seen.add(p)\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "def _a622_eol_check(product_hint: str, version_hint: str) -> Tuple[str, str, int, str]:\n",
        "    \"\"\"\n",
        "    Resolve product slug → fetch cycles → best-match cycle → compute status.\n",
        "    Returns: (status, eol_date, days_left, slug)\n",
        "    \"\"\"\n",
        "    slug = _a622_eol_resolve_slug(product_hint) or product_hint\n",
        "    cycles = _a622_eol_fetch_cycles(slug)\n",
        "    if not cycles:\n",
        "        return (\"unknown\", \"\", 0, slug)\n",
        "    entry = _a622_eol_best_match(cycles, version_hint)\n",
        "    status, eol_date, days = _a622_eol_status(entry)\n",
        "    return (status, eol_date, days, slug)\n",
        "\n",
        "# =========================\n",
        "# Main rule (original logic + EOL overlay)\n",
        "# =========================\n",
        "def rule_a6_2_2(text: str) -> dict:\n",
        "    \"\"\"\n",
        "    A6.2.2 – List your malware-protection software (version required)\n",
        "    Robust: Accepts any number as a version if a tool is listed.\n",
        "    - If 'attached' present: compliant.\n",
        "    - Unsupported/legacy tools: non-compliant.\n",
        "    - Outdated versions: non-compliant.\n",
        "    - Otherwise, needs_more_info if no tool+version.\n",
        "    - EOL overlay runs last: platform and supported AV slugs (e.g., ClamAV, Microsoft Defender AV).\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    lowered = text.lower()\n",
        "\n",
        "    frame = {\n",
        "        \"malware_tools\":     [],\n",
        "        \"unsupported_tools\": [],\n",
        "        \"outdated_version\":  False,\n",
        "        \"versions_provided\": False,\n",
        "        \"unsupported_flag\":  False,\n",
        "        \"compliant\":         False,\n",
        "        \"needs_more_info\":   False,\n",
        "        \"non_compliant\":     False,\n",
        "        # --- EOL overlay fields ---\n",
        "        \"auto_fail\":         False,\n",
        "        \"eol_hits\":          [],\n",
        "        \"near_eol_hits\":     [],\n",
        "        \"reason\":            \"\"\n",
        "    }\n",
        "\n",
        "    if \"attached\" in lowered:\n",
        "        frame[\"compliant\"] = True\n",
        "        return frame\n",
        "\n",
        "    av_names = [\n",
        "        \"windows defender\", \"microsoft defender\", \"defender for endpoint\",\n",
        "        \"sentinelone\", \"sentinel one\", \"crowdstrike\", \"sophos\", \"bitdefender\",\n",
        "        \"eset\", \"kaspersky\", \"trellix\", \"mcafee\", \"webroot\", \"cortex xdr\",\n",
        "        \"huntress\", \"rapid7\", \"clamav\", \"trend micro\", \"xprotect\", \"msrt\", \"malwarebytes\"\n",
        "    ]\n",
        "    # version pattern: optional \"v\"/\"version\", then dotted numbers\n",
        "    av_rx = {\n",
        "        name: re.compile(rf\"{re.escape(name)}[^\\w\\d]{{0,10}}v?(?:ersion)?[^\\w\\d]{{0,5}}([\\d\\.]+)?\", re.I)\n",
        "        for name in av_names\n",
        "    }\n",
        "    # very rough major baselines (only applied when we do parse a dotted version)\n",
        "    min_major = {\n",
        "        \"windows defender\": 0, \"microsoft defender\": 0, \"defender for endpoint\": 0,\n",
        "        \"sentinelone\": 22, \"sentinel one\": 22,\n",
        "        \"crowdstrike\": 6, \"sophos\": 10, \"bitdefender\": 7, \"eset\": 9,\n",
        "        \"kaspersky\": 21, \"trend micro\": 12, \"clamav\": 1,\n",
        "        \"mcafee\": 10, \"trellix\": 1, \"webroot\": 9, \"cortex xdr\": 7,\n",
        "        \"malwarebytes\": 4,\n",
        "    }\n",
        "    legacy_rx = {\n",
        "        \"avg\": r\"\\bavg\\b\",\n",
        "        \"avast\": r\"\\bavast\\b\",\n",
        "        \"windows live onecare\": r\"windows live onecare\",\n",
        "        \"microsoft security essentials\": r\"\\b(?:ms|microsoft)\\s+security\\s+essentials\\b\",\n",
        "        \"norton (old)\": r\"\\bnorton(?: antivirus| 360)?\\b(?:\\s*v?\\d{1,2})?\",\n",
        "        \"panda (legacy)\": r\"\\bpanda\\s+(?:free|security)\\b\",\n",
        "        \"malwarebytes (free)\": r\"\\bmalwarebytes\\b(?!.*premium)\",\n",
        "    }\n",
        "\n",
        "    found_tools = set()\n",
        "    found_versions = set()\n",
        "    outdated = False\n",
        "\n",
        "    # 1) Regex match for known tools + attached version\n",
        "    for name, rx in av_rx.items():\n",
        "        for m in rx.finditer(lowered):\n",
        "            found_tools.add(name)\n",
        "            ver = m.group(1)\n",
        "            if ver and re.search(r\"\\d\", ver):\n",
        "                found_versions.add(ver)\n",
        "                try:\n",
        "                    major = int(ver.split(\".\")[0])\n",
        "                    key = name\n",
        "                    if major < min_major.get(key, 0):\n",
        "                        outdated = True\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    # 2) Legacy/unsupported check\n",
        "    for name, rx in legacy_rx.items():\n",
        "        if re.search(rx, lowered):\n",
        "            frame[\"unsupported_tools\"].append(name)\n",
        "            frame[\"unsupported_flag\"] = True\n",
        "\n",
        "    # 3) spaCy fallback for PRODUCT/ORG names → map back to our known list if substring match\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in {\"PRODUCT\", \"ORG\"}:\n",
        "            token = ent.text.lower()\n",
        "            for name in av_names:\n",
        "                if name in token and name not in found_tools:\n",
        "                    found_tools.add(name)\n",
        "\n",
        "    # 4) Any dotted versions anywhere count if at least one tool is present\n",
        "    has_any_dotted = bool(re.search(r\"\\b\\d+\\.\\d+(?:\\.\\d+){0,2}\\b\", lowered))\n",
        "    if found_tools and (found_versions or has_any_dotted):\n",
        "        frame[\"versions_provided\"] = True\n",
        "\n",
        "    frame[\"malware_tools\"] = sorted(found_tools)\n",
        "    frame[\"unsupported_tools\"] = sorted(set(frame[\"unsupported_tools\"]))\n",
        "    frame[\"outdated_version\"] = outdated\n",
        "\n",
        "    # 5) Core label logic (unchanged)\n",
        "    if frame[\"unsupported_flag\"] or frame[\"outdated_version\"]:\n",
        "        frame[\"non_compliant\"] = True\n",
        "    elif frame[\"malware_tools\"] and frame[\"versions_provided\"]:\n",
        "        frame[\"compliant\"] = True\n",
        "    else:\n",
        "        frame[\"needs_more_info\"] = True\n",
        "\n",
        "    # --- Reason assignment (pre-EOL overlay) ---\n",
        "    if frame[\"compliant\"]:\n",
        "        frame[\"reason\"] = f\"Compliant: Malware protection tool(s) listed ({', '.join(frame['malware_tools'])}) with version(s).\"\n",
        "    elif frame[\"non_compliant\"]:\n",
        "        if frame[\"unsupported_flag\"]:\n",
        "            frame[\"reason\"] = f\"Non-compliant: Unsupported or legacy tool(s) detected ({', '.join(frame['unsupported_tools'])}).\"\n",
        "        elif frame[\"outdated_version\"]:\n",
        "            frame[\"reason\"] = \"Non-compliant: Malware protection version is below the minimum supported level.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Non-compliant: Tool or version provided is not acceptable.\"\n",
        "    elif frame[\"needs_more_info\"] and not frame[\"malware_tools\"]:\n",
        "        frame[\"reason\"] = \"Needs more information: No malware protection tool detected.\"\n",
        "    elif frame[\"needs_more_info\"] and not frame[\"versions_provided\"]:\n",
        "        frame[\"reason\"] = \"Needs more information: Tool listed without a version number.\"\n",
        "    elif frame[\"needs_more_info\"]:\n",
        "        frame[\"reason\"] = \"Needs more information: Incomplete version or tool information.\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: Unclear response.\"\n",
        "\n",
        "    # =========================\n",
        "    # EOL overlay (runs last)\n",
        "    #   • If any EOL → set auto_fail + non_compliant and override reason\n",
        "    #   • If any near-EOL → append note to reason\n",
        "    # =========================\n",
        "    try:\n",
        "        candidates = _a622_eol_candidates_from_text(text)\n",
        "        eol_hits, near_hits = [], []\n",
        "\n",
        "        for prod_key, ver in candidates:\n",
        "            slug = _a622_eol_resolve_slug(prod_key) or prod_key\n",
        "            if not slug:\n",
        "                continue\n",
        "            status, eol_date, days_left, _ = _a622_eol_check(prod_key, ver)\n",
        "            label = f\"{slug} {ver}\".strip()\n",
        "            if status == \"eol\":\n",
        "                eol_hits.append(f\"{label} – EOL {eol_date}\")\n",
        "            elif status == \"near\":\n",
        "                near_hits.append(f\"{label} – EOL {eol_date} (in {days_left} days)\")\n",
        "\n",
        "        if eol_hits:\n",
        "            frame[\"auto_fail\"] = True\n",
        "            frame[\"non_compliant\"] = True\n",
        "            frame[\"compliant\"] = False\n",
        "            frame[\"needs_more_info\"] = False\n",
        "            frame[\"eol_hits\"] = eol_hits\n",
        "            frame[\"reason\"] = \"Auto-fail: End-of-life software/platform detected — \" + \"; \".join(eol_hits) + \".\"\n",
        "        elif near_hits:\n",
        "            frame[\"near_eol_hits\"] = near_hits\n",
        "            frame[\"reason\"] = frame[\"reason\"].rstrip(\".\") + \". Note: Near end-of-life — \" + \"; \".join(near_hits) + \".\"\n",
        "\n",
        "    except Exception:\n",
        "        # If EOL lookups fail, keep the original label/reason intact.\n",
        "        pass\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------ #\n",
        "#  A6.2.3 – “List your e-mail applications” (UPDATED with EOL overlay)\n",
        "#  Helpers renamed/namespaced only; functionality unchanged\n",
        "# ------------------------------------------------------------------ #\n",
        "\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n",
        "\n",
        "# =========================\n",
        "# EOL helpers (endoflife.date) — A6.2.3 namespace\n",
        "# =========================\n",
        "import requests\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "A623_NEAR_EOL_DAYS = 180  # tweak the “near EOL” note window\n",
        "\n",
        "# Map product name → endoflife.date slug\n",
        "# (We’ll also map Outlook year versions to Office year slugs.)\n",
        "_A623_EOLDATE_SLUGS = {\n",
        "    # Suites / apps\n",
        "    \"microsoft 365 apps\": \"microsoft-365-apps\",\n",
        "    \"office 365\": \"microsoft-365-apps\",\n",
        "    \"o365\": \"microsoft-365-apps\",\n",
        "    \"m365\": \"microsoft-365-apps\",\n",
        "    \"outlook 365\": \"microsoft-365-apps\",\n",
        "    \"outlook for microsoft 365\": \"microsoft-365-apps\",\n",
        "    \"thunderbird\": \"thunderbird\",\n",
        "    \"exchange server\": \"exchange-server\",\n",
        "    \"microsoft exchange server\": \"exchange-server\",\n",
        "\n",
        "    # OS/platforms that may be mentioned in this answer\n",
        "    \"windows\": \"windows\",\n",
        "    \"windows 10\": \"windows\",\n",
        "    \"windows 11\": \"windows\",\n",
        "    \"windows server\": \"windows-server\",\n",
        "    \"macos\": \"macos\",\n",
        "    \"os x\": \"macos\",\n",
        "    \"ubuntu\": \"ubuntu\",\n",
        "    \"debian\": \"debian\",\n",
        "    \"rhel\": \"rhel\",\n",
        "    \"red hat\": \"rhel\",\n",
        "    \"red hat enterprise linux\": \"rhel\",\n",
        "    \"centos\": \"centos\",\n",
        "    \"rocky linux\": \"rocky-linux\",\n",
        "    \"almalinux\": \"almalinux-os\",\n",
        "    \"oracle linux\": \"oracle-linux\",\n",
        "    \"opensuse\": \"opensuse\",\n",
        "    \"arch linux\": \"archlinux\",\n",
        "    \"kali\": \"kali\",\n",
        "    \"parrot\": \"parrot\",\n",
        "}\n",
        "\n",
        "# Office year slugs on endoflife.date (used for “Outlook/Office 20xx”)\n",
        "_A623_OFFICE_YEAR_TO_SLUG = {\n",
        "    \"2010\": \"office-2010\",\n",
        "    \"2013\": \"office-2013\",\n",
        "    \"2016\": \"office-2016\",\n",
        "    \"2019\": \"office-2019\",\n",
        "    \"2021\": \"office-2021\",\n",
        "}\n",
        "\n",
        "def _a623_eol_resolve_slug(name: str, version_hint: str = \"\") -> str | None:\n",
        "    \"\"\"\n",
        "    Resolve a user-provided product name (and optional version) to an endoflife.date slug.\n",
        "    Special-cases:\n",
        "      - “Outlook 20xx” → corresponding Office year slug.\n",
        "      - “Office 20xx”  → corresponding Office year slug.\n",
        "      - “Outlook 365 / Office 365 / Microsoft 365 Apps” → microsoft-365-apps\n",
        "    \"\"\"\n",
        "    if not name:\n",
        "        return None\n",
        "    key = name.lower().strip()\n",
        "\n",
        "    # Outlook/Office year mapping\n",
        "    y = None\n",
        "    # Try to spot a year inside either the name or the version hint\n",
        "    m_name = re.search(r\"\\b(2010|2013|2016|2019|2021)\\b\", key)\n",
        "    m_ver  = re.search(r\"\\b(2010|2013|2016|2019|2021)\\b\", (version_hint or \"\"))\n",
        "    if m_name:\n",
        "        y = m_name.group(1)\n",
        "    elif m_ver:\n",
        "        y = m_ver.group(1)\n",
        "    if (\"outlook\" in key or \"office\" in key) and y and y in _A623_OFFICE_YEAR_TO_SLUG:\n",
        "        return _A623_OFFICE_YEAR_TO_SLUG[y]\n",
        "\n",
        "    # Microsoft 365 Apps variants\n",
        "    if (\"365\" in key and (\"outlook\" in key or \"office\" in key)) or \"microsoft 365 apps\" in key:\n",
        "        return \"microsoft-365-apps\"\n",
        "\n",
        "    # Generic dictionary lookup + fuzzy contains\n",
        "    if key in _A623_EOLDATE_SLUGS:\n",
        "        return _A623_EOLDATE_SLUGS[key]\n",
        "    for k, v in _A623_EOLDATE_SLUGS.items():\n",
        "        if k in key:\n",
        "            return v\n",
        "    return None\n",
        "\n",
        "def _a623_eol_fetch_cycles(slug: str):\n",
        "    \"\"\"Fetch & cache product cycles from endoflife.date.\"\"\"\n",
        "    if not hasattr(_a623_eol_fetch_cycles, \"_cache\"):\n",
        "        _a623_eol_fetch_cycles._cache = {}\n",
        "    cache = _a623_eol_fetch_cycles._cache\n",
        "    if slug in cache:\n",
        "        return cache[slug]\n",
        "    url = f\"https://endoflife.date/api/v1/products/{slug}.json\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=8)\n",
        "        if r.ok:\n",
        "            cache[slug] = r.json()\n",
        "            return cache[slug]\n",
        "    except Exception:\n",
        "        pass\n",
        "    cache[slug] = None\n",
        "    return None\n",
        "\n",
        "def _a623_norm(s: str) -> str:\n",
        "    return (s or \"\").lower().replace(\" \", \"\").strip()\n",
        "\n",
        "def _a623_eol_best_match(cycles: list, version_hint: str):\n",
        "    \"\"\"Loose match between version_hint and a cycle entry.\"\"\"\n",
        "    if not cycles or not version_hint:\n",
        "        return None\n",
        "    q = str(version_hint).strip().lower()\n",
        "    qn = _a623_norm(q)\n",
        "    for c in cycles:\n",
        "        cyc = str(c.get(\"cycle\") or c.get(\"releaseCycle\") or \"\")\n",
        "        cl = cyc.lower()\n",
        "        cn = _a623_norm(cyc)\n",
        "        if not cn:\n",
        "            continue\n",
        "        if qn == cn or q in cl or cl in q:\n",
        "            return c\n",
        "        if cl.startswith(q):\n",
        "            return c\n",
        "    # numeric-major fallback (e.g., 115 -> 115.x)\n",
        "    try:\n",
        "        qmaj = q.split(\".\")[0]\n",
        "        if qmaj:\n",
        "            for c in cycles:\n",
        "                cyc = str(c.get(\"cycle\") or \"\")\n",
        "                if str(cyc).split(\".\")[0] == qmaj:\n",
        "                    return c\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def _a623_eol_status(entry: dict, today=None) -> Tuple[str, str, int]:\n",
        "    \"\"\"\n",
        "    Returns: (status, eol_date_str, days_left)\n",
        "      status ∈ {\"eol\", \"near\", \"ok\", \"unknown\"}\n",
        "    \"\"\"\n",
        "    if not entry:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "    eol = entry.get(\"eol\") or entry.get(\"support\") or entry.get(\"discontinued\")\n",
        "    if not eol or isinstance(eol, bool):\n",
        "        return (\"unknown\", \"\", 0)\n",
        "    try:\n",
        "        d = datetime.fromisoformat(str(eol)).replace(tzinfo=timezone.utc)\n",
        "        today = today or datetime.now(timezone.utc)\n",
        "        days = (d - today).days\n",
        "        if days < 0:\n",
        "            return (\"eol\", d.date().isoformat(), days)\n",
        "        if days <= A623_NEAR_EOL_DAYS:\n",
        "            return (\"near\", d.date().isoformat(), days)\n",
        "        return (\"ok\", d.date().isoformat(), days)\n",
        "    except Exception:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "def _a623_eol_check(product_name: str, version_text: str) -> Tuple[str, str, int, str]:\n",
        "    \"\"\"\n",
        "    Resolve product → fetch cycles → match cycle → compute status.\n",
        "    Returns: (status, eol_date, days_left, slug)\n",
        "    \"\"\"\n",
        "    slug = _a623_eol_resolve_slug(product_name, version_text) or product_name\n",
        "    cycles = _a623_eol_fetch_cycles(slug)\n",
        "    if not cycles:\n",
        "        return (\"unknown\", \"\", 0, slug)\n",
        "    entry = _a623_eol_best_match(cycles, version_text)\n",
        "    status, eol_date, days = _a623_eol_status(entry)\n",
        "    return (status, eol_date, days, slug)\n",
        "\n",
        "def _a623_eol_candidates_from_email_text(text: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Build a list of (product_hint, version_hint) from free text for email apps:\n",
        "      - Outlook/Office year versions (2010/2013/2016/2019/2021), Outlook 365 / M365 / O365\n",
        "      - Exchange Server (2013/2016/2019)\n",
        "      - Thunderbird + version\n",
        "      - OS mentions (Windows/macOS/Linux) with versions (if present)\n",
        "    \"\"\"\n",
        "    t = text.lower()\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "\n",
        "    # Outlook / Office\n",
        "    for m in re.finditer(r\"\\b(?:microsoft\\s+)?outlook\\s+(2010|2013|2016|2019|2021|365)\\b\", t):\n",
        "        pairs.append((\"outlook \" + m.group(1), m.group(1)))\n",
        "    for m in re.finditer(r\"\\b(?:microsoft\\s+)?office\\s+(2010|2013|2016|2019|2021|365)\\b\", t):\n",
        "        pairs.append((\"office \" + m.group(1), m.group(1)))\n",
        "    if re.search(r\"\\bmicrosoft\\s+365\\s+apps\\b\", t):\n",
        "        pairs.append((\"microsoft 365 apps\", \"\"))  # channels are handled on the product page\n",
        "    if re.search(r\"\\b(o365|m365)\\b\", t):\n",
        "        pairs.append((\"microsoft 365 apps\", \"\"))\n",
        "\n",
        "    # Exchange Server (on-prem)\n",
        "    for m in re.finditer(r\"\\b(?:microsoft\\s+)?exchange(?:\\s+server)?\\s+(2013|2016|2019)\\b\", t):\n",
        "        pairs.append((\"exchange server\", m.group(1)))\n",
        "\n",
        "    # Thunderbird\n",
        "    for m in re.finditer(r\"\\bthunderbird\\b[^\\w\\d]{0,10}(?:v(?:ersion)?)?[^\\w\\d]{0,5}(\\d+(?:\\.\\d+){0,3})\", t):\n",
        "        pairs.append((\"thunderbird\", m.group(1)))\n",
        "    if re.search(r\"\\bthunderbird\\b\", t) and not any(p[0] == \"thunderbird\" for p in pairs):\n",
        "        pairs.append((\"thunderbird\", \"\"))\n",
        "\n",
        "    # OS (if they mention platforms with versions)\n",
        "    for m in re.finditer(r\"\\bwindows\\s+(10|11)\\s*(\\d{2}h\\d)?\\b\", t):\n",
        "        ver = (m.group(1) + (\" \" + (m.group(2) or \"\").upper()).strip()).strip()\n",
        "        pairs.append((\"windows\", ver))\n",
        "    if re.search(r\"\\bwindows\\s+10\\b\", t) and not any(p[0] == \"windows\" and p[1].startswith(\"10\") for p in pairs):\n",
        "        pairs.append((\"windows\", \"10\"))\n",
        "    if re.search(r\"\\bwindows\\s+11\\b\", t) and not any(p[0] == \"windows\" and p[1].startswith(\"11\") for p in pairs):\n",
        "        pairs.append((\"windows\", \"11\"))\n",
        "    for m in re.finditer(r\"\\bwindows\\s+server\\s+((?:20)?\\d{2})(?:\\s*r2)?\\b\", t):\n",
        "        pairs.append((\"windows server\", m.group(1)))\n",
        "    for m in re.finditer(r\"\\bmac\\s?os\\s*(\\d{2}(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"macos\", m.group(1)))\n",
        "    for name, pat in [\n",
        "        (\"ubuntu\", r\"\\bubuntu\\s+(\\d{2}\\.\\d{2})\\b\"),\n",
        "        (\"debian\", r\"\\bdebian\\s+(\\d{1,2})\\b\"),\n",
        "        (\"rhel\", r\"\\b(?:rhel|red\\s*hat(?:\\s+enterprise)?\\s+linux)\\s+(\\d{1,2})\\b\"),\n",
        "        (\"centos\", r\"\\bcentos\\s+(\\d{1,2})\\b\"),\n",
        "        (\"rocky linux\", r\"\\brocky\\s+linux\\s+(\\d{1,2})\\b\"),\n",
        "        (\"almalinux\", r\"\\balma\\s*linux\\s+(\\d{1,2})\\b\"),\n",
        "        (\"oracle linux\", r\"\\boracle\\s+linux\\s+(\\d{1,2})\\b\"),\n",
        "        (\"opensuse\", r\"\\bopen\\s*suse\\b|\\bopensuse\\b\"),\n",
        "    ]:\n",
        "        m = re.search(pat, t)\n",
        "        if m:\n",
        "            ver = \"\"\n",
        "            if m.groups():\n",
        "                g = next((g for g in m.groups() if g), \"\")\n",
        "                ver = g.strip()\n",
        "            pairs.append((name, ver))\n",
        "\n",
        "    # Deduplicate while preserving order\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for p in pairs:\n",
        "        if p not in seen:\n",
        "            seen.add(p)\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Main rule (original logic + EOL overlay)\n",
        "# =========================\n",
        "def rule_a6_2_3(text: str) -> Dict[str, object]:\n",
        "    \"\"\"\n",
        "    A6.2.3 – List your email applications (version required)\n",
        "    - Any 'attached', 'see attached', 'see appendix', etc: compliant.\n",
        "    - Legacy/unsupported clients: non-compliant.\n",
        "    - Any number with a dot, anywhere with an app: counts as version provided.\n",
        "    - Webmail/browser access with app: counts as version provided.\n",
        "    - Otherwise, needs_more_info if no app+version.\n",
        "    - EOL overlay runs last and may set auto_fail + non_compliant with reason.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    lowered = text.lower()\n",
        "\n",
        "    frame = {\n",
        "        \"email_apps\":         [],\n",
        "        \"unsupported_apps\":   [],\n",
        "        \"versions_provided\":  False,\n",
        "        \"unsupported_flag\":   False,\n",
        "        \"compliant\":          False,\n",
        "        \"non_compliant\":      False,\n",
        "        \"needs_more_info\":    False,\n",
        "        # --- EOL overlay fields ---\n",
        "        \"auto_fail\":          False,\n",
        "        \"eol_hits\":           [],\n",
        "        \"near_eol_hits\":      [],\n",
        "        \"reason\":             \"\"\n",
        "    }\n",
        "\n",
        "    # --- 1. Accept \"see attached\", \"see appendix\", etc as compliant\n",
        "    attach_phrases = [\n",
        "        \"attached\", \"see attached\", \"see appendix\", \"see schedule\", \"see document\"\n",
        "    ]\n",
        "    if any(phrase in lowered for phrase in attach_phrases):\n",
        "        frame[\"compliant\"] = True\n",
        "        return frame\n",
        "\n",
        "    # --- 2. Known/accepted app variants and aliases\n",
        "    app_names = [\n",
        "        \"microsoft outlook\", \"ms outlook\", \"outlook\",\n",
        "        \"exchange\", \"ms exchange\", \"exchange server\",\n",
        "        \"gmail\", \"google mail\", \"google workspace\",\n",
        "        \"office 365\", \"microsoft 365\", \"ms 365\", \"o365\", \"m365\", \"microsoft 365 apps\",\n",
        "        \"superhuman\", \"iphone mail\", \"apple mail\",\n",
        "        \"thunderbird\", \"mailbird\", \"bluemail\", \"protonmail\", \"zoho mail\"\n",
        "    ]\n",
        "\n",
        "    # --- 3. Regex to find tool+version pairs (robust spacing/wording)\n",
        "    app_rx = {\n",
        "        name: re.compile(rf\"{re.escape(name)}[^\\w\\d]{{0,8}}v?(?:ersion)?[^\\w\\d]{{0,6}}?([\\d\\.]+)?\", re.I)\n",
        "        for name in app_names\n",
        "    }\n",
        "\n",
        "    # --- 4. Legacy/unsupported clients\n",
        "    legacy_rx = {\n",
        "        \"outlook 2010\":        r\"\\boutlook\\s*2010\\b\",\n",
        "        \"outlook 2013\":        r\"\\boutlook\\s*2013\\b\",\n",
        "        \"outlook express\":     r\"\\boutlook\\s*express\\b\",\n",
        "        \"windows live mail\":   r\"\\bwindows\\s*live\\s*mail\\b\",\n",
        "        \"lotus notes\":         r\"\\blotus\\s*notes\\b\",\n",
        "        \"eudora\":              r\"\\beudora\\b\",\n",
        "        # Old Office suites (even if not “Outlook” named)\n",
        "        \"office 2010\":         r\"\\boffice\\s*2010\\b\",\n",
        "        \"office 2013\":         r\"\\boffice\\s*2013\\b\",\n",
        "    }\n",
        "\n",
        "    # --- 5. Webmail/browser phrases\n",
        "    webmail_phrases = [\n",
        "        \"webmail\", \"web-based\", \"web based\", \"browser\", \"accessed through browser\",\n",
        "        \"accessed via web\", \"web email\", \"webemail\"\n",
        "    ]\n",
        "\n",
        "    found_apps = set()\n",
        "    found_versions = set()\n",
        "\n",
        "    # --- 6. Regex match for known apps + attached version\n",
        "    for name, rx in app_rx.items():\n",
        "        for m in rx.finditer(lowered):\n",
        "            found_apps.add(name)\n",
        "            ver = m.group(1)\n",
        "            if ver and re.search(r\"\\d\", ver):\n",
        "                found_versions.add(ver)\n",
        "\n",
        "    # --- 7. Legacy/unsupported check\n",
        "    for label, rx in legacy_rx.items():\n",
        "        if re.search(rx, lowered):\n",
        "            frame[\"unsupported_apps\"].append(label)\n",
        "            frame[\"unsupported_flag\"] = True\n",
        "\n",
        "    # --- 8. spaCy fallback for PRODUCT/ORG names\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in {\"PRODUCT\", \"ORG\"}:\n",
        "            token = ent.text.lower()\n",
        "            for name in app_names:\n",
        "                if name in token and name not in found_apps:\n",
        "                    found_apps.add(name)\n",
        "\n",
        "    # --- 9. Alias match in text (e.g., \"o365\", \"m365\")\n",
        "    for alias in [\"ms outlook\", \"ms exchange\", \"ms 365\", \"o365\", \"m365\", \"google mail\"]:\n",
        "        if alias in lowered and alias not in found_apps:\n",
        "            found_apps.add(alias)\n",
        "\n",
        "    # --- 10. Webmail/browser logic: if app and webmail/browser phrase, count as version provided\n",
        "    if found_apps and any(phrase in lowered for phrase in webmail_phrases):\n",
        "        frame[\"versions_provided\"] = True\n",
        "\n",
        "    # --- 11. Generic version pattern: any number with at least one dot, anywhere, if app present\n",
        "    if found_apps and (found_versions or re.search(r\"\\b\\d+\\.\\d+(?:\\.\\d+){0,2}\\b\", lowered)):\n",
        "        frame[\"versions_provided\"] = True\n",
        "\n",
        "    frame[\"email_apps\"] = sorted(found_apps)\n",
        "    frame[\"unsupported_apps\"] = sorted(set(frame[\"unsupported_apps\"]))\n",
        "\n",
        "    # --- 11b. If \"version\" (with misspellings) is followed by number, count as version provided\n",
        "    if found_apps:\n",
        "        version_word_rx = r\"\\bver(?:sion|sion|ison|soin|s|rsion|rison)?\\b[\\s:]*([0-9]+(?:\\.[0-9]+){0,3})\"\n",
        "        if re.search(version_word_rx, lowered):\n",
        "            frame[\"versions_provided\"] = True\n",
        "\n",
        "    # --- 12. Label logic\n",
        "    if frame[\"unsupported_flag\"]:\n",
        "        frame[\"non_compliant\"] = True\n",
        "    elif frame[\"email_apps\"] and frame[\"versions_provided\"]:\n",
        "        frame[\"compliant\"] = True\n",
        "    else:\n",
        "        frame[\"needs_more_info\"] = True\n",
        "\n",
        "    # --- Reason assignment (pre-EOL overlay) ---\n",
        "    if frame[\"compliant\"]:\n",
        "        frame[\"reason\"] = f\"Compliant: Email app(s) listed ({', '.join(frame['email_apps'])}) with version(s) or browser-based access.\"\n",
        "    elif frame[\"non_compliant\"]:\n",
        "        frame[\"reason\"] = f\"Non-compliant: Unsupported or legacy email client(s) detected ({', '.join(frame['unsupported_apps'])}).\"\n",
        "    elif frame[\"needs_more_info\"]:\n",
        "        if not frame[\"email_apps\"]:\n",
        "            frame[\"reason\"] = \"Needs more information: No email applications detected.\"\n",
        "        elif not frame[\"versions_provided\"]:\n",
        "            frame[\"reason\"] = \"Needs more information: Email app(s) listed but version information is missing.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Needs more information: Incomplete information on email tools and versions.\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: Unclear response.\"\n",
        "\n",
        "    # =========================\n",
        "    # EOL overlay (runs last)\n",
        "    #   • If any EOL → set auto_fail + non_compliant and override reason\n",
        "    #   • If any near-EOL → append note to reason\n",
        "    # =========================\n",
        "    try:\n",
        "        candidates = _a623_eol_candidates_from_email_text(text)\n",
        "        eol_hits, near_hits = [], []\n",
        "\n",
        "        for prod_key, ver in candidates:\n",
        "            status, eol_date, days_left, slug = _a623_eol_check(prod_key, ver)\n",
        "            label = f\"{slug} {ver}\".strip()\n",
        "            if status == \"eol\":\n",
        "                eol_hits.append(f\"{label} – EOL {eol_date}\")\n",
        "            elif status == \"near\":\n",
        "                near_hits.append(f\"{label} – EOL {eol_date} (in {days_left} days)\")\n",
        "\n",
        "        if eol_hits:\n",
        "            frame[\"auto_fail\"] = True\n",
        "            frame[\"non_compliant\"] = True\n",
        "            frame[\"compliant\"] = False\n",
        "            frame[\"needs_more_info\"] = False\n",
        "            frame[\"eol_hits\"] = eol_hits\n",
        "            frame[\"reason\"] = \"Auto-fail: End-of-life software detected — \" + \"; \".join(eol_hits) + \".\"\n",
        "        elif near_hits:\n",
        "            frame[\"near_eol_hits\"] = near_hits\n",
        "            frame[\"reason\"] = frame[\"reason\"].rstrip(\".\") + \". Note: Near end-of-life — \" + \"; \".join(near_hits) + \".\"\n",
        "\n",
        "    except Exception:\n",
        "        # If EOL lookups fail, keep the original label/reason intact.\n",
        "        pass\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------ #\n",
        "#  A6.2.4 – “List Office applications that create organisational data”\n",
        "#       (UPDATED with endoflife.date EOL overlay)\n",
        "#  Helpers renamed/namespaced only; functionality unchanged\n",
        "# ------------------------------------------------------------------ #\n",
        "\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n",
        "\n",
        "# =========================\n",
        "# EOL helpers (endoflife.date) — A6.2.4 namespace\n",
        "# =========================\n",
        "import requests\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "A624_NEAR_EOL_DAYS = 180  # tweak the “near EOL” note window\n",
        "\n",
        "# Map product name → endoflife.date slug\n",
        "# (Includes Office year SKUs, M365 Apps, common FOSS office suites, and optional OSes)\n",
        "_A624_EOL_SLUGS = {\n",
        "    # Microsoft suites\n",
        "    \"microsoft 365 apps\": \"microsoft-365-apps\",\n",
        "    \"office 365\": \"microsoft-365-apps\",\n",
        "    \"microsoft 365\": \"microsoft-365-apps\",\n",
        "    \"o365\": \"microsoft-365-apps\",\n",
        "    \"m365\": \"microsoft-365-apps\",\n",
        "    \"microsoft office\": \"microsoft-365-apps\",  # default when no year given\n",
        "    \"ms office\": \"microsoft-365-apps\",\n",
        "    \"office\": \"microsoft-365-apps\",\n",
        "\n",
        "    # FOSS / other office suites\n",
        "    \"libreoffice\": \"libreoffice\",\n",
        "    \"onlyoffice\": \"onlyoffice-desktop-editors\",  # desktop editors slug\n",
        "    \"only office\": \"onlyoffice-desktop-editors\",\n",
        "    \"openoffice\": \"openoffice\",\n",
        "    \"open office\": \"openoffice\",\n",
        "    \"wps office\": \"wps-office\",  # may not exist on EoL; safe to try\n",
        "    \"wps\": \"wps-office\",\n",
        "\n",
        "    # Google Workspace (rarely has “EOL”, but included for completeness)\n",
        "    \"google workspace\": \"google-workspace\",  # may return unknown; harmless\n",
        "    \"gsuite\": \"google-workspace\",\n",
        "    \"g suite\": \"google-workspace\",\n",
        "    \"google docs\": \"google-workspace\",\n",
        "\n",
        "    # Adobe/affinity (unlikely to have EoL pages, still harmless to try)\n",
        "    \"adobe creative cloud\": \"adobe-creative-cloud\",\n",
        "    \"adobe cc\": \"adobe-creative-cloud\",\n",
        "    \"adobe creative suite\": \"adobe-creative-suite\",\n",
        "    \"affinity publisher\": \"affinity-publisher\",\n",
        "\n",
        "    # Optional OS/platforms (if they appear in the same answer)\n",
        "    \"windows\": \"windows\", \"windows 10\": \"windows\", \"windows 11\": \"windows\",\n",
        "    \"macos\": \"macos\", \"os x\": \"macos\",\n",
        "    \"ubuntu\": \"ubuntu\", \"debian\": \"debian\",\n",
        "    \"rhel\": \"rhel\", \"red hat\": \"rhel\", \"red hat enterprise linux\": \"rhel\",\n",
        "    \"centos\": \"centos\", \"rocky linux\": \"rocky-linux\", \"almalinux\": \"almalinux-os\",\n",
        "    \"oracle linux\": \"oracle-linux\", \"opensuse\": \"opensuse\",\n",
        "    \"arch linux\": \"archlinux\", \"kali\": \"kali\", \"parrot\": \"parrot\",\n",
        "}\n",
        "\n",
        "# Explicit mapping for Office *year* SKUs\n",
        "_A624_OFFICE_YEAR_TO_SLUG = {\n",
        "    \"2003\": \"office-2003\",\n",
        "    \"2007\": \"office-2007\",\n",
        "    \"2010\": \"office-2010\",\n",
        "    \"2013\": \"office-2013\",\n",
        "    \"2016\": \"office-2016\",\n",
        "    \"2019\": \"office-2019\",\n",
        "    \"2021\": \"office-2021\",\n",
        "    \"2024\": \"office-ltsc-2024\",  # Office LTSC 2024\n",
        "}\n",
        "\n",
        "def _a624_eol_resolve_slug(name: str, version_hint: str = \"\") -> str | None:\n",
        "    \"\"\"\n",
        "    Resolve a product name (and optional version) to an endoflife.date slug.\n",
        "    Special-cases:\n",
        "      - “Office/Outlook 20xx” → corresponding Office year slug\n",
        "      - “Office 365 / Microsoft 365 Apps / O365 / M365” → microsoft-365-apps\n",
        "      - Fuzzy contains fallback for dictionary above\n",
        "    \"\"\"\n",
        "    if not name:\n",
        "        return None\n",
        "    key = name.lower().strip()\n",
        "\n",
        "    # Office/Outlook year mapping from name or version\n",
        "    y = None\n",
        "    m_name = re.search(r\"\\b(2003|2007|2010|2013|2016|2019|2021|2024)\\b\", key)\n",
        "    m_ver  = re.search(r\"\\b(2003|2007|2010|2013|2016|2019|2021|2024)\\b\", (version_hint or \"\"))\n",
        "    if m_name:\n",
        "        y = m_name.group(1)\n",
        "    elif m_ver:\n",
        "        y = m_ver.group(1)\n",
        "    if (\"office\" in key or \"outlook\" in key) and y and y in _A624_OFFICE_YEAR_TO_SLUG:\n",
        "        return _A624_OFFICE_YEAR_TO_SLUG[y]\n",
        "\n",
        "    # Microsoft 365 Apps variants\n",
        "    if (\"365\" in key and (\"office\" in key or \"microsoft\" in key)) or \"microsoft 365 apps\" in key:\n",
        "        return \"microsoft-365-apps\"\n",
        "\n",
        "    # Dictionary lookup + fuzzy contains\n",
        "    if key in _A624_EOL_SLUGS:\n",
        "        return _A624_EOL_SLUGS[key]\n",
        "    for k, v in _A624_EOL_SLUGS.items():\n",
        "        if k in key:\n",
        "            return v\n",
        "    return None\n",
        "\n",
        "def _a624_eol_fetch_cycles(slug: str):\n",
        "    \"\"\"Fetch & cache product cycles from endoflife.date.\"\"\"\n",
        "    if not hasattr(_a624_eol_fetch_cycles, \"_cache\"):\n",
        "        _a624_eol_fetch_cycles._cache = {}\n",
        "    cache = _a624_eol_fetch_cycles._cache\n",
        "    if slug in cache:\n",
        "        return cache[slug]\n",
        "    url = f\"https://endoflife.date/api/v1/products/{slug}.json\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=8)\n",
        "        if r.ok:\n",
        "            cache[slug] = r.json()\n",
        "            return cache[slug]\n",
        "    except Exception:\n",
        "        pass\n",
        "    cache[slug] = None\n",
        "    return None\n",
        "\n",
        "def _a624_norm(s: str) -> str:\n",
        "    return (s or \"\").lower().replace(\" \", \"\").strip()\n",
        "\n",
        "def _a624_eol_best_match(cycles: list, version_hint: str):\n",
        "    \"\"\"\n",
        "    Loose match between version_hint and a cycle entry.\n",
        "    Works for year, semantic versions, or channel names (when present).\n",
        "    \"\"\"\n",
        "    if not cycles or not version_hint:\n",
        "        return None\n",
        "    q = str(version_hint).strip().lower()\n",
        "    qn = _a624_norm(q)\n",
        "    for c in cycles:\n",
        "        cyc = str(c.get(\"cycle\") or c.get(\"releaseCycle\") or \"\")\n",
        "        cl = cyc.lower()\n",
        "        cn = _a624_norm(cyc)\n",
        "        if not cn:\n",
        "            continue\n",
        "        if qn == cn or q in cl or cl in q:\n",
        "            return c\n",
        "        if cl.startswith(q):\n",
        "            return c\n",
        "    # numeric-major fallback (e.g., \"7\" → \"7.x\" or \"24.2\")\n",
        "    try:\n",
        "        qmaj = q.split(\".\")[0]\n",
        "        if qmaj:\n",
        "            for c in cycles:\n",
        "                cyc = str(c.get(\"cycle\") or \"\")\n",
        "                if str(cyc).split(\".\")[0] == qmaj:\n",
        "                    return c\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def _a624_eol_status(entry: dict, today=None) -> Tuple[str, str, int]:\n",
        "    \"\"\"\n",
        "    Returns: (status, eol_date_str, days_left)\n",
        "      status ∈ {\"eol\", \"near\", \"ok\", \"unknown\"}\n",
        "    \"\"\"\n",
        "    if not entry:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "    eol = entry.get(\"eol\") or entry.get(\"support\") or entry.get(\"discontinued\")\n",
        "    if not eol or isinstance(eol, bool):\n",
        "        return (\"unknown\", \"\", 0)\n",
        "    try:\n",
        "        d = datetime.fromisoformat(str(eol)).replace(tzinfo=timezone.utc)\n",
        "        today = today or datetime.now(timezone.utc)\n",
        "        days = (d - today).days\n",
        "        if days < 0:\n",
        "            return (\"eol\", d.date().isoformat(), days)\n",
        "        if days <= A624_NEAR_EOL_DAYS:\n",
        "            return (\"near\", d.date().isoformat(), days)\n",
        "        return (\"ok\", d.date().isoformat(), days)\n",
        "    except Exception:\n",
        "        return (\"unknown\", \"\", 0)\n",
        "\n",
        "def _a624_eol_check(product_name: str, version_text: str) -> Tuple[str, str, int, str]:\n",
        "    \"\"\"\n",
        "    Resolve product → fetch cycles → match cycle → compute status.\n",
        "    Returns: (status, eol_date, days_left, slug)\n",
        "    \"\"\"\n",
        "    slug = _a624_eol_resolve_slug(product_name, version_text) or product_name\n",
        "    cycles = _a624_eol_fetch_cycles(slug)\n",
        "    if not cycles:\n",
        "        return (\"unknown\", \"\", 0, slug)\n",
        "    entry = _a624_eol_best_match(cycles, version_text)\n",
        "    status, eol_date, days = _a624_eol_status(entry)\n",
        "    return (status, eol_date, days, slug)\n",
        "\n",
        "def _a624_eol_candidates_from_office_text(text: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Extract (product_hint, version_hint) from free text for A6.2.4:\n",
        "      - Office/Outlook 20xx / LTSC 2024 / 365 variants\n",
        "      - LibreOffice / OpenOffice / ONLYOFFICE + version if present\n",
        "      - WPS Office, Google Workspace (best effort)\n",
        "      - Optional OS mentions (Windows/macOS/Linux) with versions\n",
        "    \"\"\"\n",
        "    t = text.lower()\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "\n",
        "    # Office / Outlook year SKUs / LTSC\n",
        "    for m in re.finditer(r\"\\b(?:microsoft\\s+)?(?:office|outlook)\\s+(2003|2007|2010|2013|2016|2019|2021|2024)\\b\", t):\n",
        "        y = m.group(1)\n",
        "        pairs.append((f\"office {y}\", y))\n",
        "    # LTSC wording\n",
        "    if re.search(r\"\\boffice\\s*(ltsc)\\s*2024\\b\", t):\n",
        "        pairs.append((\"office 2024\", \"2024\"))\n",
        "\n",
        "    # Microsoft 365 Apps / 365 aliases\n",
        "    if re.search(r\"\\bmicrosoft\\s+365\\s+apps\\b\", t):\n",
        "        pairs.append((\"microsoft 365 apps\", \"\"))\n",
        "    if re.search(r\"\\b(office\\s*365|microsoft\\s*365|o365|m365)\\b\", t):\n",
        "        pairs.append((\"microsoft 365 apps\", \"\"))\n",
        "\n",
        "    # LibreOffice, OpenOffice, ONLYOFFICE, WPS (capture versions)\n",
        "    for name in [\"libreoffice\", \"openoffice\", \"onlyoffice\", \"only office\", \"wps office\", \"wps\"]:\n",
        "        # e.g., \"libreoffice 7.6\", \"openoffice v4.1.14\"\n",
        "        m_iter = re.finditer(rf\"\\b{name}\\b[^\\w\\d]{{0,12}}v?(?:ersion)?[^\\w\\d]{{0,6}}?(\\d+(?:\\.\\d+)+)\", t)\n",
        "        captured = False\n",
        "        for m in m_iter:\n",
        "            pairs.append((name, m.group(1)))\n",
        "            captured = True\n",
        "        if not captured and re.search(rf\"\\b{name}\\b\", t):\n",
        "            # No explicit numeric version, still include product to allow EOL check\n",
        "            pairs.append((name, \"\"))\n",
        "\n",
        "    # Google Workspace / Docs\n",
        "    if re.search(r\"\\b(google\\s+workspace|gsuite|g\\s*suite)\\b\", t):\n",
        "        pairs.append((\"google workspace\", \"\"))\n",
        "\n",
        "    # Optional OS/platforms (if they’re mentioned with versions)\n",
        "    # Windows 10/11 + channels\n",
        "    for m in re.finditer(r\"\\bwindows\\s+(10|11)\\s*(\\d{2}h\\d)?\\b\", t):\n",
        "        ver = (m.group(1) + (\" \" + (m.group(2) or \"\").upper()).strip()).strip()\n",
        "        pairs.append((\"windows\", ver))\n",
        "    if re.search(r\"\\bwindows\\s+10\\b\", t) and not any(p[0] == \"windows\" and p[1].startswith(\"10\") for p in pairs):\n",
        "        pairs.append((\"windows\", \"10\"))\n",
        "    if re.search(r\"\\bwindows\\s+11\\b\", t) and not any(p[0] == \"windows\" and p[1].startswith(\"11\") for p in pairs):\n",
        "        pairs.append((\"windows\", \"11\"))\n",
        "\n",
        "    # macOS numeric\n",
        "    for m in re.finditer(r\"\\bmac\\s?os\\s*(\\d{2}(?:\\.\\d+)*)\\b\", t):\n",
        "        pairs.append((\"macos\", m.group(1)))\n",
        "\n",
        "    # Linux distros (best-effort)\n",
        "    for name, pat in [\n",
        "        (\"ubuntu\", r\"\\bubuntu\\s+(\\d{2}\\.\\d{2})\\b\"),\n",
        "        (\"debian\", r\"\\bdebian\\s+(\\d{1,2})\\b\"),\n",
        "        (\"rhel\", r\"\\b(?:rhel|red\\s*hat(?:\\s+enterprise)?\\s+linux)\\s+(\\d{1,2})\\b\"),\n",
        "        (\"centos\", r\"\\bcentos\\s+(\\d{1,2})\\b\"),\n",
        "        (\"rocky linux\", r\"\\brocky\\s+linux\\s+(\\d{1,2})\\b\"),\n",
        "        (\"almalinux\", r\"\\balma\\s*linux\\s+(\\d{1,2})\\b\"),\n",
        "        (\"oracle linux\", r\"\\boracle\\s+linux\\s+(\\d{1,2})\\b\"),\n",
        "        (\"opensuse\", r\"\\bopen\\s*suse\\b|\\bopensuse\\b\"),\n",
        "    ]:\n",
        "        m = re.search(pat, t)\n",
        "        if m:\n",
        "            ver = \"\"\n",
        "            if m.groups():\n",
        "                g = next((g for g in m.groups() if g), \"\")\n",
        "                ver = g.strip()\n",
        "            pairs.append((name, ver))\n",
        "\n",
        "    # Deduplicate in-order\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for p in pairs:\n",
        "        if p not in seen:\n",
        "            seen.add(p)\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Main rule (original logic + EOL overlay)\n",
        "# =========================\n",
        "def rule_a6_2_4(text: str) -> Dict[str, object]:\n",
        "    \"\"\"\n",
        "    A6.2.4 – List your office/document software (version required)\n",
        "    - Accepts 'attached', 'see appendix', etc as compliant.\n",
        "    - Legacy/EOL suites: non-compliant.\n",
        "    - If 'version' (misspelled ok) is followed by a number (with or without dot): version provided.\n",
        "    - Otherwise, only numbers with at least one dot (e.g. X.Y, X.Y.Z) count as version.\n",
        "    - Browser/web/online logic: version provided if app and phrase.\n",
        "    - Otherwise, needs_more_info if no app+version.\n",
        "    - EOL overlay runs last and may set auto_fail + non_compliant with reason.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    lowered = text.lower()\n",
        "\n",
        "    frame = {\n",
        "        \"office_apps\":        [],\n",
        "        \"unsupported_apps\":   [],\n",
        "        \"versions_provided\":  False,\n",
        "        \"unsupported_flag\":   False,\n",
        "        \"compliant\":          False,\n",
        "        \"non_compliant\":      False,\n",
        "        \"needs_more_info\":    False,\n",
        "        # --- EOL overlay fields ---\n",
        "        \"auto_fail\":          False,\n",
        "        \"eol_hits\":           [],\n",
        "        \"near_eol_hits\":      [],\n",
        "        \"reason\":             \"\"\n",
        "    }\n",
        "\n",
        "    # 1. Accept \"see attached\", \"see appendix\", etc as compliant\n",
        "    attach_phrases = [\n",
        "        \"attached\", \"see attached\", \"see appendix\", \"see schedule\", \"see document\"\n",
        "    ]\n",
        "    if any(phrase in lowered for phrase in attach_phrases):\n",
        "        frame[\"compliant\"] = True\n",
        "        return frame\n",
        "\n",
        "    # 2. Known/accepted suites + aliases\n",
        "    app_names = [\n",
        "        \"microsoft 365\", \"ms 365\", \"office 365\", \"microsoft office\", \"ms office\", \"o365\", \"m365\",\n",
        "        \"google workspace\", \"google docs\", \"google drive\", \"gsuite\", \"g suite\",\n",
        "        \"libreoffice\", \"libre office\", \"onlyoffice\", \"only office\",\n",
        "        \"office 2016\", \"office 2019\", \"office 2021\", \"office 2024\",\n",
        "        \"affinity publisher\", \"adobe creative cloud\", \"adobe cc\", \"adobe suite\", \"adobe creative suite\",\n",
        "        \"netdocuments\", \"ndoffice\", \"wps office\", \"wps\", \"openoffice\", \"open office\"\n",
        "    ]\n",
        "    app_rx = {\n",
        "        name: re.compile(rf\"{re.escape(name)}[^\\w\\d]{{0,8}}v?(?:ersion)?[^\\w\\d]{{0,6}}?([\\d\\.]+)?\", re.I)\n",
        "        for name in app_names\n",
        "    }\n",
        "\n",
        "    # 3. Legacy/EOL suites (hard-stop non-compliant list)\n",
        "    legacy_rx = {\n",
        "        \"office 2003\":        r\"\\boffice\\s*2003\\b\",\n",
        "        \"office 2007\":        r\"\\boffice\\s*2007\\b\",\n",
        "        \"office 2010\":        r\"\\boffice\\s*2010\\b\",\n",
        "        \"office 2013\":        r\"\\boffice\\s*2013\\b\",\n",
        "        \"openoffice\":         r\"\\bopen[\\s\\-]?office\\b\",  # treat as legacy here\n",
        "        \"lotus notes\":        r\"\\blotus\\s+notes\\b\",\n",
        "        \"wordperfect\":        r\"\\bwordperfect\\b\",\n",
        "    }\n",
        "\n",
        "    # 4. Web/online/browser access phrases\n",
        "    web_phrases = [\n",
        "        \"web-based\", \"web based\", \"browser\", \"browser-based\",\n",
        "        \"online\", \"cloud\", \"online version\", \"in the browser\"\n",
        "    ]\n",
        "\n",
        "    found_apps = set()\n",
        "    found_versions = set()\n",
        "    found_version_word = False\n",
        "\n",
        "    # 5. Regex match for known apps + attached version\n",
        "    for name, rx in app_rx.items():\n",
        "        for m in rx.finditer(lowered):\n",
        "            found_apps.add(name)\n",
        "            ver = m.group(1)\n",
        "            if ver and re.search(r\"\\d\", ver):\n",
        "                found_versions.add(ver)\n",
        "\n",
        "    # 6. Legacy/EOL check\n",
        "    for label, rx in legacy_rx.items():\n",
        "        if re.search(rx, lowered):\n",
        "            frame[\"unsupported_apps\"].append(label)\n",
        "            frame[\"unsupported_flag\"] = True\n",
        "\n",
        "    # 7. spaCy PRODUCT fallback (suite-y hints)\n",
        "    SUITE_HINT = re.compile(r\"(office|suite|docs|cloud|workspace|wps|libre|only)\", re.I)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PRODUCT\" and SUITE_HINT.search(ent.text):\n",
        "            t = ent.text.strip()\n",
        "            if t.lower() not in [a.lower() for a in found_apps]:\n",
        "                found_apps.add(t)\n",
        "\n",
        "    # 8. Alias matching\n",
        "    for alias in [\"ms 365\", \"ms office\", \"o365\", \"m365\", \"gsuite\", \"libre office\", \"open office\", \"only office\"]:\n",
        "        if alias in lowered and alias not in found_apps:\n",
        "            found_apps.add(alias)\n",
        "\n",
        "    # 9. Web/online/browser logic: if app and web phrase, version provided\n",
        "    if found_apps and any(phrase in lowered for phrase in web_phrases):\n",
        "        frame[\"versions_provided\"] = True\n",
        "\n",
        "    # 10. \"version\" (misspelled ok) followed by number (with or without dot)\n",
        "    if found_apps:\n",
        "        # Accept \"version\", \"verison\", \"versoin\", etc., followed by number (with or without dot)\n",
        "        version_word_rx = r\"\\bver(?:sion|sion|ison|soin|s|rsion|rison)?\\b[\\s:]*([0-9]+(?:\\.[0-9]+){0,3}|[0-9]+)\"\n",
        "        m = re.search(version_word_rx, lowered)\n",
        "        if m:\n",
        "            frame[\"versions_provided\"] = True\n",
        "            found_version_word = True\n",
        "\n",
        "    # 11. Generic dotted number anywhere if app present and no explicit \"version\" word found\n",
        "    if found_apps and not frame[\"versions_provided\"]:\n",
        "        if re.search(r\"\\b\\d+\\.\\d+(?:\\.\\d+){0,2}\\b\", lowered):\n",
        "            frame[\"versions_provided\"] = True\n",
        "\n",
        "    frame[\"office_apps\"] = sorted(found_apps)\n",
        "    frame[\"unsupported_apps\"] = sorted(set(frame[\"unsupported_apps\"]))\n",
        "\n",
        "    # 12. Label logic (unchanged)\n",
        "    if frame[\"unsupported_flag\"]:\n",
        "        frame[\"non_compliant\"] = True\n",
        "    elif frame[\"office_apps\"] and frame[\"versions_provided\"]:\n",
        "        frame[\"compliant\"] = True\n",
        "    else:\n",
        "        frame[\"needs_more_info\"] = True\n",
        "\n",
        "    # --- Reason assignment (pre-EOL overlay) ---\n",
        "    if frame[\"compliant\"]:\n",
        "        frame[\"reason\"] = f\"Compliant: Office/document software listed ({', '.join(frame['office_apps'])}) with version(s) or browser-based access.\"\n",
        "    elif frame[\"non_compliant\"]:\n",
        "        frame[\"reason\"] = f\"Non-compliant: Unsupported or legacy software detected ({', '.join(frame['unsupported_apps'])}).\"\n",
        "    elif frame[\"needs_more_info\"]:\n",
        "        if not frame[\"office_apps\"]:\n",
        "            frame[\"reason\"] = \"Needs more information: No office or document editing software detected.\"\n",
        "        elif not frame[\"versions_provided\"]:\n",
        "            frame[\"reason\"] = \"Needs more information: Software listed but version information missing.\"\n",
        "        else:\n",
        "            frame[\"reason\"] = \"Needs more information: Incomplete details about document software and versioning.\"\n",
        "    else:\n",
        "        frame[\"reason\"] = \"Needs more information: Unclear response.\"\n",
        "\n",
        "    # =========================\n",
        "    # EOL overlay (runs last)\n",
        "    #   • If any EOL → set auto_fail + non_compliant and override reason\n",
        "    #   • If any near-EOL → append note to reason\n",
        "    # =========================\n",
        "    try:\n",
        "        candidates = _a624_eol_candidates_from_office_text(text)\n",
        "        eol_hits, near_hits = [], []\n",
        "\n",
        "        for prod_key, ver in candidates:\n",
        "            status, eol_date, days_left, slug = _a624_eol_check(prod_key, ver)\n",
        "            label = f\"{slug} {ver}\".strip()\n",
        "            if status == \"eol\":\n",
        "                eol_hits.append(f\"{label} – EOL {eol_date}\")\n",
        "            elif status == \"near\":\n",
        "                near_hits.append(f\"{label} – EOL {eol_date} (in {days_left} days)\")\n",
        "\n",
        "        if eol_hits:\n",
        "            frame[\"auto_fail\"] = True\n",
        "            frame[\"non_compliant\"] = True\n",
        "            frame[\"compliant\"] = False\n",
        "            frame[\"needs_more_info\"] = False\n",
        "            frame[\"eol_hits\"] = eol_hits\n",
        "            frame[\"reason\"] = \"Auto-fail: End-of-life software detected — \" + \"; \".join(eol_hits) + \".\"\n",
        "        elif near_hits:\n",
        "            frame[\"near_eol_hits\"] = near_hits\n",
        "            frame[\"reason\"] = frame[\"reason\"].rstrip(\".\") + \". Note: Near end-of-life — \" + \"; \".join(near_hits) + \".\"\n",
        "\n",
        "    except Exception:\n",
        "        # If EOL lookups fail, keep original label/reason intact.\n",
        "        pass\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  Register rule dispatch map\n",
        "question_rules = {\n",
        "\"A2.2\": rule_a2_2,\n",
        "\"A2.10\": rule_a2_10,\n",
        "\"A2.3\": rule_a2_3,\n",
        "\"A2.4\": rule_a2_4,\n",
        "\"A2.4.1\": rule_a2_4_1,\n",
        "\"A2.5\": rule_a2_5,\n",
        "\"A2.6\": rule_a2_6,\n",
        "\"A2.7\": rule_a2_7,\n",
        "\"A2.7.1\": rule_a2_7_1,\n",
        "\"A2.8\": rule_a2_8,\n",
        "\"A2.9\": rule_a2_9,\n",
        "\"A4.12\": rule_a4_12,\n",
        "\"A6.2.1\": rule_a6_2_1,\n",
        "\"A6.2.2\": rule_a6_2_2,\n",
        "\"A6.2.3\": rule_a6_2_3,\n",
        "\"A6.2.4\": rule_a6_2_4\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# --- semantic extraction function ---\n",
        "def extract_semantic_frame(question_id, answer_text):\n",
        "    rule_fn = question_rules.get(question_id)\n",
        "    if rule_fn is None:\n",
        "        return None\n",
        "    return rule_fn(answer_text)\n",
        "\n",
        "# Canonical labels (change here once, if you ever need to)\n",
        "LABELS = {\n",
        "    \"FAIL\": \"Fail\",\n",
        "    \"COMPLIANT\": \"Compliant\",\n",
        "    \"MORE_INFO\": \"More-information\",\n",
        "    \"NON_COMPLIANT\": \"Non-compliant\",\n",
        "    \"UNKNOWN\": \"Unknown\",\n",
        "}\n",
        "\n",
        "def _normalize_legacy_flags(frame: dict) -> None:\n",
        "    \"\"\"\n",
        "    Map legacy fields/labels (e.g., non_compliant_auto_fail) to the canonical flags.\n",
        "    Mutates frame in-place.\n",
        "    \"\"\"\n",
        "    if not isinstance(frame, dict):\n",
        "        return\n",
        "\n",
        "    # Legacy boolean flag\n",
        "    if frame.get(\"non_compliant_auto_fail\"):\n",
        "        frame[\"auto_fail\"] = True\n",
        "\n",
        "    # Legacy string label\n",
        "    raw_label = str(frame.get(\"label\", \"\")).strip().lower().replace(\" \", \"\").replace(\"-\", \"_\")\n",
        "    if raw_label in {\"non_compliant_auto_fail\", \"autofail\"}:\n",
        "        frame[\"auto_fail\"] = True\n",
        "\n",
        "# --- Extract status + reason from semantic frame ---\n",
        "def get_compliance_label_and_reason(frame: dict, answer_text: str = None) -> (str, str):\n",
        "    \"\"\"\n",
        "    Final label precedence:\n",
        "      1) blank answer -> Fail\n",
        "      2) auto_fail    -> Fail\n",
        "      3) compliant    -> Compliant\n",
        "      4) needs_more_info -> More-information\n",
        "      5) non_compliant -> Non-compliant\n",
        "      6) else -> Unknown\n",
        "    \"\"\"\n",
        "    # Blank answer short-circuit (optional: pass raw input as answer_text)\n",
        "    if answer_text is not None and not str(answer_text).strip():\n",
        "        return LABELS[\"FAIL\"], \"Answer is blank.\"\n",
        "\n",
        "    if not isinstance(frame, dict) or not frame:\n",
        "        return LABELS[\"UNKNOWN\"], \"No semantic frame produced.\"\n",
        "\n",
        "    # Normalize any legacy flags/labels first\n",
        "    _normalize_legacy_flags(frame)\n",
        "\n",
        "    # 1) Auto-fail has absolute priority (covers EOL, pentest OS, etc.)\n",
        "    if frame.get(\"auto_fail\"):\n",
        "        reason = (frame.get(\"reason\") or \"Auto-fail condition triggered.\").strip()\n",
        "        return LABELS[\"FAIL\"], reason\n",
        "\n",
        "    # 2) Compliant\n",
        "    if frame.get(\"compliant\"):\n",
        "        reason = (frame.get(\"reason\") or \"Compliant answer.\").strip()\n",
        "        return LABELS[\"COMPLIANT\"], reason\n",
        "\n",
        "    # 3) Needs more info\n",
        "    if frame.get(\"needs_more_info\"):\n",
        "        reason = (frame.get(\"reason\") or \"Needs more information.\").strip()\n",
        "        return LABELS[\"MORE_INFO\"], reason\n",
        "\n",
        "    # 4) Non-compliant\n",
        "    if frame.get(\"non_compliant\"):\n",
        "        reason = (frame.get(\"reason\") or \"Non-compliant answer.\").strip()\n",
        "        return LABELS[\"NON_COMPLIANT\"], reason\n",
        "\n",
        "    # 5) Fallback\n",
        "    return LABELS[\"UNKNOWN\"], (frame.get(\"reason\") or \"\").strip()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agy0C2KR1P4E"
      },
      "source": [
        "### Rubrics Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwmY4jaBXnXZ",
        "outputId": "9d968f4c-3721-44f9-b8a1-a4d38e7d655e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing rubrics.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile rubrics.py\n",
        "\n",
        "RUBRICS = {}\n",
        "\n",
        "\n",
        "\n",
        "RUBRICS[\"A4.1.1\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Answer shows at least ONE acceptable boundary measure for remote / home workers:\"\n",
        "        \"  • software firewall on the endpoint,\"\n",
        "        \"  • corporate VPN that tunnels to a central / cloud firewall,\"\n",
        "        \"  • cloud-managed or MDM-enforced firewall rules.\",\n",
        "        \"If none of the above is mentioned the configuration is considered non-compliant.\"\n",
        "    ],\n",
        "\n",
        "    \"ref_section\": \"CE guide § A4.1.1 – Remote worker boundary controls\"\n",
        "}\n",
        "\n",
        "\n",
        "RUBRICS[\"A4.2.1\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Answer must confirm that firewall passwords ARE changed and describe HOW.\",\n",
        "        \"Indicators of a good process: interface/portal named, strong-password policy,\"\n",
        "        \" use of password manager, or formally delegated to a named third-party MSP.\",\n",
        "        \"If no change method is described the response is Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "    \"ref_section\": \"CE guide § A4.2.1 – Default password change process\"\n",
        "}\n",
        "\n",
        "RUBRICS[\"A4.10\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Must state EITHER multi-factor authentication OR restriction to trusted / fixed IPs.\",\n",
        "        \"If neither is present the access control is Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "    \"ref_section\": \"CE guide § A4.10 – Secure admin interface requirements\"\n",
        "}\n",
        "\n",
        "RUBRICS[\"A4.6\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Answer should show a REPEATABLE PROCESS for closing or removing services / ports.\",\n",
        "        \"Positive signals: explicit disable/remove verbs, change-ticket system (Jira, CAB, \"\n",
        "        \"ServiceNow), named responsible role, regular review schedule, reference to policy.\",\n",
        "        \"If no process detail is given the answer is Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "    \"ref_section\": \"CE guide § A4.6 – Requirement to disable unneeded services\"\n",
        "}\n",
        "\n",
        "RUBRICS[\"A5.1\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Answer must show the organisation *understands software minimisation* AND/OR\"\n",
        "        \" describes a *process or tool* used to keep endpoint builds clean.\",\n",
        "        \"Positive signals ⮕ any of: ‘standard build’, ‘minimal install’, Intune/Jamf/SCCM/RMM \"\n",
        "        \"automation, routine software reviews, cloud-app toggling etc.\",\n",
        "        \"If no understanding *and* no process is evident ➜ Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "    \"ref_section\": \"CE guide § A5.1 – Secure configuration / software minimisation\"\n",
        "}\n",
        "\n",
        "\n",
        "RUBRICS[\"A5.6\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Answer should outline a CLEAR password-reset process triggered by compromise \"\n",
        "        \"or vendor alert.  Indicators:\",\n",
        "        \"  • help-desk / ticket workflow (ServiceNow, Jira, CAB…),\",\n",
        "        \"  • admin-initiated or user self-service reset,\",\n",
        "        \"  • malware / AV scan as part of response,\",\n",
        "        \"  • MFA / lockout controls, strong password policy, or third-party MSP handling.\",\n",
        "        \"If no reset mechanism is described ➜ Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "    \"ref_section\": \"CE guide § A5.6 – Response to compromised credentials\"\n",
        "}\n",
        "\n",
        "RUBRICS[\"A5.10\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Answer must list at least one unlocking method **and** satisfy CE strength rules:\",\n",
        "        \"  • biometric (fingerprint / Face ID / Windows Hello / Touch ID) OR\",\n",
        "        \"  • PIN / password ≥ 6 characters, OR\",\n",
        "        \"  • MFA + ≥ 8-char password, OR\",\n",
        "        \"  • stand-alone ≥ 12-char password.\",\n",
        "        \"If no method is stated ➜ Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "    \"ref_section\": \"CE guide § A5.10 – Device unlocking requirements\"\n",
        "}\n",
        "\n",
        "RUBRICS[\"A6.4.2\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Answer must show EITHER:\",\n",
        "        \"  • Auto-updates are enabled for **all** OS + router/firewall firmware,  OR\",\n",
        "        \"  • A manual / scripted process that *explicitly* installs high-risk or \"\n",
        "        \"critical patches **within 14 days** (SLA stated).\",\n",
        "        \"Tools (Intune, Datto RMM, WSUS, etc.) are a positive signal but not mandatory.\",\n",
        "        \"If neither auto nor SLA-bound manual process is clear ➜ Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"ref_section\": \"CE guide § A6.4 – Security update mgmt (OS & firmware)\"\n",
        "}\n",
        "\n",
        "\n",
        "RUBRICS[\"A6.5.2\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Same logic as A6.4.2 but for *application* software (incl. plugins).\",\n",
        "        \"Must state auto-update OR manual process + 14-day SLA.\",\n",
        "        \"Mention of patch tools (Intune, Jamf, RMM…) is a plus.\",\n",
        "        \"If nothing shows a ≤14-day process ➜ Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "    \"ref_section\": \"CE guide § A6.5 – Security update mgmt (applications)\"\n",
        "}\n",
        "\n",
        "\n",
        "RUBRICS[\"A6.7\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Answer should state ONE of:\",\n",
        "        \"  • No unsupported software in scope **(explicit)**, OR\",\n",
        "        \"  • Unsupported software exists *but* is segregated (VLAN, air-gap, \"\n",
        "        \"firewall rule, no Internet).\",\n",
        "        \"If unsupported software admitted with *no* segregation description \"\n",
        "        \"➜ Non-Compliant.\",\n",
        "\n",
        "    ],\n",
        "\n",
        "    \"ref_section\": \"CE guide § A6.7 – Unsupported / legacy software handling\"\n",
        "}\n",
        "\n",
        "RUBRICS[\"A7.1\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Compliant answer MUST include *all three* elements:\",\n",
        "        \"  1️ Named role/authority that approves (e.g. IT Manager, HR, Director).\",\n",
        "        \"  2️ Mention of an approval *process / workflow* (ticket, form, induction, CAB…).\",\n",
        "        \"  3️ Implicit or explicit confirmation that approval happens **before** account creation.\",\n",
        "        \"Tools (ServiceDesk, JIRA, Intune, JumpCloud, etc.) = bonus clarity.\",\n",
        "        \"If none → Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"ref_section\": \"CE guide § A7 – User Access Control (account approval)\"\n",
        "}\n",
        "\n",
        "\n",
        "RUBRICS[\"A7.10\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Answer must evidence at least ONE of:\",\n",
        "        \"  • Multi-Factor Authentication (MFA / 2FA).\",\n",
        "        \"  • Account lockout ≤ 10 failed attempts (or ‘smart-lockout’).\",\n",
        "        \"  • Rate-throttling ≤ 10 guesses in 5 minutes.\",\n",
        "        \"If none of these controls described → Non-Compliant.\",\n",
        "\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"ref_section\": \"CE guide § A7.10 – Brute-force mitigation for external services\"\n",
        "}\n",
        "\n",
        "\n",
        "RUBRICS[\"A7.11\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Three CE-allowed options (any one is OK):\",\n",
        "        \"  A️⃣ MFA + ≥ 8-char password (no max length).\",\n",
        "        \"  B️⃣ ≥ 8-char password **plus** automatic block of common passwords (deny-list).\",\n",
        "        \"  C️⃣ ≥ 12-char password (no max length).\",\n",
        "        \"Max-length restrictions invalidate compliance.\",\n",
        "        \"If none of A/B/C fully met → Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"ref_section\": \"CE guide § A7.11 – Password-based authentication controls\"\n",
        "}\n",
        "\n",
        "RUBRICS[\"A7.12\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Compliant if answer shows **any** of:\",\n",
        "        \"  • Training / awareness on avoiding common or reused passwords.\",\n",
        "        \"  • Promotion of NCSC ‘Three Random Words’.\",\n",
        "        \"  • Advice or provision of password manager / secure storage.\",\n",
        "        \"  • Explicit statement that regular forced expiry **not** used, and no mandatory complexity.\",\n",
        "        \"Nothing substantive → Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"ref_section\": \"CE guide § A7.12 – User education on passwords\"\n",
        "}\n",
        "\n",
        "RUBRICS[\"A7.3\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Compliant answer MUST cover *all three* elements:\",\n",
        "        \"  1️ Existence of a formal **leaver / off-boarding process**.\",\n",
        "        \"  2️ Named parties involved (HR and/or IT).\",\n",
        "        \"  3️ Action to **disable / delete / revoke** accounts on, or very close to, the leaving date.\",\n",
        "        \"Help-desk ticketing or audit reviews = bonus but not mandatory.\",\n",
        "        \"If no process or action described → Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"ref_section\": \"CE guide § A7 – User Access Control (account removal)\"\n",
        "}\n",
        "\n",
        "RUBRICS[\"A7.4\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Compliant when answer shows BOTH:\",\n",
        "        \"  1️ Statement of **least-privilege / role-based access** principle.\",\n",
        "        \"  2️ Mechanism for granting / adjusting permissions (e.g. AD groups, RBAC, ticket, approval, periodic review).\",\n",
        "        \"Role-change handling & periodic access review give extra confidence.\",\n",
        "        \"None → Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"ref_section\": \"CE guide § A7 – User Access Control (least privilege)\"\n",
        "}\n",
        "\n",
        "RUBRICS[\"A7.5\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Compliant answer needs **both**:\",\n",
        "        \"  1️ *Formal* documented process or workflow (policy, ticket, CAB, written request…).\",\n",
        "        \"  2️ Approval by a senior role (director, manager, CISO, etc.).\",\n",
        "        \"Mention of *separate admin accounts* strengthens compliance.\",\n",
        "        \"Neither → Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"ref_section\": \"CE guide § A7 – Admin-account approval\"\n",
        "}\n",
        "\n",
        "RUBRICS[\"A7.6\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Compliant when answer demonstrates BOTH:\",\n",
        "        \"  1️ **Separation** of admin and standard accounts (or Just-in-Time / PIM).\",\n",
        "        \"  2️ Controls to prevent everyday browsing/email on admin creds:\",\n",
        "        \"     • Technical (UAC, LAPS, Intune policies, Entra PIM, Jamf, ThreatLocker…) or\",\n",
        "        \"     • Procedural (policy forbids daily use, MSP restrictions).\",\n",
        "        \"Neither → Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"ref_section\": \"CE guide § A7 – Admin-account misuse prevention\"\n",
        "}\n",
        "\n",
        "RUBRICS[\"A7.7\"] = {\n",
        "    \"criteria\": [\n",
        "        \"Compliant answer must show BOTH:\",\n",
        "        \"  1️ Clear statement that administrator accounts are **NOT used for web/email/normal work**\",\n",
        "        \"     – e.g. ‘admin accounts only for installs’, ‘no browsing from admin’\",\n",
        "        \"  2️ Enforcement mechanism:\",\n",
        "        \"     • Policy / AUP, OR\",\n",
        "        \"     • Staff training / induction, OR\",\n",
        "        \"     • Technical control (ThreatLocker, PIM, Jamf, LAPS, etc.).\",\n",
        "        \"If no separation at all → Non-Compliant.\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"ref_section\": \"CE guide § A7 – Admin-account everyday-use prevention\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7tgKS1-1T5N"
      },
      "source": [
        "### LlaMa-8B & Mistral-7B Logic Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPY0rIYy-vTw",
        "outputId": "2714fb03-98a2-402d-c37e-eae5435766da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing llama_mistral.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile llama_mistral.py\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "from rubrics import RUBRICS as rubrics\n",
        "\n",
        "FREE_TEXT_IDS = [\n",
        "    \"A4.1.1\", \"A4.10\", \"A4.2.1\", \"A4.6\", \"A5.1\", \"A5.10\", \"A5.6\",\n",
        "    \"A6.4.2\", \"A6.5.2\", \"A6.7\", \"A7.1\", \"A7.10\", \"A7.11\", \"A7.12\",\n",
        "    \"A7.3\", \"A7.4\", \"A7.5\", \"A7.6\", \"A7.7\"\n",
        "]\n",
        "\n",
        "# === LLAMA3 SETUP ===\n",
        "LLAMA_MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "LLAMA_ADAPTER_DIR = \"/content/drive/MyDrive/project/CB/llama3_lora_adapter_FULL\"\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_ID, use_fast=True)\n",
        "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "\n",
        "llama_base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    LLAMA_MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "llama_model = PeftModel.from_pretrained(llama_base_model, LLAMA_ADAPTER_DIR)\n",
        "llama_model.eval()\n",
        "\n",
        "LLAMA_SYSTEM_MSG = (\n",
        "    \"You are a Cyber Essentials assessor.\\n\"\n",
        "    \"For each question, you will be provided:\\n\"\n",
        "    \"1. The question text.\\n\"\n",
        "    \"2. The applicant's answer.\\n\"\n",
        "    \"3. The marking rubric (assessment criteria) for this question.\\n\\n\"\n",
        "    \"Your task:\\n\"\n",
        "    \"- Read the rubric criteria carefully.\\n\"\n",
        "    \"- Judge ONLY according to the rubric, not personal opinion.\\n\"\n",
        "    \"- Output strictly TWO fields on separate lines:\\n\"\n",
        "    \"   LABEL: (Compliant or Non-Compliant)\\n\"\n",
        "    \"   REASON: (Short, precise justification using the rubric wording; say *which* part of the criteria was met/missed)\\n\"\n",
        "    \"Your reply **MUST** be in the exact format:\\n\"\n",
        "    \"LABEL: <Compliant or Non-Compliant>\\n\"\n",
        "    \"REASON: <Your short, criteria-based reason>\\n\"\n",
        "    \"Do NOT add anything else. Do NOT hedge. Just the two lines.\"\n",
        ")\n",
        "\n",
        "def build_llama_prompt(row, rubrics):\n",
        "    criteria = \"\\n\".join(rubrics[row[\"question_id\"]][\"criteria\"])\n",
        "    return (\n",
        "        f\"<|system|>\\n{LLAMA_SYSTEM_MSG}</s>\\n\"\n",
        "        f\"<|user|>\\n\"\n",
        "        f\"Question:\\n{row['question_text']}\\n\\n\"\n",
        "        f\"Applicant answer:\\n{row['answer_text']}\\n\\n\"\n",
        "        f\"Marking rubric for this question:\\n{criteria}\\n</s>\\n\"\n",
        "        f\"<|assistant|>\"\n",
        "    )\n",
        "\n",
        "def parse_llama_output(response):\n",
        "    m = re.search(r\"LABEL:\\s*(Compliant|Non-Compliant)\", response)\n",
        "    label = m.group(1) if m else \"UNKNOWN\"\n",
        "    reason = \"\"\n",
        "    for line in response.splitlines():\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"REASON:\"):\n",
        "            r = line.replace(\"REASON:\", \"\").strip()\n",
        "            if r and not (\n",
        "                r.lower().startswith(\"short, precise justification\")\n",
        "                or r.startswith(\"<Your short\")\n",
        "                or r.lower().startswith(\"(short, precise justification\")\n",
        "                or r.startswith(\"Your short, criteria-based reason\")\n",
        "                or r == \"\"\n",
        "            ):\n",
        "                reason = r\n",
        "                break\n",
        "    # Fallback: next non-empty line after \"REASON:\"\n",
        "    if not reason:\n",
        "        lines = [l.strip() for l in response.splitlines()]\n",
        "        for idx, line in enumerate(lines):\n",
        "            if line.startswith(\"REASON:\"):\n",
        "                for next_line in lines[idx+1:]:\n",
        "                    if next_line and not next_line.lower().startswith(\"short, precise justification\"):\n",
        "                        reason = next_line\n",
        "                        break\n",
        "                break\n",
        "    return label, reason\n",
        "\n",
        "def classify_rubric_llama(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    outputs = []\n",
        "    for _, row in df.iterrows():\n",
        "        if row[\"question_id\"] not in FREE_TEXT_IDS:\n",
        "            continue\n",
        "        prompt = build_llama_prompt(row, rubrics)\n",
        "        inputs = llama_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(llama_model.device)\n",
        "        with torch.no_grad():\n",
        "            out = llama_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=40,\n",
        "                do_sample=False,\n",
        "                pad_token_id=llama_tokenizer.pad_token_id\n",
        "            )\n",
        "        response = llama_tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "        label, reason = parse_llama_output(response)\n",
        "        outputs.append({\n",
        "            \"question_id\": row[\"question_id\"],\n",
        "            \"question_text\": row[\"question_text\"],\n",
        "            \"answer_text\": row[\"answer_text\"],\n",
        "            \"llm_label\": label,\n",
        "            \"llm_reason\": reason,\n",
        "        })\n",
        "    return pd.DataFrame(outputs)\n",
        "\n",
        "# === MISTRAL SETUP ===\n",
        "MISTRAL_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "MISTRAL_ADAPTER_DIR = Path(\"/content/drive/MyDrive/project/CB/mistral7b_lora_final_full\")\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(MISTRAL_MODEL_ID, use_fast=True)\n",
        "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
        "\n",
        "mistral_base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MISTRAL_MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "mistral_model = PeftModel.from_pretrained(mistral_base_model, MISTRAL_ADAPTER_DIR)\n",
        "mistral_model.eval()\n",
        "\n",
        "MISTRAL_SYSTEM_MSG = (\n",
        "    \"You are a Cyber Essentials assessor.\\n\"\n",
        "    \"Classify the applicant answer as Compliant or Non-Compliant, using the rubric below.\\n\"\n",
        "    \"Reply with ONLY TWO lines:\\n\"\n",
        "    \"LABEL: <Compliant or Non-Compliant>\\n\"\n",
        "    \"REASON: <Short reason, quoting the rubric criteria met or missed.>\"\n",
        ")\n",
        "def build_mistral_prompt(row: dict) -> str:\n",
        "    rubric = rubrics.get(row[\"question_id\"])\n",
        "    criteria = \"\\n\".join(rubric[\"criteria\"]) if rubric else \"No rubric found.\"\n",
        "    user_msg = (\n",
        "        f\"Question:\\n{row['question_text']}\\n\\n\"\n",
        "        f\"Applicant answer:\\n{row['answer_text']}\\n\\n\"\n",
        "        f\"Rubric:\\n{criteria}\"\n",
        "    )\n",
        "    return (\n",
        "        \"<s>[INST] <<SYS>>\\n\"\n",
        "        f\"{MISTRAL_SYSTEM_MSG}\\n\"\n",
        "        \"<</SYS>>\\n\"\n",
        "        f\"{user_msg}\\n\"\n",
        "        \"[/INST]\"\n",
        "    )\n",
        "\n",
        "label_re = re.compile(r\"(?i)label\\s*:\\s*(compliant|non[\\s-]?compliant)\")\n",
        "reason_re = re.compile(r\"reason\\s*:\\s*(.+)\", re.IGNORECASE | re.DOTALL)\n",
        "def extract_label(text: str) -> str:\n",
        "    m = label_re.search(text)\n",
        "    if not m: return \"UNKNOWN\"\n",
        "    raw = m.group(1).lower().replace(\" \", \"-\")\n",
        "    return \"Compliant\" if raw.startswith(\"compliant\") else \"Non-Compliant\"\n",
        "def extract_reason(text: str) -> str:\n",
        "    m = reason_re.search(text)\n",
        "    if not m: return \"\"\n",
        "    reason = m.group(1).strip()\n",
        "    reason = reason.split(\"LABEL:\")[0].strip()\n",
        "    return reason\n",
        "\n",
        "def classify_rubric_mistral(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    outputs = []\n",
        "    for _, row in df.iterrows():\n",
        "        if row[\"question_id\"] not in FREE_TEXT_IDS:\n",
        "            continue\n",
        "        prompt = build_mistral_prompt(row)\n",
        "        inputs = mistral_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(mistral_model.device)\n",
        "        with torch.no_grad():\n",
        "            out = mistral_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=48,\n",
        "                do_sample=False,\n",
        "                pad_token_id=mistral_tokenizer.eos_token_id,\n",
        "                eos_token_id=mistral_tokenizer.eos_token_id,\n",
        "            )\n",
        "        text = mistral_tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "        answer = text.split('[/INST]', 1)[-1].strip() if '[/INST]' in text else text\n",
        "        label = extract_label(answer)\n",
        "        reason = extract_reason(answer)\n",
        "        outputs.append({\n",
        "            \"question_id\": row[\"question_id\"],\n",
        "            \"question_text\": row[\"question_text\"],\n",
        "            \"answer_text\": row[\"answer_text\"],\n",
        "            \"mistral_label\": label,\n",
        "            \"mistral_reason\": reason,\n",
        "        })\n",
        "    return pd.DataFrame(outputs)\n",
        "\n",
        "def combine_llama_mistral(df_llama, df_mistral):\n",
        "    # Merge on question_id, question_text, answer_text for alignment\n",
        "    merged = df_llama.merge(\n",
        "        df_mistral,\n",
        "        on=[\"question_id\", \"question_text\", \"answer_text\"],\n",
        "        how=\"outer\"\n",
        "    )\n",
        "    final_labels, final_reasons = [], []\n",
        "    for _, row in merged.iterrows():\n",
        "        # If both models agree\n",
        "        if row[\"llm_label\"] == row[\"mistral_label\"]:\n",
        "            final_labels.append(row[\"llm_label\"])\n",
        "            final_reasons.append(row[\"llm_reason\"])\n",
        "        else:\n",
        "            # If either model says Non-Compliant, pick Non-Compliant & its reason\n",
        "            if row[\"llm_label\"] == \"Non-Compliant\":\n",
        "                final_labels.append(\"Non-Compliant\")\n",
        "                final_reasons.append(row[\"llm_reason\"])\n",
        "            else:\n",
        "                final_labels.append(\"Non-Compliant\")\n",
        "                final_reasons.append(row[\"mistral_reason\"])\n",
        "    merged[\"final_label\"] = final_labels\n",
        "    merged[\"final_reason\"] = final_reasons\n",
        "    return merged\n",
        "\n",
        "\n",
        "from captum.attr import LayerIntegratedGradients\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# ---------- more robust sectioner (works even if markers are missing) ----------\n",
        "def split_prompt_sections(prompt: str, answer_text: str, tokenizer):\n",
        "    \"\"\"\n",
        "    Split the prompt into sections (System/Question, Answer Header, Answer, Rubric).\n",
        "    Works even if some markers are missing, and returns tokens + section labels.\n",
        "    \"\"\"\n",
        "    rubric_markers = [\"Marking rubric for this question:\", \"Rubric:\"]\n",
        "    answer_marker = \"Applicant answer:\"\n",
        "    q_marker = \"Question:\"\n",
        "\n",
        "    p = prompt\n",
        "    lp = p.lower()\n",
        "\n",
        "    # Find ends of \"user\" part (before assistant tag / end of INST)\n",
        "    asst_pos = p.find(\"<|assistant|>\")\n",
        "    inst_end = p.find(\"[/INST]\")\n",
        "    user_end = asst_pos if asst_pos != -1 else (inst_end if inst_end != -1 else len(p))\n",
        "\n",
        "    q_start = lp.find(q_marker.lower())\n",
        "    ans_start = lp.find(answer_marker.lower())\n",
        "    rub_start = -1\n",
        "    rub_used = None\n",
        "    for m in rubric_markers:\n",
        "        pos = lp.find(m.lower())\n",
        "        if pos != -1:\n",
        "            rub_start = pos\n",
        "            rub_used = m\n",
        "            break\n",
        "\n",
        "    # Build string blocks if present\n",
        "    tokens, labels = [], []\n",
        "\n",
        "    def add_block(text, label):\n",
        "        if not text:\n",
        "            return\n",
        "        tks = tokenizer.tokenize(text)\n",
        "        tokens.extend(tks)\n",
        "        labels.extend([label] * len(tks))\n",
        "\n",
        "    # System/Question block: everything from start up to first known header\n",
        "    cut_points = [x for x in [q_start, ans_start, rub_start, user_end] if x != -1]\n",
        "    first_cut = min(cut_points) if cut_points else user_end\n",
        "    add_block(p[:first_cut], \"System/Question\")\n",
        "\n",
        "    # Question (if present)\n",
        "    if q_start != -1:\n",
        "        q_end = min([x for x in [ans_start, rub_start, user_end] if x != -1] or [user_end])\n",
        "        add_block(p[q_start:q_end], \"System/Question\")\n",
        "\n",
        "    # Answer header + (external) answer text (if present)\n",
        "    if ans_start != -1:\n",
        "        add_block(p[ans_start:ans_start + len(answer_marker)], \"Answer Header\")\n",
        "        add_block(answer_text, \"Answer\")\n",
        "\n",
        "    # Rubric (if present)\n",
        "    if rub_start != -1:\n",
        "        rub_end = user_end\n",
        "        add_block(p[rub_start:rub_end], \"Rubric\")\n",
        "\n",
        "    # Tail after user (assistant tags, etc.)\n",
        "    add_block(p[user_end:], \"Other\")\n",
        "\n",
        "    return tokens, labels\n",
        "\n",
        "\n",
        "# ---------- IG runner with focus trimming + true-label option ----------\n",
        "\n",
        "def run_xai_ig(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt: str,\n",
        "    answer_text: str,\n",
        "    device,\n",
        "    n_steps: int = 4, #change to 8,16,32,64 if computational costs allow\n",
        "    max_len: int = 512,\n",
        "    target_label: str | None = None  # \"Compliant\", \"Non-Compliant\", \"pred\", or None for margin\n",
        "):\n",
        "    \"\"\"\n",
        "    Integrated Gradients over the embedding layer at the 'LABEL: ' next-token.\n",
        "    If target_label is:\n",
        "      - \"Compliant\" / \"Non-Compliant\": explain that class logit\n",
        "      - \"pred\": explain whichever of the two is higher at the actual input\n",
        "      - None: use margin logit(Compliant) - logit(Non)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    prompt_for_ig = prompt + \"LABEL: \"\n",
        "    enc = tokenizer(\n",
        "        prompt_for_ig,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "    )\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    attention_mask = enc[\"attention_mask\"].to(device)\n",
        "\n",
        "    def first_token_id(s: str) -> int:\n",
        "        ids = tokenizer.encode(s, add_special_tokens=False)\n",
        "        if not ids:\n",
        "            ids = tokenizer.encode(\" \" + s, add_special_tokens=False)\n",
        "        return ids[0]\n",
        "\n",
        "    tok_C = first_token_id(\"Compliant\")\n",
        "    tok_N = first_token_id(\"Non\")  # first token of \"Non-Compliant\"\n",
        "\n",
        "    # Decide scalar to explain\n",
        "    chosen_id = None\n",
        "    if target_label is not None and isinstance(target_label, str):\n",
        "        t = target_label.strip().lower()\n",
        "        if t.startswith(\"comp\"):\n",
        "            chosen_id = tok_C\n",
        "        elif t.startswith(\"non\"):\n",
        "            chosen_id = tok_N\n",
        "        elif t == \"pred\":\n",
        "            with torch.no_grad():\n",
        "                out0 = model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False)\n",
        "                nxt0 = out0.logits[:, -1, :]\n",
        "                chosen_id = tok_C if (nxt0[:, tok_C] >= nxt0[:, tok_N]).item() else tok_N\n",
        "\n",
        "    def fwd_margin(input_ids, attention_mask):\n",
        "        out = model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False)\n",
        "        nxt = out.logits[:, -1, :]\n",
        "        return nxt[:, tok_C] - nxt[:, tok_N]\n",
        "\n",
        "    def fwd_class(input_ids, attention_mask, class_id: int):\n",
        "        out = model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False)\n",
        "        nxt = out.logits[:, -1, :]\n",
        "        return nxt[:, class_id]\n",
        "\n",
        "    emb_layer = model.get_input_embeddings()\n",
        "    lig = LayerIntegratedGradients(\n",
        "        (lambda ids, mask: fwd_class(ids, mask, chosen_id)) if chosen_id is not None else fwd_margin,\n",
        "        emb_layer\n",
        "    )\n",
        "\n",
        "    atts, _ = lig.attribute(\n",
        "        inputs=input_ids,\n",
        "        additional_forward_args=(attention_mask,),\n",
        "        n_steps=n_steps,\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "\n",
        "    token_attr = atts.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0).tolist())\n",
        "\n",
        "    # Sections (best-effort)\n",
        "    sec_tokens, sec_labels = split_prompt_sections(prompt, answer_text, tokenizer)\n",
        "    if len(tokens) != len(sec_labels):\n",
        "        sec_labels = (sec_labels + [\"Other\"] * len(tokens))[:len(tokens)]\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"Token\": tokens,\n",
        "        \"Attribution\": token_attr,\n",
        "        \"Abs Attribution\": np.abs(token_attr),\n",
        "        \"Section\": sec_labels\n",
        "    })\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# ----------- Section color helper for Streamlit display -----------\n",
        "SECTION_COLORS = {\n",
        "    \"System/Question\": \"#ADD8E6\",  # Light blue\n",
        "    \"Answer Header\":   \"#FFD700\",  # Gold\n",
        "    \"Answer\":          \"#90EE90\",  # Light green\n",
        "    \"Rubric\":          \"#FFB6C1\",  # Light pink\n",
        "    \"Other\":           \"#E0E0E0\",  # Grey\n",
        "}\n",
        "\n",
        "def color_sections(val: str) -> str:\n",
        "    return f\"background-color: {SECTION_COLORS.get(val, '#F0F0F0')}; font-weight: 500;\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MPGd7xfm1qw"
      },
      "source": [
        "### **STREAMLIT APP: LAUNCH AND USER INTERFACE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFXvkCWg_yw9"
      },
      "source": [
        "### Logo upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "DcVdh8c__xxh",
        "outputId": "171cb078-fa6f-417b-c2f1-45dab03bf679"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eaef6d31-4c9c-4fa7-9bac-3090a3a49809\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-eaef6d31-4c9c-4fa7-9bac-3090a3a49809\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving logo-appCE.svg to logo-appCE.svg\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'logo-appCE.svg': b'<svg width=\"120\" height=\"120\" viewBox=\"0 0 120 120\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\\n  <rect width=\"120\" height=\"120\" rx=\"28\" fill=\"#25354B\"></rect>\\n  <!-- Document Icon -->\\n  <rect x=\"16\" y=\"30\" width=\"32\" height=\"48\" rx=\"6\" fill=\"#F2F6F9\"></rect>\\n  <rect x=\"22\" y=\"38\" width=\"18\" height=\"6\" rx=\"3\" fill=\"#B2FFDA\"></rect>\\n  <rect x=\"22\" y=\"50\" width=\"18\" height=\"6\" rx=\"3\" fill=\"#B2FFDA\"></rect>\\n  <!-- Classifier (tick) in circle -->\\n  <circle cx=\"58\" cy=\"54\" r=\"14\" fill=\"#27AE60\"></circle>\\n  <polyline points=\"52,56 58,62 66,48\" fill=\"none\" stroke=\"#fff\" stroke-width=\"3\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></polyline>\\n  <!-- Person Icon (HITL) -->\\n  <circle cx=\"92\" cy=\"56\" r=\"10\" fill=\"#FFD700\" stroke=\"#E1A800\" stroke-width=\"2\"></circle>\\n  <ellipse cx=\"92\" cy=\"73\" rx=\"13\" ry=\"8\" fill=\"#FFD700\" stroke=\"#E1A800\" stroke-width=\"2\"></ellipse>\\n</svg>\\n'}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()   # This pops up a window; select your SVG/logo file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjpeNj2Q21pF"
      },
      "source": [
        "### Streamlit code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKYJgVajDyVP"
      },
      "outputs": [],
      "source": [
        "code = '''\n",
        "import streamlit as st\n",
        "import yaml, os, time, re, hashlib, json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import pdfplumber\n",
        "import fitz\n",
        "from io import BytesIO\n",
        "import datetime\n",
        "from datetime import datetime, timezone\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ================== SECURITY CONSTANTS & HELPERS ==================\n",
        "MAX_PDF_MB = 10\n",
        "SUSPICIOUS_KEYS = {b\"/JavaScript\", b\"/JS\", b\"/AA\", b\"/OpenAction\", b\"/Launch\",\n",
        "                   b\"/EmbeddedFile\", b\"/RichMedia\", b\"/AcroForm\"}\n",
        "\n",
        "def bytes_startswith_pdf(header: bytes) -> bool:\n",
        "    return header.startswith(b\"%PDF-\")\n",
        "\n",
        "def scan_pdf_bytes(raw: bytes) -> dict:\n",
        "    # Lightweight heuristic scan for active content indicators (NOT AV)\n",
        "    found = {k.decode(\"latin-1\"): (k in raw) for k in SUSPICIOUS_KEYS}\n",
        "    return found\n",
        "\n",
        "def validate_uploaded_pdf(uploaded_file) -> str:\n",
        "    # NOTE: We only hard-fail on size/MIME/header. Suspicious keys become a warning, not a block.\n",
        "    size_mb = (uploaded_file.size or 0) / (1024 * 1024)\n",
        "    if size_mb > MAX_PDF_MB:\n",
        "        return f\"File too large: {size_mb:.1f} MB > {MAX_PDF_MB} MB.\"\n",
        "\n",
        "    mime = getattr(uploaded_file, \"type\", \"\")\n",
        "    if mime and \"pdf\" not in mime.lower():\n",
        "        return f\"Unexpected MIME type: {mime}.\"\n",
        "\n",
        "    raw = uploaded_file.getvalue()\n",
        "    if not bytes_startswith_pdf(raw[:8]):\n",
        "        return \"File does not look like a PDF (missing %PDF- header).\"\n",
        "\n",
        "    return \"\"  # OK\n",
        "\n",
        "# ================== MINIMAL AUDIT LOG (IN-MEMORY) =================\n",
        "if \"audit\" not in st.session_state:\n",
        "    st.session_state[\"audit\"] = []\n",
        "\n",
        "def log_event(event_type: str, **details):\n",
        "    st.session_state[\"audit\"].append({\n",
        "        \"ts\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"event\": event_type,\n",
        "        **details\n",
        "    })\n",
        "\n",
        "def sha256_bytes(b: bytes) -> str:\n",
        "    return hashlib.sha256(b).hexdigest()\n",
        "\n",
        "# ================== MANUAL AUTH SECTION ===========================\n",
        "def login_screen():\n",
        "    st.image(\"logo-appCE.svg\", width=96)\n",
        "    st.title(\"CheckLoop\")\n",
        "    st.caption(\"AI Classifies, You Decide\")\n",
        "    st.text_input(\"Username\", key=\"username\")\n",
        "    st.text_input(\"Password\", type=\"password\", key=\"password\")\n",
        "    if st.button(\"Login\"):\n",
        "        if st.session_state.get(\"username\") == \"Assessor\" and st.session_state.get(\"password\") == \"Assessor-Drift!93_Tk7p\":\n",
        "            st.session_state[\"password_correct\"] = True\n",
        "            st.rerun()\n",
        "        else:\n",
        "            st.error(\"Invalid username or password.\")\n",
        "\n",
        "if \"password_correct\" not in st.session_state or not st.session_state[\"password_correct\"]:\n",
        "    login_screen()\n",
        "    st.stop()\n",
        "\n",
        "# ---- Main app starts only after successful login ----\n",
        "st.success(f\"👋 Welcome, **Assessor**!\")\n",
        "log_event(\"login_success\", user=st.session_state.get(\"username\", \"Assessor\"))\n",
        "\n",
        "with st.sidebar:\n",
        "    st.subheader(\"Session\")\n",
        "    st.write(f\"User: **Assessor**\")\n",
        "    if st.button(\"Logout\"):\n",
        "        log_event(\"logout_clicked\", user=st.session_state.get(\"username\", \"Assessor\"))\n",
        "        st.session_state.clear()\n",
        "        st.rerun()\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"FAQ / Help\")\n",
        "    st.info(\"\"\"\n",
        "        - **What happens to my upload?**\n",
        "        Your uploaded PDF is processed in memory and never saved server-side.\n",
        "        - **How do I export results?**\n",
        "        Use the export button after manual review for a full log.\n",
        "        - **How are labels decided?**\n",
        "        AI makes suggestions, but you can override any answer.\n",
        "    \"\"\")\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"Privacy / Terms\")\n",
        "    st.caption(\"\"\"\n",
        "        This app does **not** permanently store any uploaded data.\n",
        "        All data is cleared when your session ends. For further info, contact your administrator.\n",
        "    \"\"\")\n",
        "\n",
        "# ================== CONSENT GATE (BEFORE UPLOAD) ==================\n",
        "if \"consent_ok\" not in st.session_state:\n",
        "    st.session_state[\"consent_ok\"] = False\n",
        "\n",
        "if not st.session_state[\"consent_ok\"]:\n",
        "    with st.expander(\"Data Privacy & Consent\", expanded=True):\n",
        "        st.write(\"\"\"\n",
        "        • Your PDF is processed in-memory within this session (no server persistence).\n",
        "        • Outputs (CSV/PDF) are created only if you download them locally.\n",
        "        • Upload only data you are authorised to process.\n",
        "        \"\"\")\n",
        "        if st.checkbox(\"I confirm I have authority to upload this file and consent to in-session processing.\", key=\"consent_checkbox\"):\n",
        "            st.session_state[\"consent_ok\"] = True\n",
        "            st.rerun()\n",
        "    # Do not show uploader until consent given\n",
        "    st.stop()\n",
        "else:\n",
        "    st.caption(\"✅ Consent confirmed for this session.\")\n",
        "\n",
        "# ================== UPLOAD (AFTER CONSENT) ========================\n",
        "uploaded_file = st.file_uploader(\"Choose a Cyber Essentials PDF\", type=\"pdf\")\n",
        "extract_mode = st.selectbox(\"Extraction method\", [\"Table\", \"Raw Text\"])\n",
        "\n",
        "# Prepare df placeholder so later sections don't error if no upload yet\n",
        "df = pd.DataFrame()\n",
        "\n",
        "if uploaded_file:\n",
        "    # Validation (hard-fail only on basic checks)\n",
        "    msg = validate_uploaded_pdf(uploaded_file)\n",
        "    if msg:\n",
        "        st.error(msg)\n",
        "        st.stop()\n",
        "\n",
        "    # Soft warning for active-content indicators (NOT blocking)\n",
        "    raw = uploaded_file.getvalue()\n",
        "    indicators = scan_pdf_bytes(raw)\n",
        "    risky = [k for k, present in indicators.items() if present]\n",
        "    if risky:\n",
        "        st.warning(\"Heads up: PDF contains auto-action indicators (\" + \", \".join(risky) + \"). Proceeding anyway.\")\n",
        "        log_event(\"pdf_warning\", indicators=\",\".join(risky))\n",
        "\n",
        "    # Audit: file accepted\n",
        "    log_event(\"file_uploaded\",\n",
        "              filename=uploaded_file.name,\n",
        "              size=uploaded_file.size,\n",
        "              sha256=sha256_bytes(uploaded_file.getvalue()))\n",
        "\n",
        "# ================== PIPELINE IMPORTS ==============================\n",
        "from yesnorules import classify_compliance_with_reason, RULE_IDS\n",
        "from semantic_rules import extract_semantic_frame, question_rules, get_compliance_label_and_reason\n",
        "from llama_mistral import (\n",
        "    classify_rubric_llama,\n",
        "    classify_rubric_mistral,\n",
        "    combine_llama_mistral,\n",
        "    FREE_TEXT_IDS,\n",
        "    build_llama_prompt,\n",
        "    build_mistral_prompt,\n",
        "    run_xai_ig,\n",
        "    llama_model, llama_tokenizer, mistral_model, mistral_tokenizer\n",
        " )\n",
        "from rubrics import RUBRICS as rubrics\n",
        "\n",
        "# ================== PDF PARSING HELPERS ===========================\n",
        "def parse_pdf_table(file: BytesIO) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    with pdfplumber.open(file) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            for table in page.extract_tables() or []:\n",
        "                if not table or len(table) < 2:\n",
        "                    continue\n",
        "                for q, a, *_ in table[1:]:\n",
        "                    q, a = (q or \"\").strip(), (a or \"\").strip()\n",
        "                    if not q and not a:\n",
        "                        continue\n",
        "                    m = re.match(r\"^(A\\\\d+(?:\\\\.\\\\d+)+)\", q)\n",
        "                    rows.append({\n",
        "                        \"question_id\": m.group(1) if m else None,\n",
        "                        \"question_text\": q,\n",
        "                        \"answer_text\": a,\n",
        "                    })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "BLOCK_RE = re.compile(r\"(A\\\\d+\\\\.\\\\d+[A-Za-z0-9\\\\-]*.*?)\\\\n(.*?)(?=\\\\nA\\\\d+\\\\.\\\\d+|\\\\Z)\", re.DOTALL)\n",
        "NOTE_RE = re.compile(r\"^\\\\s*(Notes:|Applicant Notes:)\", re.I)\n",
        "\n",
        "def extract_text(file: BytesIO) -> str:\n",
        "    with fitz.open(stream=file.read(), filetype=\"pdf\") as doc:\n",
        "        return \"\\\\n\".join(pg.get_text() for pg in doc)\n",
        "\n",
        "def parse_block(qid: str, blob: str) -> dict:\n",
        "    lines = [l.strip() for l in blob.strip().splitlines()]\n",
        "    try:\n",
        "        blank = lines.index(\"\")\n",
        "    except ValueError:\n",
        "        blank = len(lines) - 1\n",
        "    q_text = \" \".join(lines[: blank + 1]).strip()\n",
        "    ans_raw = \" \".join(lines[blank + 1 :]).strip() or lines[-1].strip()\n",
        "    notes, parts = [], []\n",
        "    for seg in ans_raw.split(\"│\") if \"│\" in ans_raw else [ans_raw]:\n",
        "        if NOTE_RE.match(seg):\n",
        "            notes.append(seg.split(\":\", 1)[-1].strip())\n",
        "        else:\n",
        "            parts.append(seg.strip())\n",
        "    answer = \" \".join(parts)\n",
        "    if notes:\n",
        "        answer = (f\"{answer} │ Notes: {' | '.join(notes)}\" if answer else f\"Notes: {' | '.join(notes)}\")\n",
        "    return {\"question_id\": qid.strip(), \"question_text\": q_text, \"answer_text\": answer}\n",
        "\n",
        "def parse_pdf_text(file: BytesIO) -> pd.DataFrame:\n",
        "    file.seek(0)\n",
        "    text = extract_text(file)\n",
        "    rows = []\n",
        "    for qid, block in BLOCK_RE.findall(text):\n",
        "        rec = parse_block(qid, block)\n",
        "        rows.append(rec)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def hash_df_for_cache(df):\n",
        "    subset = df[[\"question_id\", \"question_text\", \"answer_text\"]].copy()\n",
        "    for col in subset.columns:\n",
        "        subset[col] = subset[col].astype(str)\n",
        "    return hash(pd.util.hash_pandas_object(subset, index=True).sum())\n",
        "\n",
        "# ================== OVERRULES ====================================\n",
        "def apply_overrules(df):\n",
        "    if any(df.loc[df['question_id'].isin(['A6.2.1','A6.2.2','A6.2.3','A6.2.4']), 'best_label'] == \"Non-Compliant\"):\n",
        "        for qid in ['A6.2', 'A6.6']:\n",
        "            idx = df['question_id'] == qid\n",
        "            df.loc[idx, 'best_label'] = \"Non-Compliant\"\n",
        "            df.loc[idx, 'best_reason'] = \"Overruled: Related software/app non-compliance in A6.2.x\"\n",
        "    if any(df.loc[df['question_id'].isin(['A4.5.1','A4.6']), 'best_label'] == \"Non-Compliant\"):\n",
        "        idx = df['question_id'] == 'A4.5'\n",
        "        df.loc[idx, 'best_label'] = \"Non-Compliant\"\n",
        "        df.loc[idx, 'best_reason'] = \"Overruled: Related non-compliance in A4.5.1 or A4.6\"\n",
        "    try:\n",
        "        a714_value = df.loc[df['question_id'] == 'A7.14', 'answer_text'].iloc[0]\n",
        "        if str(a714_value).strip().lower() in ['no', 'n']:\n",
        "            for qid in ['A7.16','A7.17']:\n",
        "                idx = df['question_id'] == qid\n",
        "                df.loc[idx, 'best_label'] = \"Non-Compliant\"\n",
        "                df.loc[idx, 'best_reason'] = \"Overruled: A7.14 answered 'No'\"\n",
        "    except IndexError:\n",
        "        pass\n",
        "    for i, row in df.iterrows():\n",
        "        ans = str(row['answer_text']).strip().lower()\n",
        "        if ans == \"\" or ans in [\"not applicable\", \"n/a\"]:\n",
        "            df.at[i, 'best_label'] = \"Fail\"\n",
        "            df.at[i, 'best_reason'] = \"Overruled: Blank or not applicable answer triggers fail\"\n",
        "    return df\n",
        "\n",
        "# ================== WHOLE FORM ASSESSMENT ========================\n",
        "def assess_form(df):\n",
        "    labels = df['override_label'].replace('', pd.NA).fillna(df['best_label'])\n",
        "    if \"Fail\" in labels.values:\n",
        "        return \"Fail\"\n",
        "    elif any(l in labels.values for l in [\"Non-Compliant\", \"More Information Required\"]):\n",
        "        return \"Non-Compliant\"\n",
        "    elif all(l == \"Compliant\" for l in labels.values):\n",
        "        return \"Compliant\"\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "# ================== MAIN EXTRACTION & CLASSIFICATION =============\n",
        "if uploaded_file:\n",
        "    with st.spinner(\"Extracting...\"):\n",
        "        if extract_mode == \"Table\":\n",
        "            df = parse_pdf_table(uploaded_file)\n",
        "        else:\n",
        "            df = parse_pdf_text(uploaded_file)\n",
        "\n",
        "    if not df.empty:\n",
        "        st.success(f\"Extracted {len(df)} Q&A pairs!\")\n",
        "        st.dataframe(df, use_container_width=True)\n",
        "        ALL_CLASSIFIER_IDS = set(FREE_TEXT_IDS) | set(question_rules.keys()) | set(RULE_IDS)\n",
        "        df = df[df[\"question_id\"].isin(ALL_CLASSIFIER_IDS)].copy()\n",
        "\n",
        "        file_hash = hash_df_for_cache(df)\n",
        "        cache_key = f\"ai_results_{file_hash}\"\n",
        "        ai_time_key = f\"ai_time_{file_hash}\"\n",
        "\n",
        "        # --------- AI/LLM only runs if cache missing ----------\n",
        "        if cache_key not in st.session_state:\n",
        "            ai_start = time.perf_counter()\n",
        "            log_event(\"ai_start\")\n",
        "\n",
        "            # LLM Classification, Frame, Rule logic\n",
        "            df_llama = df[df[\"question_id\"].isin(FREE_TEXT_IDS)].copy()\n",
        "            if not df_llama.empty:\n",
        "                df_llama_results = classify_rubric_llama(df_llama)\n",
        "                df_mistral_results = classify_rubric_mistral(df_llama)\n",
        "                df_llm = combine_llama_mistral(df_llama_results, df_mistral_results)\n",
        "                df_llm.set_index(\"question_id\", inplace=True)\n",
        "            else:\n",
        "                df_llm = pd.DataFrame()\n",
        "            st.session_state[\"df_llm\"] = df_llm\n",
        "\n",
        "            # Semantic Frames\n",
        "            semantic_label, semantic_reason = [], []\n",
        "            for idx, row in df.iterrows():\n",
        "                qid = row[\"question_id\"]\n",
        "                if qid in question_rules:\n",
        "                    frame = extract_semantic_frame(qid, row[\"answer_text\"])\n",
        "                    label, reason = get_compliance_label_and_reason(frame, row[\"answer_text\"])\n",
        "                    semantic_label.append(label)\n",
        "                    semantic_reason.append(reason)\n",
        "                else:\n",
        "                    semantic_label.append(\"\")\n",
        "                    semantic_reason.append(\"\")\n",
        "            df[\"semantic_label\"] = semantic_label\n",
        "            df[\"semantic_reason\"] = semantic_reason\n",
        "\n",
        "            # Rule-based\n",
        "            rule_label, rule_reason = [], []\n",
        "            for idx, row in df.iterrows():\n",
        "                qid = row[\"question_id\"]\n",
        "                if qid in RULE_IDS:\n",
        "                    label, reason = classify_compliance_with_reason(row)\n",
        "                    rule_label.append(label)\n",
        "                    rule_reason.append(reason)\n",
        "                else:\n",
        "                    rule_label.append(\"\")\n",
        "                    rule_reason.append(\"\")\n",
        "            df[\"rule_label\"] = rule_label\n",
        "            df[\"rule_reason\"] = rule_reason\n",
        "\n",
        "            # Pick best label\n",
        "            def pick_best(row):\n",
        "                qid = row[\"question_id\"]\n",
        "                df_llm = st.session_state.get(\"df_llm\", pd.DataFrame())\n",
        "                if qid in FREE_TEXT_IDS and not df_llm.empty and qid in df_llm.index:\n",
        "                    label = df_llm.at[qid, \"final_label\"]\n",
        "                    reason = df_llm.at[qid, \"final_reason\"]\n",
        "                    if label not in [\"\", None, \"Unknown\"]:\n",
        "                        return pd.Series({\"best_label\": label, \"best_reason\": reason})\n",
        "                if qid in question_rules:\n",
        "                    label = row.get(\"semantic_label\", \"Unknown\")\n",
        "                    reason = row.get(\"semantic_reason\", \"No semantic result\")\n",
        "                    if label not in [\"\", None, \"Unknown\"]:\n",
        "                        return pd.Series({\"best_label\": label, \"best_reason\": reason})\n",
        "                if qid in RULE_IDS:\n",
        "                    label = row.get(\"rule_label\", \"Unknown\")\n",
        "                    reason = row.get(\"rule_reason\", \"No rules result\")\n",
        "                    if label not in [\"\", None, \"Unknown\"]:\n",
        "                        return pd.Series({\"best_label\": label, \"best_reason\": reason})\n",
        "                return pd.Series({\"best_label\": \"Unknown\", \"best_reason\": \"No matching classifier\"})\n",
        "            df[[\"best_label\", \"best_reason\"]] = df.apply(pick_best, axis=1)\n",
        "\n",
        "            df[\"override_label\"] = \"\"\n",
        "            df[\"override_reason\"] = \"\"\n",
        "            df = apply_overrules(df)\n",
        "\n",
        "            st.session_state[cache_key] = df.copy()\n",
        "            ai_end = time.perf_counter()\n",
        "            ai_time = ai_end - ai_start\n",
        "            st.session_state[ai_time_key] = ai_time\n",
        "            log_event(\"ai_end\", seconds=round(ai_time, 3))\n",
        "        else:\n",
        "            df = st.session_state[cache_key].copy()\n",
        "            ai_time = st.session_state.get(ai_time_key, 0)\n",
        "\n",
        "        # ------- Always show timing (from cache, doesn't update after overrides) -----\n",
        "        mins, secs = divmod(ai_time, 60)\n",
        "        st.info(f\"AI classification and scoring completed in **{int(mins)} min {int(secs)} sec** (before human override).\")\n",
        "\n",
        "        # ============= MANUAL OVERRIDE UI SECTION =============\n",
        "        st.subheader(\"Manual Override: Human-in-the-Loop (HITL) Corrections\")\n",
        "        editable_cols = [\n",
        "            \"question_id\", \"question_text\", \"answer_text\",\n",
        "            \"best_label\", \"best_reason\", \"override_label\", \"override_reason\"\n",
        "        ]\n",
        "        edited_df = st.data_editor(\n",
        "            df[editable_cols],\n",
        "            num_rows=\"dynamic\",\n",
        "            hide_index=True,\n",
        "            use_container_width=True,\n",
        "            key=\"hitl_editor\"\n",
        "        )\n",
        "\n",
        "        # ============= OVERRIDDEN LOG EXPORTS =============\n",
        "        overridden = edited_df[edited_df[\"override_label\"].str.strip() != \"\"].copy()\n",
        "        if not overridden.empty:\n",
        "            st.markdown(\"#### Download Override Log (CSV/PDF)\")\n",
        "            overridden[\"prev_label\"] = overridden[\"best_label\"]\n",
        "            overridden[\"prev_reason\"] = overridden[\"best_reason\"]\n",
        "            st.dataframe(\n",
        "                overridden[\n",
        "                    [\n",
        "                        \"question_id\", \"question_text\", \"answer_text\",\n",
        "                        \"prev_label\", \"prev_reason\", \"override_label\", \"override_reason\"\n",
        "                    ]\n",
        "                ],\n",
        "                use_container_width=True\n",
        "            )\n",
        "            # Download CSV\n",
        "            csv_bytes = overridden.to_csv(index=False)\n",
        "            st.download_button(\n",
        "                label=\"Download Override Log (CSV)\",\n",
        "                data=csv_bytes,\n",
        "                file_name=\"manual_override_log.csv\",\n",
        "                mime=\"text/csv\"\n",
        "            )\n",
        "            log_event(\"export_prepared\", kind=\"override_log_csv\", rows=int(len(overridden)))\n",
        "\n",
        "            # Download PDF of only overrides\n",
        "            overridden[\"Previous Label\"] = overridden[\"best_label\"]\n",
        "            overridden[\"Previous Reason\"] = overridden[\"best_reason\"]\n",
        "            html_override = overridden[[\n",
        "                \"question_id\", \"question_text\", \"answer_text\",\n",
        "                \"Previous Label\", \"Previous Reason\",\n",
        "                \"override_label\", \"override_reason\"\n",
        "            ]].rename(columns={\n",
        "               \"question_id\": \"Question ID\",\n",
        "               \"question_text\": \"Question\",\n",
        "               \"answer_text\": \"Answer\",\n",
        "               \"override_label\": \"Override Label\",\n",
        "               \"override_reason\": \"Override Reason\"\n",
        "            }).to_html(index=False, justify=\"center\")\n",
        "\n",
        "            try:\n",
        "                import pdfkit\n",
        "                config = pdfkit.configuration(wkhtmltopdf='/usr/bin/wkhtmltopdf')\n",
        "                pdf_bytes = pdfkit.from_string(html_override, False, configuration=config)\n",
        "                st.download_button(\n",
        "                    label=\"Download Override Log (PDF)\",\n",
        "                    data=pdf_bytes,\n",
        "                    file_name=\"manual_override_log.pdf\",\n",
        "                    mime=\"application/pdf\"\n",
        "                )\n",
        "                log_event(\"export_prepared\", kind=\"override_log_pdf\", rows=int(len(overridden)))\n",
        "            except Exception as e:\n",
        "                st.info(f\"PDF override log export not available: {e}\")\n",
        "        else:\n",
        "            st.info(\"No manual overrides yet. Edit a row to override and download.\")\n",
        "\n",
        "# ============= XAI INPUT OPTION =============\n",
        "if not df.empty and any(qid in FREE_TEXT_IDS for qid in df[\"question_id\"]):\n",
        "    with st.expander(\"🔍 Advanced: XAI Token-Level Attribution Table (Llama vs Mistral)\", expanded=False):\n",
        "        st.subheader(\"🔍 XAI: Token-Level Attribution Table (Llama vs Mistral)\")\n",
        "        st.info(\n",
        "            \"This section displays explainability (XAI) tables **only for answers automatically judged by the AI models (Llama/Mistral)**. \"\n",
        "            \"Explore which tokens most influenced the AI’s compliance decision. \"\n",
        "            \"Use the filter and sort options to focus on important answer sections.\"\n",
        "        )\n",
        "\n",
        "        # --- Load classification results from session ---\n",
        "        df_llm = st.session_state.get(\"df_llm\", pd.DataFrame())\n",
        "        xai_qids = [qid for qid in df[\"question_id\"] if qid in FREE_TEXT_IDS]\n",
        "\n",
        "        xai_qid = st.selectbox(\"Select a Question ID for XAI visualization\", xai_qids, key=\"xai_qid\")\n",
        "        xai_row = df[df[\"question_id\"] == xai_qid].iloc[0]\n",
        "        st.markdown(f\"**Question:** {xai_row['question_text']}\")\n",
        "        st.markdown(f\"**Applicant Answer:** {xai_row['answer_text']}\")\n",
        "\n",
        "        # --- Model Decision Summary ---\n",
        "        llama_label = (\n",
        "            df_llm.loc[xai_qid, \"llm_label\"]\n",
        "            if not df_llm.empty and \"llm_label\" in df_llm.columns and xai_qid in df_llm.index\n",
        "            else None\n",
        "        )\n",
        "        mistral_label = (\n",
        "            df_llm.loc[xai_qid, \"mistral_label\"]\n",
        "            if not df_llm.empty and \"mistral_label\" in df_llm.columns and xai_qid in df_llm.index\n",
        "            else None\n",
        "        )\n",
        "        llama_reason = (\n",
        "            df_llm.loc[xai_qid, \"llm_reason\"]\n",
        "            if not df_llm.empty and \"llm_reason\" in df_llm.columns and xai_qid in df_llm.index\n",
        "            else \"\"\n",
        "        )\n",
        "        mistral_reason = (\n",
        "            df_llm.loc[xai_qid, \"mistral_reason\"]\n",
        "            if not df_llm.empty and \"mistral_reason\" in df_llm.columns and xai_qid in df_llm.index\n",
        "            else \"\"\n",
        "        )\n",
        "        with st.container():\n",
        "            st.markdown(\"#### 🧾 Model Decision Summary\")\n",
        "            st.markdown(\n",
        "                f\"\"\"<div style='margin-bottom:0.7em'>\n",
        "                <b>Llama-3:</b> <span style=\"color:#007acc;\">{llama_label or \"N/A\"}</span>\n",
        "                <br><i>{llama_reason or \"No reason available.\"}</i>\n",
        "                <hr style=\"margin:0.2em 0;\">\n",
        "                <b>Mistral-7B:</b> <span style=\"color:#eb5247;\">{mistral_label or \"N/A\"}</span>\n",
        "                <br><i>{mistral_reason or \"No reason available.\"}</i>\n",
        "                </div>\"\"\",\n",
        "                unsafe_allow_html=True\n",
        "            )\n",
        "\n",
        "        # --- XAI run logic ---\n",
        "        session_key_llama = f\"llama_xai_{xai_qid}\"\n",
        "        session_key_mistral = f\"mistral_xai_{xai_qid}\"\n",
        "\n",
        "        if st.button(\"Run XAI on this answer\", key=\"run_xai\"):\n",
        "            if llama_label is None or mistral_label is None:\n",
        "                st.warning(\"Please run Llama/Mistral classification first.\")\n",
        "            else:\n",
        "                llama_prompt = build_llama_prompt(xai_row, rubrics)\n",
        "                mistral_prompt = build_mistral_prompt(xai_row)\n",
        "                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "                # IMPORTANT: Force margin view → positive = Compliant, negative = Non-Compliant\n",
        "                with st.spinner(\"Running IG for Llama...\"):\n",
        "                    llama_df = run_xai_ig(\n",
        "                        model=llama_model,\n",
        "                        tokenizer=llama_tokenizer,\n",
        "                        prompt=llama_prompt,\n",
        "                        answer_text=xai_row[\"answer_text\"],\n",
        "                        device=device,\n",
        "                        target_label=None   # << margin: logit(C) - logit(N)\n",
        "                    )\n",
        "\n",
        "                with st.spinner(\"Running IG for Mistral...\"):\n",
        "                    mistral_df = run_xai_ig(\n",
        "                        model=mistral_model,\n",
        "                        tokenizer=mistral_tokenizer,\n",
        "                        prompt=mistral_prompt,\n",
        "                        answer_text=xai_row[\"answer_text\"],\n",
        "                        device=device,\n",
        "                        target_label=None   # << margin: logit(C) - logit(N)\n",
        "                    )\n",
        "\n",
        "                # Save results for this QID\n",
        "                st.session_state[session_key_llama] = llama_df\n",
        "                st.session_state[session_key_mistral] = mistral_df\n",
        "\n",
        "                # --- Retrieve and display XAI attribution tables ---\n",
        "                llama_df = st.session_state.get(session_key_llama)\n",
        "                mistral_df = st.session_state.get(session_key_mistral)\n",
        "\n",
        "                if llama_df is not None and mistral_df is not None:\n",
        "                    # Show unique sections for user help\n",
        "                    for tag, frame in [(\"Llama\", llama_df), (\"Mistral\", mistral_df)]:\n",
        "                        st.info(f\"{tag} unique token sections: {sorted(frame['Section'].unique())}\")\n",
        "\n",
        "                    # Section filter option\n",
        "                    all_sections = sorted(set(llama_df[\"Section\"].unique()) | set(mistral_df[\"Section\"].unique()))\n",
        "                    section_options = [\"All\"] + all_sections\n",
        "                    default_idx = section_options.index(\"Answer\") if \"Answer\" in section_options else 0\n",
        "                    selected_section = st.selectbox(\n",
        "                        \"Filter by Section\",\n",
        "                        section_options,\n",
        "                        index=default_idx,\n",
        "                        key=\"section_filter\"\n",
        "                    )\n",
        "\n",
        "                    def filter_section(df, section):\n",
        "                        return df if section == \"All\" else df[df[\"Section\"] == section]\n",
        "\n",
        "                    section_colors = {\n",
        "                        \"System/Question\": \"#BBDEFB\",\n",
        "                        \"Answer Header\": \"#FFE082\",\n",
        "                        \"Answer\": \"#AED581\",\n",
        "                        \"Rubric\": \"#F8BBD0\",\n",
        "                        \"Other\": \"#B0BEC5\",\n",
        "                    }\n",
        "\n",
        "                    def color_sections(val):\n",
        "                        return f\"background-color: {section_colors.get(val, '#ECECEC')}; font-weight: 500;\"\n",
        "\n",
        "                    st.markdown(\n",
        "                        \"Tip: Filter by Section (e.g., *Answer*), or sort by *Abs Attribution* \"\n",
        "                        \"to find the most influential tokens.\"\n",
        "                    )\n",
        "\n",
        "                    # ---- Llama-3 Table ----\n",
        "                    display_llama_df = filter_section(llama_df, selected_section)\n",
        "                    st.markdown(\"### 🔎 Llama-3 Token Attribution Table\")\n",
        "                    st.dataframe(\n",
        "                        display_llama_df.style.applymap(color_sections, subset=[\"Section\"]),\n",
        "                        use_container_width=True\n",
        "                    )\n",
        "\n",
        "                    # ---- Mistral-7B Table ----\n",
        "                    display_mistral_df = filter_section(mistral_df, selected_section)\n",
        "                    st.markdown(\"### 🔎 Mistral-7B Token Attribution Table\")\n",
        "                    st.dataframe(\n",
        "                        display_mistral_df.style.applymap(color_sections, subset=[\"Section\"]),\n",
        "                        use_container_width=True\n",
        "                    )\n",
        "\n",
        "                    st.markdown(\"---\")\n",
        "                    st.markdown(\"#### ℹ️ How to interpret XAI tables\")\n",
        "                    st.write(\"\"\"\n",
        "                        - **Token:** The model input tokens (words/subwords).\n",
        "                        - **Attribution:** Positive ⇒ supports **Compliant**; Negative ⇒ supports **Non-Compliant**.\n",
        "                        - **Abs Attribution:** (absolute value) shows overall influence, regardless of sign.\n",
        "                        - **Section:** Prompt part (System/Question, Answer Header, Answer, Rubric, Other).\n",
        "                        - **Tip:** Sort by 'Abs Attribution' to surface tokens that most impact the decision, regardless of direction.\n",
        "                        - **Best practice:** Focus on tokens in the 'Answer' section for applicant-driven impact.\n",
        "                    \"\"\")\n",
        "\n",
        "\n",
        "\n",
        "# ============= FINAL PDF EXPORT (all rows, uses override if present) =============\n",
        "def get_final_export_df(df):\n",
        "    df[\"final_label\"] = df[\"override_label\"].where(df[\"override_label\"].str.strip() != \"\", df[\"best_label\"])\n",
        "    df[\"final_reason\"] = df[\"override_reason\"].where(df[\"override_label\"].str.strip() != \"\", df[\"best_reason\"])\n",
        "    export_cols = {\n",
        "        \"question_id\": \"Question ID\",\n",
        "        \"question_text\": \"Question\",\n",
        "        \"answer_text\": \"Answer\",\n",
        "        \"final_label\": \"Final Label\"\n",
        "    }\n",
        "    return df[list(export_cols.keys())].rename(columns=export_cols)\n",
        "\n",
        "if uploaded_file and not df.empty:\n",
        "    export_df = get_final_export_df(edited_df)\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"Download Full Final Assessment as PDF\")\n",
        "    html = export_df.to_html(index=False, justify=\"center\")\n",
        "    try:\n",
        "        import pdfkit\n",
        "        config = pdfkit.configuration(wkhtmltopdf='/usr/bin/wkhtmltopdf')\n",
        "        pdf_bytes = pdfkit.from_string(html, False, configuration=config)\n",
        "        st.download_button(\n",
        "            label=\"Download Final Assessment PDF\",\n",
        "            data=pdf_bytes,\n",
        "            file_name=\"final_assessment.pdf\",\n",
        "            mime=\"application/pdf\"\n",
        "        )\n",
        "        log_event(\"export_prepared\", kind=\"final_assessment_pdf\", rows=int(len(export_df)))\n",
        "    except Exception as e:\n",
        "        st.info(f\"PDF export not available: {e}\")\n",
        "\n",
        "    # ==== WHOLE ASSESSMENT SUMMARY (always updated with overrides) ====\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"Final Application Assessment\")\n",
        "    final_result = assess_form(edited_df)\n",
        "\n",
        "    result_emoji = {\n",
        "        \"Compliant\": \",Congratulations! 🎉\",\n",
        "        \"Non-Compliant\": \",Needs Improvement🙅‍♂️\",\n",
        "        \"Fail\": \",Action Required😰\",\n",
        "        \"Unknown\": \"🤔❓\"\n",
        "    }\n",
        "    emoji = result_emoji.get(final_result, \"❓\")\n",
        "    st.markdown(f\"**Application Result:** {final_result} {emoji}\")\n",
        "    st.caption(\"All sections filter and display only relevant question types per logic (rules, semantic, LLM, overruling, HITL).\")\n",
        "\n",
        "# ================== SESSION AUDIT LOG & PURGE =====================\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"Session Audit Log\")\n",
        "if st.session_state[\"audit\"]:\n",
        "    audit_csv = pd.DataFrame(st.session_state[\"audit\"]).to_csv(index=False)\n",
        "    st.download_button(\"Download Audit Log (CSV)\", data=audit_csv,\n",
        "                       file_name=\"audit_log.csv\", mime=\"text/csv\")\n",
        "else:\n",
        "    st.caption(\"No audit events yet.\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "if st.button(\"🔒 Purge all session data now\"):\n",
        "    st.session_state.clear()\n",
        "    st.success(\"Session cleared. Please reload the page.\")\n",
        "    st.stop()\n",
        "'''\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND-AYRWr3Fq7"
      },
      "source": [
        "### Running and Deploying the Streamlit App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcHt-Q8Kv8oS",
        "outputId": "d5c6a01d-55b1-433b-f706-c89c8c038e4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your Streamlit app is live at: NgrokTunnel: \"https://ca60ea22bc64.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "# 1. Add your token (only needs to be done once per runtime)\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"30KjX8LyV0stvUHopwvg1u3yfoi_5wqXCWQEDraJMzWdUYJk2\")\n",
        "\n",
        "# 2. Start the tunnel and app\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "\n",
        "print(f\"Your Streamlit app is live at: {public_url}\")\n",
        "\n",
        "!streamlit run app.py &>/content/logs.txt &\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9od9ce71GAh"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()  # This will attempt to kill all running tunnels in this Colab session.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}